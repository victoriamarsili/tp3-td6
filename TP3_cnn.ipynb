{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad Torcuato Di Tella\n",
        "\n",
        "Licenciatura en Tecnología Digital\\\n",
        "**Tecnología Digital VI: Inteligencia Artificial**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import tarfile\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchaudio.datasets import GTZAN\n",
        "from torch.utils.data import DataLoader\n",
        "import torchaudio.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# TP3: Encodeador de música\n",
        "\n",
        "\n",
        "\n",
        "## Orden de pasos\n",
        "\n",
        "0. Elijan GPU para que corra mas rapido (RAM --> change runtime type --> T4 GPU)\n",
        "1. Descargamos el dataset y lo descomprimimos en alguna carpeta en nuestro drive.\n",
        "2. Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos.\n",
        "3. Visualización de los archivos\n",
        "4. Clasificación\n",
        "5. Evaluación\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaimeamigo\u001b[0m (\u001b[33msansonmariano-universidad-torcuato-di-tella\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\45235544\\_netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\45235544\\Documents\\TD 6\\tp3-td6\\wandb\\run-20241107_145606-xs1ye9ty</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/xs1ye9ty' target=\"_blank\">woven-sun-70</a></strong> to <a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6' target=\"_blank\">https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/xs1ye9ty' target=\"_blank\">https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/xs1ye9ty</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/xs1ye9ty?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x19fdc3a4290>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_name='TP3-TD6'\n",
        "username = \"sansonmariano-universidad-torcuato-di-tella\"\n",
        "wandb.login(key=\"d2875c91a36209496ee81454cccd95ebe3dc948d\")\n",
        "wandb.init(project = project_name, entity = username)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_seed = 42\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Definir parámetros\n",
        "samplerate = 22050\n",
        "data_dir = './genres_5sec'\n",
        "\n",
        "init_batch_size = 20\n",
        "init_num_epochs = 10\n",
        "init_lr = 0.0005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para parsear géneros\n",
        "def parse_genres(fname):\n",
        "    parts = fname.split('/')[-1].split('.')[0]\n",
        "    return parts\n",
        "\n",
        "# Definir la clase del dataset\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.files = []\n",
        "        for c in os.listdir(root):\n",
        "            self.files += [os.path.join(root, c, fname) for fname in os.listdir(os.path.join(root, c)) if fname.endswith('.wav')]\n",
        "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fpath = self.files[idx]\n",
        "        genre = parse_genres(fpath)\n",
        "        class_idx = self.classes.index(genre)\n",
        "        audio = torchaudio.load(fpath)[0]\n",
        "        return audio, class_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Function to normalize the audio data across the dataset\n",
        "def normalize_audio_data(dataset):\n",
        "    \"\"\"\n",
        "    Normalize the dataset by calculating the mean and standard deviation\n",
        "    of all audio samples and then applying standardization.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    for i in range(len(dataset)):\n",
        "        audio, _ = dataset[i]\n",
        "        all_data.append(audio)\n",
        "    stacked_data = torch.cat(all_data, dim=1)  # Concatenate along the time dimension for mean/std calculation\n",
        "    mean = stacked_data.mean()\n",
        "    std = stacked_data.std()\n",
        "\n",
        "    # Apply normalization to each sample\n",
        "    normalized_data = []\n",
        "    for i in range(len(dataset)):\n",
        "        audio, label = dataset[i]\n",
        "        normalized_audio = (audio - mean) / std\n",
        "        normalized_data.append((normalized_audio, label))\n",
        "\n",
        "    return normalized_data\n",
        "\n",
        "# Function to create stratified train, validation, and test DataLoaders\n",
        "def create_dataloaders(dataset, batch_size, test_size=0.3, val_size=0.5, random_state=42):\n",
        "    \"\"\"\n",
        "    Normalizes the dataset, splits it into train, validation, and test subsets,\n",
        "    and returns corresponding DataLoaders.\n",
        "    \"\"\"\n",
        "    # Normalize the dataset\n",
        "    normalized_data = normalize_audio_data(dataset)\n",
        "\n",
        "    # Extract labels for stratified splitting\n",
        "    labels = [label for _, label in normalized_data]\n",
        "\n",
        "    # Stratified split: train and temporary (val+test) split\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    for train_idx, temp_idx in split.split(range(len(normalized_data)), labels):\n",
        "        train_dataset = Subset(normalized_data, train_idx)\n",
        "        temp_dataset = Subset(normalized_data, temp_idx)\n",
        "\n",
        "    # Stratified split on temp data: validation and test split\n",
        "    val_test_labels = [labels[i] for i in temp_idx]\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
        "    for val_idx, test_idx in split.split(temp_idx, val_test_labels):\n",
        "        val_dataset = Subset(normalized_data, [temp_idx[i] for i in val_idx])\n",
        "        test_dataset = Subset(normalized_data, [temp_idx[i] for i in test_idx])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Usage example with MusicDataset\n",
        "dataset = MusicDataset(data_dir)\n",
        "batch_size = 20\n",
        "train_loader, val_loader, test_loader = create_dataloaders(dataset, batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_files=os.listdir(data_dir)\n",
        "\n",
        "classes = []\n",
        "\n",
        "for file in list_files:\n",
        "\n",
        "  name='{}/{}'.format(data_dir,file)\n",
        "\n",
        "  if os.path.isdir(name):\n",
        "\n",
        "    classes.append(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Visualización de los archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def audio_to_spectrogram(waveform):\n",
        "    # Ensure the waveform is in the correct shape\n",
        "    if len(waveform.shape) == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "    \n",
        "    # Convert the waveform to a spectrogram\n",
        "    spectrogram = tt.Spectrogram()(waveform)\n",
        "    return spectrogram\n",
        "\n",
        "def process_dataloader_to_spectrograms(dataloader):\n",
        "    spectrograms = []\n",
        "    \n",
        "    for batch in dataloader:\n",
        "        # Assuming the batch is a tuple (waveforms, labels) and waveforms are the audio data\n",
        "        waveforms, labels = batch\n",
        "        \n",
        "        # Process each waveform in the batch\n",
        "        batch_spectrograms = [audio_to_spectrogram(waveform) for waveform in waveforms]\n",
        "        \n",
        "        # Append to the list of spectrograms\n",
        "        spectrograms.append((torch.stack(batch_spectrograms), labels))\n",
        "    \n",
        "    return spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_channels = 1  # for RGB images, or 1 for grayscale\n",
        "num_classes = 10    # depends on your specific classification task\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_spectogram = process_dataloader_to_spectrograms(train_loader)\n",
        "val_spectogram = process_dataloader_to_spectrograms(val_loader)\n",
        "test_spectogram = process_dataloader_to_spectrograms(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modelo que recibe el tamaño de la capa y la cantidad de capas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajuste en la clase del modelo\n",
        "class ExperimentNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, layer_size, layers):\n",
        "        super(ExperimentNN, self).__init__()\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% probability\n",
        "\n",
        "        for layer in range(layers - 1):\n",
        "            if layer == 0:\n",
        "                self.fc_layers.append(nn.Linear(input_size, layer_size))\n",
        "            else:\n",
        "                self.fc_layers.append(nn.Linear(layer_size, layer_size))\n",
        "        \n",
        "        # Output layer\n",
        "        self.fc_layers.append(nn.Linear(layer_size, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Aplanar la onda de audio\n",
        "        x = (x - x.mean()) / x.std()  # Normalización\n",
        "        for fc in self.fc_layers[:-1]:  # Skip last layer\n",
        "            x = self.dropout(F.relu(fc(x)))  # ReLU + Dropout\n",
        "        x = self.fc_layers[-1](x)  # Output layer (no activation)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import copy\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, epochs, train_loader, val_loader, device):\n",
        "    best_loss = float(\"inf\")\n",
        "    best_model_state = None  # To store the best model's state\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Check if the current validation loss is the best\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_state = copy.deepcopy(model.state_dict())  # Save model state\n",
        "            print(f\"New best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Clear cache and collect garbage\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Load the best model's state into the model and return it\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return best_loss, model  # Return the final best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experimentando con 2 capas y 32 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 3.6866\n",
            "Validation loss: 3.6617\n",
            "New best validation loss: 3.6617\n",
            "Epoch 2/15 - Train loss: 2.8721\n",
            "Validation loss: 4.1420\n",
            "Epoch 3/15 - Train loss: 2.3004\n",
            "Validation loss: 4.1272\n",
            "Epoch 4/15 - Train loss: 1.7740\n",
            "Validation loss: 4.3407\n",
            "Epoch 5/15 - Train loss: 1.5943\n",
            "Validation loss: 4.5019\n",
            "Epoch 6/15 - Train loss: 1.2865\n",
            "Validation loss: 4.4943\n",
            "Epoch 7/15 - Train loss: 1.1259\n",
            "Validation loss: 4.8583\n",
            "Epoch 8/15 - Train loss: 1.0920\n",
            "Validation loss: 5.0810\n",
            "Epoch 9/15 - Train loss: 1.0538\n",
            "Validation loss: 5.0647\n",
            "Epoch 10/15 - Train loss: 0.7920\n",
            "Validation loss: 5.0582\n",
            "Epoch 11/15 - Train loss: 0.9502\n",
            "Validation loss: 5.3087\n",
            "Epoch 12/15 - Train loss: 0.8757\n",
            "Validation loss: 4.8651\n",
            "Epoch 13/15 - Train loss: 0.8179\n",
            "Validation loss: 5.0272\n",
            "Epoch 14/15 - Train loss: 0.7303\n",
            "Validation loss: 4.9836\n",
            "Epoch 15/15 - Train loss: 0.6440\n",
            "Validation loss: 5.3749\n",
            "Loss de validación para 2 capas y 32 unidades: 3.6616984605789185\n",
            "Precisión de validación: 12.16%\n",
            "Nuevo mejor modelo encontrado con 2 capas y 32 unidades.\n",
            "Experimentando con 2 capas y 64 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 4.0198\n",
            "Validation loss: 3.8270\n",
            "New best validation loss: 3.8270\n",
            "Epoch 2/15 - Train loss: 1.8573\n",
            "Validation loss: 4.4113\n",
            "Epoch 3/15 - Train loss: 1.2930\n",
            "Validation loss: 4.7826\n",
            "Epoch 4/15 - Train loss: 1.1551\n",
            "Validation loss: 4.9070\n",
            "Epoch 5/15 - Train loss: 1.0446\n",
            "Validation loss: 4.8932\n",
            "Epoch 6/15 - Train loss: 1.0565\n",
            "Validation loss: 5.3568\n",
            "Epoch 7/15 - Train loss: 0.7904\n",
            "Validation loss: 5.1008\n",
            "Epoch 8/15 - Train loss: 0.5511\n",
            "Validation loss: 5.2004\n",
            "Epoch 9/15 - Train loss: 0.5769\n",
            "Validation loss: 5.1328\n",
            "Epoch 10/15 - Train loss: 0.4939\n",
            "Validation loss: 5.1774\n",
            "Epoch 11/15 - Train loss: 0.5348\n",
            "Validation loss: 5.3178\n",
            "Epoch 12/15 - Train loss: 0.4630\n",
            "Validation loss: 5.0585\n",
            "Epoch 13/15 - Train loss: 0.5227\n",
            "Validation loss: 5.1670\n",
            "Epoch 14/15 - Train loss: 0.5051\n",
            "Validation loss: 5.2858\n",
            "Epoch 15/15 - Train loss: 0.3386\n",
            "Validation loss: 5.1350\n",
            "Loss de validación para 2 capas y 64 unidades: 3.827012598514557\n",
            "Precisión de validación: 13.51%\n",
            "Nuevo mejor modelo encontrado con 2 capas y 64 unidades.\n",
            "Experimentando con 2 capas y 128 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 4.3049\n",
            "Validation loss: 4.1640\n",
            "New best validation loss: 4.1640\n",
            "Epoch 2/15 - Train loss: 1.2078\n",
            "Validation loss: 4.9522\n",
            "Epoch 3/15 - Train loss: 0.8017\n",
            "Validation loss: 5.3843\n",
            "Epoch 4/15 - Train loss: 0.6688\n",
            "Validation loss: 5.3659\n",
            "Epoch 5/15 - Train loss: 0.4337\n",
            "Validation loss: 5.4666\n",
            "Epoch 6/15 - Train loss: 0.4669\n",
            "Validation loss: 5.3318\n",
            "Epoch 7/15 - Train loss: 0.4532\n",
            "Validation loss: 5.2556\n",
            "Epoch 8/15 - Train loss: 0.3383\n",
            "Validation loss: 5.2370\n",
            "Epoch 9/15 - Train loss: 0.3345\n",
            "Validation loss: 5.3588\n",
            "Epoch 10/15 - Train loss: 0.3212\n",
            "Validation loss: 5.2594\n",
            "Epoch 11/15 - Train loss: 0.2639\n",
            "Validation loss: 5.2997\n",
            "Epoch 12/15 - Train loss: 0.3466\n",
            "Validation loss: 5.4069\n",
            "Epoch 13/15 - Train loss: 0.2116\n",
            "Validation loss: 5.4290\n",
            "Epoch 14/15 - Train loss: 0.2166\n",
            "Validation loss: 5.2445\n",
            "Epoch 15/15 - Train loss: 0.1522\n",
            "Validation loss: 5.4559\n",
            "Loss de validación para 2 capas y 128 unidades: 4.163956642150879\n",
            "Precisión de validación: 12.16%\n",
            "Experimentando con 2 capas y 256 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 4.8508\n",
            "Validation loss: 4.5434\n",
            "New best validation loss: 4.5434\n",
            "Epoch 2/15 - Train loss: 0.8571\n",
            "Validation loss: 5.8040\n",
            "Epoch 3/15 - Train loss: 0.3503\n",
            "Validation loss: 5.4840\n",
            "Epoch 4/15 - Train loss: 0.3091\n",
            "Validation loss: 5.3242\n",
            "Epoch 5/15 - Train loss: 0.1942\n",
            "Validation loss: 5.3827\n",
            "Epoch 6/15 - Train loss: 0.1712\n",
            "Validation loss: 5.4711\n",
            "Epoch 7/15 - Train loss: 0.1887\n",
            "Validation loss: 5.5036\n",
            "Epoch 8/15 - Train loss: 0.2701\n",
            "Validation loss: 5.4954\n",
            "Epoch 9/15 - Train loss: 0.1566\n",
            "Validation loss: 5.2126\n",
            "Epoch 10/15 - Train loss: 0.1301\n",
            "Validation loss: 5.2998\n",
            "Epoch 11/15 - Train loss: 0.1372\n",
            "Validation loss: 5.4851\n",
            "Epoch 12/15 - Train loss: 0.2006\n",
            "Validation loss: 5.5472\n",
            "Epoch 13/15 - Train loss: 0.1328\n",
            "Validation loss: 5.4420\n",
            "Epoch 14/15 - Train loss: 0.0756\n",
            "Validation loss: 5.2622\n",
            "Epoch 15/15 - Train loss: 0.1927\n",
            "Validation loss: 5.6324\n",
            "Loss de validación para 2 capas y 256 unidades: 4.543418198823929\n",
            "Precisión de validación: 17.57%\n",
            "Nuevo mejor modelo encontrado con 2 capas y 256 unidades.\n",
            "Experimentando con 3 capas y 32 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.8061\n",
            "Validation loss: 2.4470\n",
            "New best validation loss: 2.4470\n",
            "Epoch 2/15 - Train loss: 4.4693\n",
            "Validation loss: 2.4439\n",
            "New best validation loss: 2.4439\n",
            "Epoch 3/15 - Train loss: 4.4679\n",
            "Validation loss: 2.4249\n",
            "New best validation loss: 2.4249\n",
            "Epoch 4/15 - Train loss: 4.1030\n",
            "Validation loss: 2.3500\n",
            "New best validation loss: 2.3500\n",
            "Epoch 5/15 - Train loss: 3.8993\n",
            "Validation loss: 2.4221\n",
            "Epoch 6/15 - Train loss: 3.7388\n",
            "Validation loss: 2.3317\n",
            "New best validation loss: 2.3317\n",
            "Epoch 7/15 - Train loss: 3.4676\n",
            "Validation loss: 2.2768\n",
            "New best validation loss: 2.2768\n",
            "Epoch 8/15 - Train loss: 3.6241\n",
            "Validation loss: 2.2771\n",
            "Epoch 9/15 - Train loss: 3.1585\n",
            "Validation loss: 2.2682\n",
            "New best validation loss: 2.2682\n",
            "Epoch 10/15 - Train loss: 3.1083\n",
            "Validation loss: 2.2751\n",
            "Epoch 11/15 - Train loss: 3.0544\n",
            "Validation loss: 2.3001\n",
            "Epoch 12/15 - Train loss: 2.8720\n",
            "Validation loss: 2.2781\n",
            "Epoch 13/15 - Train loss: 2.6017\n",
            "Validation loss: 2.2733\n",
            "Epoch 14/15 - Train loss: 2.5669\n",
            "Validation loss: 2.2573\n",
            "New best validation loss: 2.2573\n",
            "Epoch 15/15 - Train loss: 2.5046\n",
            "Validation loss: 2.2228\n",
            "New best validation loss: 2.2228\n",
            "Loss de validación para 3 capas y 32 unidades: 2.2228189408779144\n",
            "Precisión de validación: 18.92%\n",
            "Nuevo mejor modelo encontrado con 3 capas y 32 unidades.\n",
            "Experimentando con 3 capas y 64 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.8822\n",
            "Validation loss: 2.3954\n",
            "New best validation loss: 2.3954\n",
            "Epoch 2/15 - Train loss: 4.3398\n",
            "Validation loss: 2.4203\n",
            "Epoch 3/15 - Train loss: 4.1623\n",
            "Validation loss: 2.3464\n",
            "New best validation loss: 2.3464\n",
            "Epoch 4/15 - Train loss: 3.6821\n",
            "Validation loss: 2.4303\n",
            "Epoch 5/15 - Train loss: 3.8954\n",
            "Validation loss: 2.4120\n",
            "Epoch 6/15 - Train loss: 3.3796\n",
            "Validation loss: 2.3670\n",
            "Epoch 7/15 - Train loss: 3.1939\n",
            "Validation loss: 2.3413\n",
            "New best validation loss: 2.3413\n",
            "Epoch 8/15 - Train loss: 3.1335\n",
            "Validation loss: 2.3573\n",
            "Epoch 9/15 - Train loss: 2.7069\n",
            "Validation loss: 2.3216\n",
            "New best validation loss: 2.3216\n",
            "Epoch 10/15 - Train loss: 2.5331\n",
            "Validation loss: 2.3799\n",
            "Epoch 11/15 - Train loss: 2.5123\n",
            "Validation loss: 2.3643\n",
            "Epoch 12/15 - Train loss: 2.1602\n",
            "Validation loss: 2.2978\n",
            "New best validation loss: 2.2978\n",
            "Epoch 13/15 - Train loss: 2.2321\n",
            "Validation loss: 2.2964\n",
            "New best validation loss: 2.2964\n",
            "Epoch 14/15 - Train loss: 1.9761\n",
            "Validation loss: 2.2728\n",
            "New best validation loss: 2.2728\n",
            "Epoch 15/15 - Train loss: 1.8189\n",
            "Validation loss: 2.2934\n",
            "Loss de validación para 3 capas y 64 unidades: 2.272807091474533\n",
            "Precisión de validación: 25.68%\n",
            "Nuevo mejor modelo encontrado con 3 capas y 64 unidades.\n",
            "Experimentando con 3 capas y 128 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.8188\n",
            "Validation loss: 2.3938\n",
            "New best validation loss: 2.3938\n",
            "Epoch 2/15 - Train loss: 3.8005\n",
            "Validation loss: 2.4250\n",
            "Epoch 3/15 - Train loss: 3.6181\n",
            "Validation loss: 2.5078\n",
            "Epoch 4/15 - Train loss: 3.2451\n",
            "Validation loss: 2.5083\n",
            "Epoch 5/15 - Train loss: 3.0399\n",
            "Validation loss: 2.4041\n",
            "Epoch 6/15 - Train loss: 2.5900\n",
            "Validation loss: 2.4768\n",
            "Epoch 7/15 - Train loss: 2.3194\n",
            "Validation loss: 2.5665\n",
            "Epoch 8/15 - Train loss: 2.3113\n",
            "Validation loss: 2.4976\n",
            "Epoch 9/15 - Train loss: 2.0363\n",
            "Validation loss: 2.5867\n",
            "Epoch 10/15 - Train loss: 1.7563\n",
            "Validation loss: 2.5310\n",
            "Epoch 11/15 - Train loss: 1.6281\n",
            "Validation loss: 2.5733\n",
            "Epoch 12/15 - Train loss: 1.3952\n",
            "Validation loss: 2.5253\n",
            "Epoch 13/15 - Train loss: 1.5097\n",
            "Validation loss: 2.5944\n",
            "Epoch 14/15 - Train loss: 1.5820\n",
            "Validation loss: 2.6392\n",
            "Epoch 15/15 - Train loss: 1.1672\n",
            "Validation loss: 2.6233\n",
            "Loss de validación para 3 capas y 128 unidades: 2.393796920776367\n",
            "Precisión de validación: 9.46%\n",
            "Experimentando con 3 capas y 256 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.7417\n",
            "Validation loss: 2.3944\n",
            "New best validation loss: 2.3944\n",
            "Epoch 2/15 - Train loss: 3.0506\n",
            "Validation loss: 2.5054\n",
            "Epoch 3/15 - Train loss: 2.6166\n",
            "Validation loss: 2.4624\n",
            "Epoch 4/15 - Train loss: 2.3004\n",
            "Validation loss: 2.5027\n",
            "Epoch 5/15 - Train loss: 1.7523\n",
            "Validation loss: 2.5107\n",
            "Epoch 6/15 - Train loss: 1.6527\n",
            "Validation loss: 2.5970\n",
            "Epoch 7/15 - Train loss: 1.2308\n",
            "Validation loss: 2.7502\n",
            "Epoch 8/15 - Train loss: 0.8965\n",
            "Validation loss: 2.6328\n",
            "Epoch 9/15 - Train loss: 1.0903\n",
            "Validation loss: 2.6815\n",
            "Epoch 10/15 - Train loss: 0.8618\n",
            "Validation loss: 2.8232\n",
            "Epoch 11/15 - Train loss: 0.8091\n",
            "Validation loss: 2.9107\n",
            "Epoch 12/15 - Train loss: 0.6177\n",
            "Validation loss: 3.1020\n",
            "Epoch 13/15 - Train loss: 0.7837\n",
            "Validation loss: 2.9645\n",
            "Epoch 14/15 - Train loss: 0.6284\n",
            "Validation loss: 3.1448\n",
            "Epoch 15/15 - Train loss: 0.5527\n",
            "Validation loss: 3.1412\n",
            "Loss de validación para 3 capas y 256 unidades: 2.3943848609924316\n",
            "Precisión de validación: 11.49%\n",
            "Experimentando con 5 capas y 32 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3868\n",
            "Validation loss: 2.2856\n",
            "New best validation loss: 2.2856\n",
            "Epoch 2/15 - Train loss: 2.9049\n",
            "Validation loss: 2.2839\n",
            "New best validation loss: 2.2839\n",
            "Epoch 3/15 - Train loss: 2.7075\n",
            "Validation loss: 2.2881\n",
            "Epoch 4/15 - Train loss: 2.6783\n",
            "Validation loss: 2.2932\n",
            "Epoch 5/15 - Train loss: 2.5922\n",
            "Validation loss: 2.2957\n",
            "Epoch 6/15 - Train loss: 2.4555\n",
            "Validation loss: 2.2946\n",
            "Epoch 7/15 - Train loss: 2.5033\n",
            "Validation loss: 2.2931\n",
            "Epoch 8/15 - Train loss: 2.4523\n",
            "Validation loss: 2.2972\n",
            "Epoch 9/15 - Train loss: 2.4159\n",
            "Validation loss: 2.2996\n",
            "Epoch 10/15 - Train loss: 2.3562\n",
            "Validation loss: 2.2952\n",
            "Epoch 11/15 - Train loss: 2.3766\n",
            "Validation loss: 2.2955\n",
            "Epoch 12/15 - Train loss: 2.3541\n",
            "Validation loss: 2.3014\n",
            "Epoch 13/15 - Train loss: 2.3255\n",
            "Validation loss: 2.2990\n",
            "Epoch 14/15 - Train loss: 2.3302\n",
            "Validation loss: 2.2985\n",
            "Epoch 15/15 - Train loss: 2.2930\n",
            "Validation loss: 2.2971\n",
            "Loss de validación para 5 capas y 32 unidades: 2.283860594034195\n",
            "Precisión de validación: 11.49%\n",
            "Experimentando con 5 capas y 64 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3706\n",
            "Validation loss: 2.2812\n",
            "New best validation loss: 2.2812\n",
            "Epoch 2/15 - Train loss: 2.9077\n",
            "Validation loss: 2.2856\n",
            "Epoch 3/15 - Train loss: 2.7944\n",
            "Validation loss: 2.2867\n",
            "Epoch 4/15 - Train loss: 2.6757\n",
            "Validation loss: 2.2909\n",
            "Epoch 5/15 - Train loss: 2.5860\n",
            "Validation loss: 2.2908\n",
            "Epoch 6/15 - Train loss: 2.5158\n",
            "Validation loss: 2.2985\n",
            "Epoch 7/15 - Train loss: 2.4790\n",
            "Validation loss: 2.2955\n",
            "Epoch 8/15 - Train loss: 2.4535\n",
            "Validation loss: 2.3005\n",
            "Epoch 9/15 - Train loss: 2.4249\n",
            "Validation loss: 2.2942\n",
            "Epoch 10/15 - Train loss: 2.3850\n",
            "Validation loss: 2.2989\n",
            "Epoch 11/15 - Train loss: 2.3676\n",
            "Validation loss: 2.2932\n",
            "Epoch 12/15 - Train loss: 2.3016\n",
            "Validation loss: 2.3021\n",
            "Epoch 13/15 - Train loss: 2.2958\n",
            "Validation loss: 2.2972\n",
            "Epoch 14/15 - Train loss: 2.2868\n",
            "Validation loss: 2.3018\n",
            "Epoch 15/15 - Train loss: 2.2782\n",
            "Validation loss: 2.2959\n",
            "Loss de validación para 5 capas y 64 unidades: 2.281173050403595\n",
            "Precisión de validación: 7.43%\n",
            "Experimentando con 5 capas y 128 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3200\n",
            "Validation loss: 2.2915\n",
            "New best validation loss: 2.2915\n",
            "Epoch 2/15 - Train loss: 2.8537\n",
            "Validation loss: 2.2886\n",
            "New best validation loss: 2.2886\n",
            "Epoch 3/15 - Train loss: 2.7261\n",
            "Validation loss: 2.2951\n",
            "Epoch 4/15 - Train loss: 2.6824\n",
            "Validation loss: 2.2913\n",
            "Epoch 5/15 - Train loss: 2.5317\n",
            "Validation loss: 2.2915\n",
            "Epoch 6/15 - Train loss: 2.4370\n",
            "Validation loss: 2.2908\n",
            "Epoch 7/15 - Train loss: 2.3963\n",
            "Validation loss: 2.2893\n",
            "Epoch 8/15 - Train loss: 2.3459\n",
            "Validation loss: 2.2942\n",
            "Epoch 9/15 - Train loss: 2.3759\n",
            "Validation loss: 2.2901\n",
            "Epoch 10/15 - Train loss: 2.2919\n",
            "Validation loss: 2.2924\n",
            "Epoch 11/15 - Train loss: 2.2881\n",
            "Validation loss: 2.2902\n",
            "Epoch 12/15 - Train loss: 2.2831\n",
            "Validation loss: 2.2919\n",
            "Epoch 13/15 - Train loss: 2.2514\n",
            "Validation loss: 2.2926\n",
            "Epoch 14/15 - Train loss: 2.2658\n",
            "Validation loss: 2.2950\n",
            "Epoch 15/15 - Train loss: 2.2238\n",
            "Validation loss: 2.2911\n",
            "Loss de validación para 5 capas y 128 unidades: 2.2885722219944\n",
            "Precisión de validación: 6.76%\n",
            "Experimentando con 5 capas y 256 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3660\n",
            "Validation loss: 2.2650\n",
            "New best validation loss: 2.2650\n",
            "Epoch 2/15 - Train loss: 2.8927\n",
            "Validation loss: 2.2821\n",
            "Epoch 3/15 - Train loss: 2.6734\n",
            "Validation loss: 2.2930\n",
            "Epoch 4/15 - Train loss: 2.4983\n",
            "Validation loss: 2.2958\n",
            "Epoch 5/15 - Train loss: 2.4498\n",
            "Validation loss: 2.2979\n",
            "Epoch 6/15 - Train loss: 2.4362\n",
            "Validation loss: 2.2930\n",
            "Epoch 7/15 - Train loss: 2.3063\n",
            "Validation loss: 2.2904\n",
            "Epoch 8/15 - Train loss: 2.3257\n",
            "Validation loss: 2.2920\n",
            "Epoch 9/15 - Train loss: 2.2348\n",
            "Validation loss: 2.2879\n",
            "Epoch 10/15 - Train loss: 2.2072\n",
            "Validation loss: 2.2867\n",
            "Epoch 11/15 - Train loss: 2.1998\n",
            "Validation loss: 2.2876\n",
            "Epoch 12/15 - Train loss: 2.0910\n",
            "Validation loss: 2.2833\n",
            "Epoch 13/15 - Train loss: 2.0167\n",
            "Validation loss: 2.2776\n",
            "Epoch 14/15 - Train loss: 2.0387\n",
            "Validation loss: 2.2767\n",
            "Epoch 15/15 - Train loss: 1.9243\n",
            "Validation loss: 2.2745\n",
            "Loss de validación para 5 capas y 256 unidades: 2.264957159757614\n",
            "Precisión de validación: 11.49%\n",
            "Experimentando con 7 capas y 32 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3045\n",
            "Validation loss: 2.3048\n",
            "New best validation loss: 2.3048\n",
            "Epoch 2/15 - Train loss: 2.3248\n",
            "Validation loss: 2.3061\n",
            "Epoch 3/15 - Train loss: 2.3820\n",
            "Validation loss: 2.3073\n",
            "Epoch 4/15 - Train loss: 2.3486\n",
            "Validation loss: 2.3050\n",
            "Epoch 5/15 - Train loss: 2.2933\n",
            "Validation loss: 2.3038\n",
            "New best validation loss: 2.3038\n",
            "Epoch 6/15 - Train loss: 2.2881\n",
            "Validation loss: 2.3040\n",
            "Epoch 7/15 - Train loss: 2.2923\n",
            "Validation loss: 2.3037\n",
            "New best validation loss: 2.3037\n",
            "Epoch 8/15 - Train loss: 2.2857\n",
            "Validation loss: 2.3041\n",
            "Epoch 9/15 - Train loss: 2.2889\n",
            "Validation loss: 2.3025\n",
            "New best validation loss: 2.3025\n",
            "Epoch 10/15 - Train loss: 2.2742\n",
            "Validation loss: 2.3028\n",
            "Epoch 11/15 - Train loss: 2.2486\n",
            "Validation loss: 2.3024\n",
            "New best validation loss: 2.3024\n",
            "Epoch 12/15 - Train loss: 2.2448\n",
            "Validation loss: 2.3048\n",
            "Epoch 13/15 - Train loss: 2.2580\n",
            "Validation loss: 2.3025\n",
            "Epoch 14/15 - Train loss: 2.2438\n",
            "Validation loss: 2.3049\n",
            "Epoch 15/15 - Train loss: 2.2418\n",
            "Validation loss: 2.3009\n",
            "New best validation loss: 2.3009\n",
            "Loss de validación para 7 capas y 32 unidades: 2.3009308874607086\n",
            "Precisión de validación: 10.14%\n",
            "Experimentando con 7 capas y 64 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3120\n",
            "Validation loss: 2.3045\n",
            "New best validation loss: 2.3045\n",
            "Epoch 2/15 - Train loss: 2.3578\n",
            "Validation loss: 2.3054\n",
            "Epoch 3/15 - Train loss: 2.3577\n",
            "Validation loss: 2.3040\n",
            "New best validation loss: 2.3040\n",
            "Epoch 4/15 - Train loss: 2.3204\n",
            "Validation loss: 2.3023\n",
            "New best validation loss: 2.3023\n",
            "Epoch 5/15 - Train loss: 2.3162\n",
            "Validation loss: 2.3023\n",
            "Epoch 6/15 - Train loss: 2.3160\n",
            "Validation loss: 2.3021\n",
            "New best validation loss: 2.3021\n",
            "Epoch 7/15 - Train loss: 2.2822\n",
            "Validation loss: 2.2992\n",
            "New best validation loss: 2.2992\n",
            "Epoch 8/15 - Train loss: 2.2610\n",
            "Validation loss: 2.3048\n",
            "Epoch 9/15 - Train loss: 2.2625\n",
            "Validation loss: 2.3044\n",
            "Epoch 10/15 - Train loss: 2.2507\n",
            "Validation loss: 2.3030\n",
            "Epoch 11/15 - Train loss: 2.2436\n",
            "Validation loss: 2.3025\n",
            "Epoch 12/15 - Train loss: 2.2376\n",
            "Validation loss: 2.3019\n",
            "Epoch 13/15 - Train loss: 2.2205\n",
            "Validation loss: 2.3065\n",
            "Epoch 14/15 - Train loss: 2.2212\n",
            "Validation loss: 2.2963\n",
            "New best validation loss: 2.2963\n",
            "Epoch 15/15 - Train loss: 2.2204\n",
            "Validation loss: 2.3032\n",
            "Loss de validación para 7 capas y 64 unidades: 2.296349197626114\n",
            "Precisión de validación: 10.14%\n",
            "Experimentando con 7 capas y 128 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.3094\n",
            "Validation loss: 2.2971\n",
            "New best validation loss: 2.2971\n",
            "Epoch 2/15 - Train loss: 2.3888\n",
            "Validation loss: 2.2998\n",
            "Epoch 3/15 - Train loss: 2.3304\n",
            "Validation loss: 2.2986\n",
            "Epoch 4/15 - Train loss: 2.3119\n",
            "Validation loss: 2.2988\n",
            "Epoch 5/15 - Train loss: 2.3181\n",
            "Validation loss: 2.2992\n",
            "Epoch 6/15 - Train loss: 2.2740\n",
            "Validation loss: 2.2990\n",
            "Epoch 7/15 - Train loss: 2.2611\n",
            "Validation loss: 2.3011\n",
            "Epoch 8/15 - Train loss: 2.2678\n",
            "Validation loss: 2.2988\n",
            "Epoch 9/15 - Train loss: 2.2490\n",
            "Validation loss: 2.2962\n",
            "New best validation loss: 2.2962\n",
            "Epoch 10/15 - Train loss: 2.2328\n",
            "Validation loss: 2.2945\n",
            "New best validation loss: 2.2945\n",
            "Epoch 11/15 - Train loss: 2.2161\n",
            "Validation loss: 2.2967\n",
            "Epoch 12/15 - Train loss: 2.2304\n",
            "Validation loss: 2.3012\n",
            "Epoch 13/15 - Train loss: 2.2045\n",
            "Validation loss: 2.3007\n",
            "Epoch 14/15 - Train loss: 2.1871\n",
            "Validation loss: 2.3001\n",
            "Epoch 15/15 - Train loss: 2.1732\n",
            "Validation loss: 2.3006\n",
            "Loss de validación para 7 capas y 128 unidades: 2.2944642305374146\n",
            "Precisión de validación: 10.81%\n",
            "Experimentando con 7 capas y 256 unidades por capa...\n",
            "Epoch 1/15 - Train loss: 2.2878\n",
            "Validation loss: 2.2748\n",
            "New best validation loss: 2.2748\n",
            "Epoch 2/15 - Train loss: 2.4250\n",
            "Validation loss: 2.2960\n",
            "Epoch 3/15 - Train loss: 2.3011\n",
            "Validation loss: 2.2976\n",
            "Epoch 4/15 - Train loss: 2.2860\n",
            "Validation loss: 2.2992\n",
            "Epoch 5/15 - Train loss: 2.2512\n",
            "Validation loss: 2.2988\n",
            "Epoch 6/15 - Train loss: 2.2525\n",
            "Validation loss: 2.2978\n",
            "Epoch 7/15 - Train loss: 2.2372\n",
            "Validation loss: 2.3004\n",
            "Epoch 8/15 - Train loss: 2.2381\n",
            "Validation loss: 2.2940\n",
            "Epoch 9/15 - Train loss: 2.1917\n",
            "Validation loss: 2.3035\n",
            "Epoch 10/15 - Train loss: 2.1965\n",
            "Validation loss: 2.3012\n",
            "Epoch 11/15 - Train loss: 2.1889\n",
            "Validation loss: 2.2943\n",
            "Epoch 12/15 - Train loss: 2.1865\n",
            "Validation loss: 2.2988\n",
            "Epoch 13/15 - Train loss: 2.1714\n",
            "Validation loss: 2.3100\n",
            "Epoch 14/15 - Train loss: 2.1262\n",
            "Validation loss: 2.3099\n",
            "Epoch 15/15 - Train loss: 2.1300\n",
            "Validation loss: 2.3200\n",
            "Loss de validación para 7 capas y 256 unidades: 2.2747628688812256\n",
            "Precisión de validación: 9.46%\n",
            "Mejor modelo: 7 capas y 256 unidades, con precisión de validación de 25.68% y pérdida de validación de 2.2728\n"
          ]
        }
      ],
      "source": [
        "\n",
        "input_size = samplerate * 5\n",
        "\n",
        "num_classes = len(dataset.classes)  # Número de clases (géneros musicales)\n",
        "\n",
        "# Define nuevos valores de hiperparámetros para experimentar\n",
        "layers_list = [2, 3, 5, 7]       # Pruebas con más capas\n",
        "sizes_list = [32, 64, 128, 256]  # Pruebas con más unidades en cada capa\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_val_accuracy = 0\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 1e-4\n",
        "best_model = None\n",
        "\n",
        "# Loop de experimentación\n",
        "for layers in layers_list:\n",
        "    for size in sizes_list:\n",
        "        print(f\"Experimentando con {layers} capas y {size} unidades por capa...\")\n",
        "        \n",
        "        # Inicializar el modelo con la configuración actual\n",
        "        model = ExperimentNN(input_size,\n",
        "                             num_classes,\n",
        "                             size,\n",
        "                             layers).to(device)\n",
        "\n",
        "        # Definir el criterio y optimizador con los pesos de las clases\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "        # Define el scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # Llama a train_model pasándole el scheduler\n",
        "        validation_loss, temp_model = train_model(\n",
        "            model, criterion, optimizer, scheduler=scheduler, epochs=15,\n",
        "            train_loader=train_loader, val_loader=val_loader, device=device\n",
        "        )\n",
        "\n",
        "        # Calcular precisión de validación\n",
        "        temp_model.eval()\n",
        "        correct = 0\n",
        "        total = len(val_loader.dataset)\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = temp_model(inputs)  # Use temp_model here\n",
        "                predicted = outputs.argmax(dim=1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        print(f\"Loss de validación para {layers} capas y {size} unidades: {validation_loss}\")\n",
        "        print(f\"Precisión de validación: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Guardar el modelo con mejor precisión y menor pérdida\n",
        "        if val_accuracy > best_val_accuracy or (val_accuracy == best_val_accuracy and validation_loss < best_val_loss):\n",
        "            best_val_loss = validation_loss\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = temp_model  # Store the model with the best configuration\n",
        "            print(f\"Nuevo mejor modelo encontrado con {layers} capas y {size} unidades.\")\n",
        "\n",
        "print(f\"Mejor modelo: {layers} capas y {size} unidades, con precisión de validación de {best_val_accuracy:.2f}% y pérdida de validación de {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación final en el conjunto de prueba\n",
        "print(\"Evaluando el mejor modelo en el conjunto de prueba...\")\n",
        "best_model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = len(test_loader.dataset)\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = best_model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        predicted = outputs.argmax(dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = 100 * correct / total\n",
        "\n",
        "print(f\"Loss en el conjunto de prueba: {test_loss:.4f}\")\n",
        "print(f\"Precisión en el conjunto de prueba: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes, conv_layers_config):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Initialize the list to hold the convolutional layers\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "\n",
        "        # Initialize the number of input channels for the first layer\n",
        "        in_channels = input_channels\n",
        "\n",
        "        # Dynamically create convolutional layers based on the configuration\n",
        "        for (out_channels, kernel_size, stride, padding) in conv_layers_config:\n",
        "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
        "            in_channels = out_channels  # Update in_channels for the next layer\n",
        "\n",
        "        # Calculate the size after convolution and pooling to define the fully connected layer\n",
        "        # Assuming pooling reduces the size by a factor of 2 at each layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Get the final feature map size (after all conv and pooling layers)\n",
        "        self.final_feature_map_size = self._get_conv_output_size(input_channels, conv_layers_config)\n",
        "        \n",
        "        # Define 9 fully connected layers with 256 nodes each\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.fc_layers.append(nn.Linear(self.final_feature_map_size, 64))  # First fully connected layer\n",
        "        for _ in range(2):  # Add 8 more fully connected layers with 256 nodes\n",
        "            self.fc_layers.append(nn.Linear(64, 64))\n",
        "        \n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(64, num_classes)  # Output layer for classification\n",
        "        \n",
        "    def _get_conv_output_size(self, input_channels, conv_layers_config):\n",
        "        # Sample input size (height x width) to calculate the final feature map size\n",
        "        # You can adjust these values based on your actual input size\n",
        "        height = 201  # Replace with your actual input height\n",
        "        width = 552   # Replace with your actual input width\n",
        "        \n",
        "        # Apply each convolutional and pooling layer\n",
        "        for (out_channels, kernel_size, stride, padding) in conv_layers_config:\n",
        "            height = (height + 2 * padding - kernel_size) // stride + 1\n",
        "            width = (width + 2 * padding - kernel_size) // stride + 1\n",
        "            height = height // 2  # Max pooling halves the height\n",
        "            width = width // 2    # Max pooling halves the width\n",
        "        \n",
        "        # Return the total number of features after all convolutional and pooling layers\n",
        "        return out_channels * height * width\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply each convolutional layer followed by ReLU and pooling\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = F.relu(conv_layer(x))\n",
        "            x = self.pool(x)\n",
        "        \n",
        "        # Flatten the output before passing it to the fully connected layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
        "\n",
        "        # Apply the fully connected layers\n",
        "        for fc in self.fc_layers:\n",
        "            x = F.relu(fc(x))\n",
        "        \n",
        "        # Output layer (classification)\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Train the model and select the best gradients based on validation loss.\n",
        "    \"\"\"\n",
        "    model.train()  # Set the model to training mode\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_gradients = None  # To store gradients with the lowest validation loss\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        # Training loop\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Accumulate training loss\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # Print loss for every batch\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == len(train_loader):\n",
        "                print(f\"  Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "        \n",
        "        # Calculate average training loss for the epoch\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} Training completed. Average Loss: {avg_train_loss:.4f}\")\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = len(val_loader)\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        avg_val_loss = val_loss / total\n",
        "        print(f\"Epoch {epoch+1} Validation completed. Average Loss: {avg_val_loss:.4f}\")\n",
        "        \n",
        "        # If this epoch has the best (lowest) validation loss, save the gradients\n",
        "        if (accuracy > best_accuracy) or ((accuracy == best_accuracy) and (avg_val_loss < best_val_loss)):\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_accuracy = accuracy\n",
        "            # Capture gradients\n",
        "            best_gradients = {name: param.grad.clone() for name, param in model.named_parameters() if param.grad is not None}\n",
        "            print(f\"New best gradients stored for epoch {epoch+1} with validation loss {best_val_loss:.4f} and accuracy {accuracy}%\")\n",
        "        \n",
        "        # Set model back to training mode\n",
        "        model.train()\n",
        "\n",
        "    print(\"\\nTraining complete.\")\n",
        "    \n",
        "    return best_gradients\n",
        "\n",
        "def test_model_configuration(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0  # Total number of samples processed\n",
        "\n",
        "    # Ensure no gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:  # Assuming test_loader is the correct DataLoader for your test set\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get the predicted class labels (the one with the highest logit)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Accumulate total samples\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Accumulate correct predictions\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average test loss\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def test_multiple_configurations(train, val, test, criterion, device, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Test multiple model configurations, applying the best gradients for evaluation,\n",
        "    and save the best model based on accuracy or validation loss.\n",
        "    \"\"\"\n",
        "    # Different configurations for the CNN model\n",
        "    configurations = [\n",
        "        [(32, 3, 1, 1), (64, 3, 1, 1), (128, 3, 1, 1)],  # Configuration 1\n",
        "        [(32, 5, 1, 2), (64, 5, 1, 2)],                  # Configuration 2\n",
        "        [(16, 3, 1, 1), (32, 3, 1, 1), (64, 3, 1, 1)],   # Configuration 3\n",
        "        [(64, 3, 1, 1), (128, 3, 1, 1), (256, 3, 1, 1)]  # Configuration 4\n",
        "    ]\n",
        "    \n",
        "    best_model = None\n",
        "    best_accuracy = 0.0\n",
        "    best_loss = float(\"inf\")\n",
        "    \n",
        "    for idx, conv_layers_config in enumerate(configurations):\n",
        "        print(f\"\\nTesting Configuration {idx + 1} with convolutional layers: {conv_layers_config}\")\n",
        "        \n",
        "        # Initialize the model with the current configuration\n",
        "        model = CNN(input_channels=1, num_classes=10, conv_layers_config=conv_layers_config)\n",
        "        model.to(device)\n",
        "        \n",
        "        # Initialize optimizer (e.g., Adam) and train the model\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        # Train the model on the training set\n",
        "        best_gradients = train_model(model, train, val, criterion, optimizer, num_epochs, device)\n",
        "        \n",
        "        # Apply the best gradients to the model parameters\n",
        "        if best_gradients is not None:\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    if name in best_gradients and param.grad is not None:\n",
        "                        param.grad.copy_(best_gradients[name])\n",
        "        \n",
        "        # Evaluate the model on the test set with best gradients applied\n",
        "        test_loss, accuracy = test_model_configuration(model, test, criterion, device)\n",
        "        \n",
        "        # Print the results for the current configuration\n",
        "        print(f\"Configuration {idx + 1} Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "        \n",
        "        # Save the best model based on accuracy; if accuracy is the same, use loss as the tie-breaker\n",
        "        if accuracy > best_accuracy or (accuracy == best_accuracy and test_loss < best_loss):\n",
        "            best_accuracy = accuracy\n",
        "            best_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            print(f\"New best model found with accuracy: {best_accuracy:.2f}% and loss: {best_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\nBest Model Test Accuracy: {best_accuracy:.2f}% with Loss: {best_loss:.4f}\")\n",
        "    return best_loss, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Configuration 1 with convolutional layers: [(32, 3, 1, 1), (64, 3, 1, 1), (128, 3, 1, 1)]\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.3256\n",
            "  Batch 20/35 - Loss: 2.2824\n",
            "  Batch 30/35 - Loss: 2.3461\n",
            "  Batch 35/35 - Loss: 2.2804\n",
            "Epoch 1 Training completed. Average Loss: 27.9878\n",
            "Epoch 1 Validation completed. Average Loss: 2.3046\n",
            "New best gradients stored for epoch 1 with validation loss 2.3046\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.2523\n",
            "  Batch 20/35 - Loss: 2.1388\n",
            "  Batch 30/35 - Loss: 2.0734\n",
            "  Batch 35/35 - Loss: 1.9359\n",
            "Epoch 2 Training completed. Average Loss: 2.1825\n",
            "Epoch 2 Validation completed. Average Loss: 2.3035\n",
            "New best gradients stored for epoch 2 with validation loss 2.3035\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.0717\n",
            "  Batch 20/35 - Loss: 2.0913\n",
            "  Batch 30/35 - Loss: 1.8698\n",
            "  Batch 35/35 - Loss: 1.2375\n",
            "Epoch 3 Training completed. Average Loss: 1.9165\n",
            "Epoch 3 Validation completed. Average Loss: 2.5949\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.2280\n",
            "  Batch 20/35 - Loss: 1.5648\n",
            "  Batch 30/35 - Loss: 1.9123\n",
            "  Batch 35/35 - Loss: 1.0202\n",
            "Epoch 4 Training completed. Average Loss: 1.5816\n",
            "Epoch 4 Validation completed. Average Loss: 2.4611\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 1.1195\n",
            "  Batch 20/35 - Loss: 1.3280\n",
            "  Batch 30/35 - Loss: 1.1363\n",
            "  Batch 35/35 - Loss: 1.1470\n",
            "Epoch 5 Training completed. Average Loss: 1.1736\n",
            "Epoch 5 Validation completed. Average Loss: 3.4724\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 1.2760\n",
            "  Batch 20/35 - Loss: 0.9869\n",
            "  Batch 30/35 - Loss: 0.4734\n",
            "  Batch 35/35 - Loss: 0.0811\n",
            "Epoch 6 Training completed. Average Loss: 0.7665\n",
            "Epoch 6 Validation completed. Average Loss: 4.8670\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.8917\n",
            "  Batch 20/35 - Loss: 1.4693\n",
            "  Batch 30/35 - Loss: 0.4342\n",
            "  Batch 35/35 - Loss: 0.4802\n",
            "Epoch 7 Training completed. Average Loss: 0.8309\n",
            "Epoch 7 Validation completed. Average Loss: 4.4380\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.4988\n",
            "  Batch 20/35 - Loss: 1.8820\n",
            "  Batch 30/35 - Loss: 0.1810\n",
            "  Batch 35/35 - Loss: 0.8945\n",
            "Epoch 8 Training completed. Average Loss: 0.5284\n",
            "Epoch 8 Validation completed. Average Loss: 5.7806\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.5294\n",
            "  Batch 20/35 - Loss: 0.9948\n",
            "  Batch 30/35 - Loss: 0.4573\n",
            "  Batch 35/35 - Loss: 1.3567\n",
            "Epoch 9 Training completed. Average Loss: 0.5873\n",
            "Epoch 9 Validation completed. Average Loss: 7.8220\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.5690\n",
            "  Batch 20/35 - Loss: 0.3564\n",
            "  Batch 30/35 - Loss: 0.4327\n",
            "  Batch 35/35 - Loss: 0.0123\n",
            "Epoch 10 Training completed. Average Loss: 0.3243\n",
            "Epoch 10 Validation completed. Average Loss: 6.8209\n",
            "\n",
            "Training complete.\n",
            "Configuration 1 Test Loss: 6.5630, Test Accuracy: 30.20%\n",
            "New best model found with accuracy: 30.20% and loss: 6.5630\n",
            "\n",
            "Testing Configuration 2 with convolutional layers: [(32, 5, 1, 2), (64, 5, 1, 2)]\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.6395\n",
            "  Batch 20/35 - Loss: 2.3212\n",
            "  Batch 30/35 - Loss: 2.3158\n",
            "  Batch 35/35 - Loss: 2.2988\n",
            "Epoch 1 Training completed. Average Loss: 60.4614\n",
            "Epoch 1 Validation completed. Average Loss: 2.3068\n",
            "New best gradients stored for epoch 1 with validation loss 2.3068\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.2962\n",
            "  Batch 20/35 - Loss: 2.3190\n",
            "  Batch 30/35 - Loss: 2.3134\n",
            "  Batch 35/35 - Loss: 2.2986\n",
            "Epoch 2 Training completed. Average Loss: 2.3009\n",
            "Epoch 2 Validation completed. Average Loss: 2.3074\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.2937\n",
            "  Batch 20/35 - Loss: 2.3115\n",
            "  Batch 30/35 - Loss: 2.3083\n",
            "  Batch 35/35 - Loss: 2.2975\n",
            "Epoch 3 Training completed. Average Loss: 2.2804\n",
            "Epoch 3 Validation completed. Average Loss: 2.3062\n",
            "New best gradients stored for epoch 3 with validation loss 2.3062\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 2.2236\n",
            "  Batch 20/35 - Loss: 2.3004\n",
            "  Batch 30/35 - Loss: 2.2618\n",
            "  Batch 35/35 - Loss: 2.1267\n",
            "Epoch 4 Training completed. Average Loss: 2.1857\n",
            "Epoch 4 Validation completed. Average Loss: 2.3100\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 1.9556\n",
            "  Batch 20/35 - Loss: 2.1777\n",
            "  Batch 30/35 - Loss: 2.2243\n",
            "  Batch 35/35 - Loss: 1.4030\n",
            "Epoch 5 Training completed. Average Loss: 1.9912\n",
            "Epoch 5 Validation completed. Average Loss: 2.3075\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 1.5352\n",
            "  Batch 20/35 - Loss: 1.8556\n",
            "  Batch 30/35 - Loss: 1.7239\n",
            "  Batch 35/35 - Loss: 1.1697\n",
            "Epoch 6 Training completed. Average Loss: 1.6797\n",
            "Epoch 6 Validation completed. Average Loss: 2.7457\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 1.2321\n",
            "  Batch 20/35 - Loss: 1.8552\n",
            "  Batch 30/35 - Loss: 1.2388\n",
            "  Batch 35/35 - Loss: 0.8813\n",
            "Epoch 7 Training completed. Average Loss: 1.4297\n",
            "Epoch 7 Validation completed. Average Loss: 3.0185\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.9574\n",
            "  Batch 20/35 - Loss: 1.2033\n",
            "  Batch 30/35 - Loss: 0.7672\n",
            "  Batch 35/35 - Loss: 0.5550\n",
            "Epoch 8 Training completed. Average Loss: 1.0848\n",
            "Epoch 8 Validation completed. Average Loss: 4.1931\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.6539\n",
            "  Batch 20/35 - Loss: 0.9480\n",
            "  Batch 30/35 - Loss: 0.8824\n",
            "  Batch 35/35 - Loss: 0.2928\n",
            "Epoch 9 Training completed. Average Loss: 0.7963\n",
            "Epoch 9 Validation completed. Average Loss: 5.4833\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.5439\n",
            "  Batch 20/35 - Loss: 0.7763\n",
            "  Batch 30/35 - Loss: 0.7201\n",
            "  Batch 35/35 - Loss: 0.3598\n",
            "Epoch 10 Training completed. Average Loss: 0.6829\n",
            "Epoch 10 Validation completed. Average Loss: 5.1179\n",
            "\n",
            "Training complete.\n",
            "Configuration 2 Test Loss: 7.9978, Test Accuracy: 21.48%\n",
            "\n",
            "Testing Configuration 3 with convolutional layers: [(16, 3, 1, 1), (32, 3, 1, 1), (64, 3, 1, 1)]\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 5.4290\n",
            "  Batch 20/35 - Loss: 2.2984\n",
            "  Batch 30/35 - Loss: 2.2907\n",
            "  Batch 35/35 - Loss: 2.2556\n",
            "Epoch 1 Training completed. Average Loss: 29.7948\n",
            "Epoch 1 Validation completed. Average Loss: 2.2686\n",
            "New best gradients stored for epoch 1 with validation loss 2.2686\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.1600\n",
            "  Batch 20/35 - Loss: 2.1327\n",
            "  Batch 30/35 - Loss: 2.2910\n",
            "  Batch 35/35 - Loss: 1.8641\n",
            "Epoch 2 Training completed. Average Loss: 2.1112\n",
            "Epoch 2 Validation completed. Average Loss: 2.1767\n",
            "New best gradients stored for epoch 2 with validation loss 2.1767\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 1.6607\n",
            "  Batch 20/35 - Loss: 1.5454\n",
            "  Batch 30/35 - Loss: 2.0580\n",
            "  Batch 35/35 - Loss: 1.2100\n",
            "Epoch 3 Training completed. Average Loss: 1.7350\n",
            "Epoch 3 Validation completed. Average Loss: 2.2937\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 0.9871\n",
            "  Batch 20/35 - Loss: 1.3867\n",
            "  Batch 30/35 - Loss: 1.7884\n",
            "  Batch 35/35 - Loss: 0.8109\n",
            "Epoch 4 Training completed. Average Loss: 1.3513\n",
            "Epoch 4 Validation completed. Average Loss: 2.5380\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 0.7609\n",
            "  Batch 20/35 - Loss: 1.0401\n",
            "  Batch 30/35 - Loss: 0.9896\n",
            "  Batch 35/35 - Loss: 1.1183\n",
            "Epoch 5 Training completed. Average Loss: 1.0580\n",
            "Epoch 5 Validation completed. Average Loss: 3.3078\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 0.8677\n",
            "  Batch 20/35 - Loss: 0.9137\n",
            "  Batch 30/35 - Loss: 1.5534\n",
            "  Batch 35/35 - Loss: 4.4705\n",
            "Epoch 6 Training completed. Average Loss: 0.9414\n",
            "Epoch 6 Validation completed. Average Loss: 3.2574\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.6677\n",
            "  Batch 20/35 - Loss: 0.9964\n",
            "  Batch 30/35 - Loss: 0.4507\n",
            "  Batch 35/35 - Loss: 1.1681\n",
            "Epoch 7 Training completed. Average Loss: 0.9218\n",
            "Epoch 7 Validation completed. Average Loss: 5.9604\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.3877\n",
            "  Batch 20/35 - Loss: 0.9088\n",
            "  Batch 30/35 - Loss: 0.5638\n",
            "  Batch 35/35 - Loss: 1.6503\n",
            "Epoch 8 Training completed. Average Loss: 0.8166\n",
            "Epoch 8 Validation completed. Average Loss: 4.7277\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.4174\n",
            "  Batch 20/35 - Loss: 0.7540\n",
            "  Batch 30/35 - Loss: 0.4003\n",
            "  Batch 35/35 - Loss: 0.2772\n",
            "Epoch 9 Training completed. Average Loss: 0.6005\n",
            "Epoch 9 Validation completed. Average Loss: 5.4245\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.6595\n",
            "  Batch 20/35 - Loss: 0.4404\n",
            "  Batch 30/35 - Loss: 0.3281\n",
            "  Batch 35/35 - Loss: 0.3242\n",
            "Epoch 10 Training completed. Average Loss: 0.4632\n",
            "Epoch 10 Validation completed. Average Loss: 4.6398\n",
            "\n",
            "Training complete.\n",
            "Configuration 3 Test Loss: 4.7895, Test Accuracy: 27.52%\n",
            "\n",
            "Testing Configuration 4 with convolutional layers: [(64, 3, 1, 1), (128, 3, 1, 1), (256, 3, 1, 1)]\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.1130\n",
            "  Batch 20/35 - Loss: 2.3344\n",
            "  Batch 30/35 - Loss: 2.3127\n",
            "  Batch 35/35 - Loss: 2.3346\n",
            "Epoch 1 Training completed. Average Loss: 62.7765\n",
            "Epoch 1 Validation completed. Average Loss: 2.3281\n",
            "New best gradients stored for epoch 1 with validation loss 2.3281\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.2658\n",
            "  Batch 20/35 - Loss: 2.2253\n",
            "  Batch 30/35 - Loss: 2.2856\n",
            "  Batch 35/35 - Loss: 2.1202\n",
            "Epoch 2 Training completed. Average Loss: 2.2406\n",
            "Epoch 2 Validation completed. Average Loss: 2.2271\n",
            "New best gradients stored for epoch 2 with validation loss 2.2271\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.0273\n",
            "  Batch 20/35 - Loss: 2.0135\n",
            "  Batch 30/35 - Loss: 2.2024\n",
            "  Batch 35/35 - Loss: 1.8897\n",
            "Epoch 3 Training completed. Average Loss: 2.0700\n",
            "Epoch 3 Validation completed. Average Loss: 2.3080\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.8612\n",
            "  Batch 20/35 - Loss: 1.6151\n",
            "  Batch 30/35 - Loss: 2.1007\n",
            "  Batch 35/35 - Loss: 1.7151\n",
            "Epoch 4 Training completed. Average Loss: 1.8244\n",
            "Epoch 4 Validation completed. Average Loss: 2.3428\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 1.5487\n",
            "  Batch 20/35 - Loss: 2.1099\n",
            "  Batch 30/35 - Loss: 2.1699\n",
            "  Batch 35/35 - Loss: 1.6012\n",
            "Epoch 5 Training completed. Average Loss: 1.9765\n",
            "Epoch 5 Validation completed. Average Loss: 2.3391\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 1.3354\n",
            "  Batch 20/35 - Loss: 1.4393\n",
            "  Batch 30/35 - Loss: 2.0321\n",
            "  Batch 35/35 - Loss: 1.8898\n",
            "Epoch 6 Training completed. Average Loss: 1.6037\n",
            "Epoch 6 Validation completed. Average Loss: 2.6896\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 1.0228\n",
            "  Batch 20/35 - Loss: 1.2227\n",
            "  Batch 30/35 - Loss: 1.2980\n",
            "  Batch 35/35 - Loss: 1.6541\n",
            "Epoch 7 Training completed. Average Loss: 1.2916\n",
            "Epoch 7 Validation completed. Average Loss: 4.1210\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 1.0612\n",
            "  Batch 20/35 - Loss: 1.8933\n",
            "  Batch 30/35 - Loss: 1.1875\n",
            "  Batch 35/35 - Loss: 0.3308\n",
            "Epoch 8 Training completed. Average Loss: 1.1834\n",
            "Epoch 8 Validation completed. Average Loss: 4.4638\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.5309\n",
            "  Batch 20/35 - Loss: 1.6079\n",
            "  Batch 30/35 - Loss: 0.5178\n",
            "  Batch 35/35 - Loss: 0.1705\n",
            "Epoch 9 Training completed. Average Loss: 0.7892\n",
            "Epoch 9 Validation completed. Average Loss: 5.8831\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.4916\n",
            "  Batch 20/35 - Loss: 0.8101\n",
            "  Batch 30/35 - Loss: 0.8043\n",
            "  Batch 35/35 - Loss: 0.3749\n",
            "Epoch 10 Training completed. Average Loss: 0.6186\n",
            "Epoch 10 Validation completed. Average Loss: 7.4017\n",
            "\n",
            "Training complete.\n",
            "Configuration 4 Test Loss: 7.9867, Test Accuracy: 22.15%\n",
            "\n",
            "Best Model Test Accuracy: 30.20% with Loss: 6.5630\n"
          ]
        }
      ],
      "source": [
        "# Assume 'test_loader' is the DataLoader for your test set, and 'criterion' is the loss function (e.g., CrossEntropyLoss)\n",
        "best_model = test_multiple_configurations(train_spectogram, val_spectogram, test_spectogram, criterion, device, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 6.56303671002388 - Model: CNN(\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=220800, out_features=64, bias=True)\n",
            "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
            "  )\n",
            "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loss: {best_model[0]} - Model: {best_model[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6.56303671002388, 30.201342281879196)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn_best_model = best_model[1]\n",
        "\n",
        "test_model_configuration(cnn_best_model,test_spectogram,criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class different_act_CNN(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=10, activation_function=\"relu\"):\n",
        "        super(different_act_CNN, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers (fixed as per example)\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),  # Conv layer 1\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # Conv layer 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)              # Conv layer 3\n",
        "        ])\n",
        "        \n",
        "        # Max Pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the output feature size after conv layers and pooling\n",
        "        self.final_feature_map_size = self._get_conv_output_size(input_channels)\n",
        "\n",
        "        # Define fully connected layers (fixed as per example)\n",
        "        self.fc_layers = nn.ModuleList([\n",
        "            nn.Linear(self.final_feature_map_size, 64),  # First fully connected layer\n",
        "            nn.Linear(64, 64),                           # Second fully connected layer\n",
        "            nn.Linear(64, 64)                            # Third fully connected layer\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(64, num_classes)\n",
        "\n",
        "        # Set activation function\n",
        "        self.activation = self._get_activation_function(activation_function)\n",
        "\n",
        "    def _get_conv_output_size(self, input_channels):\n",
        "        # Sample input size (height x width)\n",
        "        height, width = 201, 552  # Replace with actual input height and width\n",
        "\n",
        "        # Pass through convolutional and pooling layers to determine final size\n",
        "        for layer in self.conv_layers:\n",
        "            height = (height + 2 * layer.padding[0] - layer.kernel_size[0]) // layer.stride[0] + 1\n",
        "            width = (width + 2 * layer.padding[1] - layer.kernel_size[1]) // layer.stride[1] + 1\n",
        "            height //= 2  # Max pooling halves the height\n",
        "            width //= 2   # Max pooling halves the width\n",
        "\n",
        "        # Output feature map size\n",
        "        return height * width * 128\n",
        "\n",
        "    def _get_activation_function(self, activation_function):\n",
        "        # Map the string to the appropriate activation function\n",
        "        if activation_function == \"relu\":\n",
        "            return F.relu\n",
        "        elif activation_function == \"leaky_relu\":\n",
        "            return F.leaky_relu\n",
        "        elif activation_function == \"tanh\":\n",
        "            return torch.tanh\n",
        "        elif activation_function == \"sigmoid\":\n",
        "            return torch.sigmoid\n",
        "        elif activation_function == \"softmax\":\n",
        "            return F.softmax\n",
        "        elif activation_function == \"elu\":\n",
        "            return F.elu\n",
        "        elif activation_function == \"selu\":\n",
        "            return F.selu\n",
        "        elif activation_function == \"gelu\":\n",
        "            return F.gelu\n",
        "        elif activation_function == \"swish\":\n",
        "            return lambda x: x * torch.sigmoid(x)  # Swish activation: x * sigmoid(x)\n",
        "        elif activation_function == \"hard_sigmoid\":\n",
        "            return F.hardsigmoid\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {activation_function}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through conv layers with the selected activation function and pooling\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = self.activation(conv_layer(x))\n",
        "            x = self.pool(x)\n",
        "        \n",
        "        # Flatten the output from conv layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Forward pass through fully connected layers with the selected activation function\n",
        "        for fc_layer in self.fc_layers:\n",
        "            x = self.activation(fc_layer(x))\n",
        "        \n",
        "        # Final output layer\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to test different activation functions\n",
        "def test_multiple_activation_functions(train, val, test, criterion, device, num_epochs=10):\n",
        "    activation_functions = [\"relu\", \"leaky_relu\", \"tanh\", \"sigmoid\", \"softmax\", \"elu\", \"selu\", \"gelu\", \"swish\", \"hard_sigmoid\"]\n",
        "    \n",
        "    best_model = None\n",
        "    best_accuracy = 0.0\n",
        "    best_loss = float(\"inf\")\n",
        "    \n",
        "    for activation_function in activation_functions:\n",
        "        print(f\"\\nTesting activation function: {activation_function}\")\n",
        "        \n",
        "        model = different_act_CNN(input_channels=1, num_classes=10, activation_function=activation_function)\n",
        "        model.to(device)\n",
        "        \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        best_gradients = train_model(model, train, val, criterion, optimizer, num_epochs, device)\n",
        "        \n",
        "        if best_gradients is not None:\n",
        "            with torch.no_grad():\n",
        "                for name, param in model.named_parameters():\n",
        "                    if name in best_gradients and param.grad is not None:\n",
        "                        param.grad.copy_(best_gradients[name])\n",
        "        \n",
        "        test_loss, accuracy = test_model_configuration(model, test, criterion, device)\n",
        "        \n",
        "        print(f\"Activation function: {activation_function} Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "        \n",
        "        if accuracy > best_accuracy or (accuracy == best_accuracy and test_loss < best_loss):\n",
        "            best_accuracy = accuracy\n",
        "            best_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            print(f\"New best model found with accuracy: {best_accuracy:.2f}% and loss: {best_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\nBest Model Test Accuracy: {best_accuracy:.2f}% with Loss: {best_loss:.4f}\")\n",
        "    return best_loss, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing activation function: relu\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 3.2699\n",
            "  Batch 20/35 - Loss: 2.2649\n",
            "  Batch 30/35 - Loss: 2.3546\n",
            "  Batch 35/35 - Loss: 2.2916\n",
            "Epoch 1 Training completed. Average Loss: 21.6914\n",
            "Epoch 1 Validation completed. Average Loss: 2.2721\n",
            "New best gradients stored for epoch 1 with validation loss 2.2721\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.3187\n",
            "  Batch 20/35 - Loss: 2.2398\n",
            "  Batch 30/35 - Loss: 2.2575\n",
            "  Batch 35/35 - Loss: 2.2021\n",
            "Epoch 2 Training completed. Average Loss: 2.1952\n",
            "Epoch 2 Validation completed. Average Loss: 2.2713\n",
            "New best gradients stored for epoch 2 with validation loss 2.2713\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.2286\n",
            "  Batch 20/35 - Loss: 2.0982\n",
            "  Batch 30/35 - Loss: 2.0084\n",
            "  Batch 35/35 - Loss: 1.9822\n",
            "Epoch 3 Training completed. Average Loss: 2.0515\n",
            "Epoch 3 Validation completed. Average Loss: 2.3029\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.5455\n",
            "  Batch 20/35 - Loss: 1.8817\n",
            "  Batch 30/35 - Loss: 1.7527\n",
            "  Batch 35/35 - Loss: 1.2315\n",
            "Epoch 4 Training completed. Average Loss: 1.6778\n",
            "Epoch 4 Validation completed. Average Loss: 3.4173\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 0.9574\n",
            "  Batch 20/35 - Loss: 1.6573\n",
            "  Batch 30/35 - Loss: 1.6491\n",
            "  Batch 35/35 - Loss: 1.0272\n",
            "Epoch 5 Training completed. Average Loss: 1.4218\n",
            "Epoch 5 Validation completed. Average Loss: 3.8778\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 0.5915\n",
            "  Batch 20/35 - Loss: 1.3256\n",
            "  Batch 30/35 - Loss: 1.4415\n",
            "  Batch 35/35 - Loss: 2.1186\n",
            "Epoch 6 Training completed. Average Loss: 1.3356\n",
            "Epoch 6 Validation completed. Average Loss: 4.1257\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.8701\n",
            "  Batch 20/35 - Loss: 1.0219\n",
            "  Batch 30/35 - Loss: 0.6543\n",
            "  Batch 35/35 - Loss: 0.5049\n",
            "Epoch 7 Training completed. Average Loss: 1.1026\n",
            "Epoch 7 Validation completed. Average Loss: 5.1482\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.5633\n",
            "  Batch 20/35 - Loss: 0.8118\n",
            "  Batch 30/35 - Loss: 0.4122\n",
            "  Batch 35/35 - Loss: 0.2068\n",
            "Epoch 8 Training completed. Average Loss: 0.6579\n",
            "Epoch 8 Validation completed. Average Loss: 6.7031\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.3390\n",
            "  Batch 20/35 - Loss: 0.5083\n",
            "  Batch 30/35 - Loss: 0.2692\n",
            "  Batch 35/35 - Loss: 0.1178\n",
            "Epoch 9 Training completed. Average Loss: 0.4536\n",
            "Epoch 9 Validation completed. Average Loss: 9.0759\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.2748\n",
            "  Batch 20/35 - Loss: 0.3638\n",
            "  Batch 30/35 - Loss: 0.2593\n",
            "  Batch 35/35 - Loss: 0.1163\n",
            "Epoch 10 Training completed. Average Loss: 0.3851\n",
            "Epoch 10 Validation completed. Average Loss: 9.7412\n",
            "\n",
            "Training complete.\n",
            "Activation function: relu Test Loss: 10.2309, Test Accuracy: 30.20%\n",
            "New best model found with accuracy: 30.20% and loss: 10.2309\n",
            "\n",
            "Testing activation function: leaky_relu\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 22.9754\n",
            "  Batch 20/35 - Loss: 4.7359\n",
            "  Batch 30/35 - Loss: 2.9961\n",
            "  Batch 35/35 - Loss: 4.0809\n",
            "Epoch 1 Training completed. Average Loss: 33.7516\n",
            "Epoch 1 Validation completed. Average Loss: 2.8606\n",
            "New best gradients stored for epoch 1 with validation loss 2.8606\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 3.2484\n",
            "  Batch 20/35 - Loss: 1.7942\n",
            "  Batch 30/35 - Loss: 1.9411\n",
            "  Batch 35/35 - Loss: 2.5169\n",
            "Epoch 2 Training completed. Average Loss: 2.5529\n",
            "Epoch 2 Validation completed. Average Loss: 2.3319\n",
            "New best gradients stored for epoch 2 with validation loss 2.3319\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.2159\n",
            "  Batch 20/35 - Loss: 1.9210\n",
            "  Batch 30/35 - Loss: 2.0006\n",
            "  Batch 35/35 - Loss: 1.5063\n",
            "Epoch 3 Training completed. Average Loss: 2.0328\n",
            "Epoch 3 Validation completed. Average Loss: 2.2910\n",
            "New best gradients stored for epoch 3 with validation loss 2.2910\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.4276\n",
            "  Batch 20/35 - Loss: 1.3466\n",
            "  Batch 30/35 - Loss: 1.5123\n",
            "  Batch 35/35 - Loss: 0.9421\n",
            "Epoch 4 Training completed. Average Loss: 1.5382\n",
            "Epoch 4 Validation completed. Average Loss: 2.0867\n",
            "New best gradients stored for epoch 4 with validation loss 2.0867\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 1.2340\n",
            "  Batch 20/35 - Loss: 1.0357\n",
            "  Batch 30/35 - Loss: 1.1943\n",
            "  Batch 35/35 - Loss: 0.7821\n",
            "Epoch 5 Training completed. Average Loss: 1.2587\n",
            "Epoch 5 Validation completed. Average Loss: 2.5236\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 0.8148\n",
            "  Batch 20/35 - Loss: 1.1920\n",
            "  Batch 30/35 - Loss: 1.1096\n",
            "  Batch 35/35 - Loss: 0.5165\n",
            "Epoch 6 Training completed. Average Loss: 1.0498\n",
            "Epoch 6 Validation completed. Average Loss: 3.0001\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.4840\n",
            "  Batch 20/35 - Loss: 0.7893\n",
            "  Batch 30/35 - Loss: 1.2295\n",
            "  Batch 35/35 - Loss: 0.3954\n",
            "Epoch 7 Training completed. Average Loss: 0.8659\n",
            "Epoch 7 Validation completed. Average Loss: 3.0972\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.9366\n",
            "  Batch 20/35 - Loss: 0.5461\n",
            "  Batch 30/35 - Loss: 0.5622\n",
            "  Batch 35/35 - Loss: 0.2986\n",
            "Epoch 8 Training completed. Average Loss: 0.7897\n",
            "Epoch 8 Validation completed. Average Loss: 3.6775\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.7229\n",
            "  Batch 20/35 - Loss: 1.3046\n",
            "  Batch 30/35 - Loss: 0.9927\n",
            "  Batch 35/35 - Loss: 0.1454\n",
            "Epoch 9 Training completed. Average Loss: 0.7842\n",
            "Epoch 9 Validation completed. Average Loss: 4.9954\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.2694\n",
            "  Batch 20/35 - Loss: 1.6727\n",
            "  Batch 30/35 - Loss: 0.4338\n",
            "  Batch 35/35 - Loss: 0.5422\n",
            "Epoch 10 Training completed. Average Loss: 0.8361\n",
            "Epoch 10 Validation completed. Average Loss: 4.0900\n",
            "\n",
            "Training complete.\n",
            "Activation function: leaky_relu Test Loss: 3.8460, Test Accuracy: 34.23%\n",
            "New best model found with accuracy: 34.23% and loss: 3.8460\n",
            "\n",
            "Testing activation function: tanh\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.2881\n",
            "  Batch 20/35 - Loss: 2.2584\n",
            "  Batch 30/35 - Loss: 2.1827\n",
            "  Batch 35/35 - Loss: 2.0870\n",
            "Epoch 1 Training completed. Average Loss: 2.2160\n",
            "Epoch 1 Validation completed. Average Loss: 2.1112\n",
            "New best gradients stored for epoch 1 with validation loss 2.1112\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.2834\n",
            "  Batch 20/35 - Loss: 2.2025\n",
            "  Batch 30/35 - Loss: 2.1176\n",
            "  Batch 35/35 - Loss: 2.0351\n",
            "Epoch 2 Training completed. Average Loss: 2.0584\n",
            "Epoch 2 Validation completed. Average Loss: 2.0397\n",
            "New best gradients stored for epoch 2 with validation loss 2.0397\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.1989\n",
            "  Batch 20/35 - Loss: 2.2448\n",
            "  Batch 30/35 - Loss: 2.0763\n",
            "  Batch 35/35 - Loss: 2.2039\n",
            "Epoch 3 Training completed. Average Loss: 2.0310\n",
            "Epoch 3 Validation completed. Average Loss: 2.0381\n",
            "New best gradients stored for epoch 3 with validation loss 2.0381\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 2.0733\n",
            "  Batch 20/35 - Loss: 2.1544\n",
            "  Batch 30/35 - Loss: 2.0782\n",
            "  Batch 35/35 - Loss: 2.1874\n",
            "Epoch 4 Training completed. Average Loss: 2.0083\n",
            "Epoch 4 Validation completed. Average Loss: 1.9850\n",
            "New best gradients stored for epoch 4 with validation loss 1.9850\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 2.1506\n",
            "  Batch 20/35 - Loss: 1.9339\n",
            "  Batch 30/35 - Loss: 2.0273\n",
            "  Batch 35/35 - Loss: 2.2310\n",
            "Epoch 5 Training completed. Average Loss: 2.0292\n",
            "Epoch 5 Validation completed. Average Loss: 1.9788\n",
            "New best gradients stored for epoch 5 with validation loss 1.9788\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 1.9633\n",
            "  Batch 20/35 - Loss: 1.9893\n",
            "  Batch 30/35 - Loss: 2.0164\n",
            "  Batch 35/35 - Loss: 2.0408\n",
            "Epoch 6 Training completed. Average Loss: 1.9554\n",
            "Epoch 6 Validation completed. Average Loss: 1.9407\n",
            "New best gradients stored for epoch 6 with validation loss 1.9407\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 1.9959\n",
            "  Batch 20/35 - Loss: 2.1264\n",
            "  Batch 30/35 - Loss: 1.9868\n",
            "  Batch 35/35 - Loss: 2.1156\n",
            "Epoch 7 Training completed. Average Loss: 1.9219\n",
            "Epoch 7 Validation completed. Average Loss: 1.9346\n",
            "New best gradients stored for epoch 7 with validation loss 1.9346\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 1.9438\n",
            "  Batch 20/35 - Loss: 2.1152\n",
            "  Batch 30/35 - Loss: 1.9817\n",
            "  Batch 35/35 - Loss: 2.2361\n",
            "Epoch 8 Training completed. Average Loss: 1.9227\n",
            "Epoch 8 Validation completed. Average Loss: 2.1692\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 1.9763\n",
            "  Batch 20/35 - Loss: 1.9145\n",
            "  Batch 30/35 - Loss: 2.0276\n",
            "  Batch 35/35 - Loss: 2.1252\n",
            "Epoch 9 Training completed. Average Loss: 1.9617\n",
            "Epoch 9 Validation completed. Average Loss: 1.9339\n",
            "New best gradients stored for epoch 9 with validation loss 1.9339\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 1.9363\n",
            "  Batch 20/35 - Loss: 2.0160\n",
            "  Batch 30/35 - Loss: 1.9974\n",
            "  Batch 35/35 - Loss: 2.0342\n",
            "Epoch 10 Training completed. Average Loss: 1.9101\n",
            "Epoch 10 Validation completed. Average Loss: 1.9261\n",
            "New best gradients stored for epoch 10 with validation loss 1.9261\n",
            "\n",
            "Training complete.\n",
            "Activation function: tanh Test Loss: 1.8843, Test Accuracy: 32.89%\n",
            "\n",
            "Testing activation function: sigmoid\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.3541\n",
            "  Batch 20/35 - Loss: 2.3309\n",
            "  Batch 30/35 - Loss: 2.3056\n",
            "  Batch 35/35 - Loss: 2.3350\n",
            "Epoch 1 Training completed. Average Loss: 2.3194\n",
            "Epoch 1 Validation completed. Average Loss: 2.3032\n",
            "New best gradients stored for epoch 1 with validation loss 2.3032\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.3144\n",
            "  Batch 20/35 - Loss: 2.3074\n",
            "  Batch 30/35 - Loss: 2.3070\n",
            "  Batch 35/35 - Loss: 2.3083\n",
            "Epoch 2 Training completed. Average Loss: 2.3087\n",
            "Epoch 2 Validation completed. Average Loss: 2.3020\n",
            "New best gradients stored for epoch 2 with validation loss 2.3020\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.3177\n",
            "  Batch 20/35 - Loss: 2.3065\n",
            "  Batch 30/35 - Loss: 2.3073\n",
            "  Batch 35/35 - Loss: 2.3100\n",
            "Epoch 3 Training completed. Average Loss: 2.3090\n",
            "Epoch 3 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 2.3169\n",
            "  Batch 20/35 - Loss: 2.3070\n",
            "  Batch 30/35 - Loss: 2.3071\n",
            "  Batch 35/35 - Loss: 2.3100\n",
            "Epoch 4 Training completed. Average Loss: 2.3088\n",
            "Epoch 4 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 2.3167\n",
            "  Batch 20/35 - Loss: 2.3068\n",
            "  Batch 30/35 - Loss: 2.3071\n",
            "  Batch 35/35 - Loss: 2.3097\n",
            "Epoch 5 Training completed. Average Loss: 2.3088\n",
            "Epoch 5 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 2.3164\n",
            "  Batch 20/35 - Loss: 2.3067\n",
            "  Batch 30/35 - Loss: 2.3071\n",
            "  Batch 35/35 - Loss: 2.3094\n",
            "Epoch 6 Training completed. Average Loss: 2.3087\n",
            "Epoch 6 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 2.3162\n",
            "  Batch 20/35 - Loss: 2.3065\n",
            "  Batch 30/35 - Loss: 2.3070\n",
            "  Batch 35/35 - Loss: 2.3092\n",
            "Epoch 7 Training completed. Average Loss: 2.3086\n",
            "Epoch 7 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 2.3159\n",
            "  Batch 20/35 - Loss: 2.3064\n",
            "  Batch 30/35 - Loss: 2.3070\n",
            "  Batch 35/35 - Loss: 2.3089\n",
            "Epoch 8 Training completed. Average Loss: 2.3085\n",
            "Epoch 8 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 2.3157\n",
            "  Batch 20/35 - Loss: 2.3063\n",
            "  Batch 30/35 - Loss: 2.3070\n",
            "  Batch 35/35 - Loss: 2.3087\n",
            "Epoch 9 Training completed. Average Loss: 2.3085\n",
            "Epoch 9 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 2.3154\n",
            "  Batch 20/35 - Loss: 2.3062\n",
            "  Batch 30/35 - Loss: 2.3070\n",
            "  Batch 35/35 - Loss: 2.3085\n",
            "Epoch 10 Training completed. Average Loss: 2.3084\n",
            "Epoch 10 Validation completed. Average Loss: 2.3021\n",
            "\n",
            "Training complete.\n",
            "Activation function: sigmoid Test Loss: 2.3029, Test Accuracy: 10.07%\n",
            "\n",
            "Testing activation function: softmax\n",
            "\n",
            "Starting Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\45235544\\AppData\\Local\\Temp\\ipykernel_17480\\3811288260.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.activation(conv_layer(x))\n",
            "C:\\Users\\45235544\\AppData\\Local\\Temp\\ipykernel_17480\\3811288260.py:81: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.activation(fc_layer(x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch 10/35 - Loss: 2.3005\n",
            "  Batch 20/35 - Loss: 2.3043\n",
            "  Batch 30/35 - Loss: 2.3177\n",
            "  Batch 35/35 - Loss: 2.3132\n",
            "Epoch 1 Training completed. Average Loss: 2.3041\n",
            "Epoch 1 Validation completed. Average Loss: 2.3033\n",
            "New best gradients stored for epoch 1 with validation loss 2.3033\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.2997\n",
            "  Batch 20/35 - Loss: 2.3035\n",
            "  Batch 30/35 - Loss: 2.3161\n",
            "  Batch 35/35 - Loss: 2.3108\n",
            "Epoch 2 Training completed. Average Loss: 2.3036\n",
            "Epoch 2 Validation completed. Average Loss: 2.3032\n",
            "New best gradients stored for epoch 2 with validation loss 2.3032\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.2996\n",
            "  Batch 20/35 - Loss: 2.3027\n",
            "  Batch 30/35 - Loss: 2.3146\n",
            "  Batch 35/35 - Loss: 2.3086\n",
            "Epoch 3 Training completed. Average Loss: 2.3034\n",
            "Epoch 3 Validation completed. Average Loss: 2.3030\n",
            "New best gradients stored for epoch 3 with validation loss 2.3030\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 2.2995\n",
            "  Batch 20/35 - Loss: 2.3021\n",
            "  Batch 30/35 - Loss: 2.3134\n",
            "  Batch 35/35 - Loss: 2.3067\n",
            "Epoch 4 Training completed. Average Loss: 2.3033\n",
            "Epoch 4 Validation completed. Average Loss: 2.3029\n",
            "New best gradients stored for epoch 4 with validation loss 2.3029\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 2.2994\n",
            "  Batch 20/35 - Loss: 2.3015\n",
            "  Batch 30/35 - Loss: 2.3123\n",
            "  Batch 35/35 - Loss: 2.3051\n",
            "Epoch 5 Training completed. Average Loss: 2.3032\n",
            "Epoch 5 Validation completed. Average Loss: 2.3028\n",
            "New best gradients stored for epoch 5 with validation loss 2.3028\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 2.2994\n",
            "  Batch 20/35 - Loss: 2.3009\n",
            "  Batch 30/35 - Loss: 2.3113\n",
            "  Batch 35/35 - Loss: 2.3035\n",
            "Epoch 6 Training completed. Average Loss: 2.3031\n",
            "Epoch 6 Validation completed. Average Loss: 2.3028\n",
            "New best gradients stored for epoch 6 with validation loss 2.3028\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 2.2994\n",
            "  Batch 20/35 - Loss: 2.3005\n",
            "  Batch 30/35 - Loss: 2.3104\n",
            "  Batch 35/35 - Loss: 2.3022\n",
            "Epoch 7 Training completed. Average Loss: 2.3030\n",
            "Epoch 7 Validation completed. Average Loss: 2.3027\n",
            "New best gradients stored for epoch 7 with validation loss 2.3027\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 2.2993\n",
            "  Batch 20/35 - Loss: 2.3001\n",
            "  Batch 30/35 - Loss: 2.3097\n",
            "  Batch 35/35 - Loss: 2.3010\n",
            "Epoch 8 Training completed. Average Loss: 2.3030\n",
            "Epoch 8 Validation completed. Average Loss: 2.3027\n",
            "New best gradients stored for epoch 8 with validation loss 2.3027\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 2.2993\n",
            "  Batch 20/35 - Loss: 2.2997\n",
            "  Batch 30/35 - Loss: 2.3091\n",
            "  Batch 35/35 - Loss: 2.2999\n",
            "Epoch 9 Training completed. Average Loss: 2.3029\n",
            "Epoch 9 Validation completed. Average Loss: 2.3027\n",
            "New best gradients stored for epoch 9 with validation loss 2.3027\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 2.2993\n",
            "  Batch 20/35 - Loss: 2.2994\n",
            "  Batch 30/35 - Loss: 2.3085\n",
            "  Batch 35/35 - Loss: 2.2990\n",
            "Epoch 10 Training completed. Average Loss: 2.3029\n",
            "Epoch 10 Validation completed. Average Loss: 2.3027\n",
            "New best gradients stored for epoch 10 with validation loss 2.3027\n",
            "\n",
            "Training complete.\n",
            "Activation function: softmax Test Loss: 2.3027, Test Accuracy: 10.07%\n",
            "\n",
            "Testing activation function: elu\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 10.3086\n",
            "  Batch 20/35 - Loss: 4.2021\n",
            "  Batch 30/35 - Loss: 3.6937\n",
            "  Batch 35/35 - Loss: 2.4436\n",
            "Epoch 1 Training completed. Average Loss: 38.1908\n",
            "Epoch 1 Validation completed. Average Loss: 2.6221\n",
            "New best gradients stored for epoch 1 with validation loss 2.6221\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.3422\n",
            "  Batch 20/35 - Loss: 2.4778\n",
            "  Batch 30/35 - Loss: 2.1584\n",
            "  Batch 35/35 - Loss: 1.8230\n",
            "Epoch 2 Training completed. Average Loss: 2.4055\n",
            "Epoch 2 Validation completed. Average Loss: 2.2230\n",
            "New best gradients stored for epoch 2 with validation loss 2.2230\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 1.6133\n",
            "  Batch 20/35 - Loss: 2.2189\n",
            "  Batch 30/35 - Loss: 1.8356\n",
            "  Batch 35/35 - Loss: 0.7182\n",
            "Epoch 3 Training completed. Average Loss: 1.6086\n",
            "Epoch 3 Validation completed. Average Loss: 2.3885\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 0.7547\n",
            "  Batch 20/35 - Loss: 1.2959\n",
            "  Batch 30/35 - Loss: 0.9838\n",
            "  Batch 35/35 - Loss: 0.7835\n",
            "Epoch 4 Training completed. Average Loss: 1.1334\n",
            "Epoch 4 Validation completed. Average Loss: 2.3730\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 0.3007\n",
            "  Batch 20/35 - Loss: 0.5794\n",
            "  Batch 30/35 - Loss: 1.2899\n",
            "  Batch 35/35 - Loss: 0.3010\n",
            "Epoch 5 Training completed. Average Loss: 0.7160\n",
            "Epoch 5 Validation completed. Average Loss: 2.8821\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 0.5237\n",
            "  Batch 20/35 - Loss: 0.5097\n",
            "  Batch 30/35 - Loss: 0.5795\n",
            "  Batch 35/35 - Loss: 0.0940\n",
            "Epoch 6 Training completed. Average Loss: 0.5889\n",
            "Epoch 6 Validation completed. Average Loss: 3.0815\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.1844\n",
            "  Batch 20/35 - Loss: 0.4656\n",
            "  Batch 30/35 - Loss: 0.4599\n",
            "  Batch 35/35 - Loss: 0.0604\n",
            "Epoch 7 Training completed. Average Loss: 0.3221\n",
            "Epoch 7 Validation completed. Average Loss: 3.5668\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.3699\n",
            "  Batch 20/35 - Loss: 0.3632\n",
            "  Batch 30/35 - Loss: 0.4692\n",
            "  Batch 35/35 - Loss: 0.4659\n",
            "Epoch 8 Training completed. Average Loss: 0.2724\n",
            "Epoch 8 Validation completed. Average Loss: 4.1636\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.1004\n",
            "  Batch 20/35 - Loss: 0.8277\n",
            "  Batch 30/35 - Loss: 0.1369\n",
            "  Batch 35/35 - Loss: 0.0321\n",
            "Epoch 9 Training completed. Average Loss: 0.1736\n",
            "Epoch 9 Validation completed. Average Loss: 4.3404\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.0843\n",
            "  Batch 20/35 - Loss: 0.0844\n",
            "  Batch 30/35 - Loss: 0.1343\n",
            "  Batch 35/35 - Loss: 0.8213\n",
            "Epoch 10 Training completed. Average Loss: 0.1673\n",
            "Epoch 10 Validation completed. Average Loss: 5.0153\n",
            "\n",
            "Training complete.\n",
            "Activation function: elu Test Loss: 4.5181, Test Accuracy: 43.62%\n",
            "New best model found with accuracy: 43.62% and loss: 4.5181\n",
            "\n",
            "Testing activation function: selu\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 27.0401\n",
            "  Batch 20/35 - Loss: 7.1696\n",
            "  Batch 30/35 - Loss: 4.2756\n",
            "  Batch 35/35 - Loss: 3.8276\n",
            "Epoch 1 Training completed. Average Loss: 70.8258\n",
            "Epoch 1 Validation completed. Average Loss: 3.6564\n",
            "New best gradients stored for epoch 1 with validation loss 3.6564\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.4031\n",
            "  Batch 20/35 - Loss: 3.3099\n",
            "  Batch 30/35 - Loss: 3.2782\n",
            "  Batch 35/35 - Loss: 1.7633\n",
            "Epoch 2 Training completed. Average Loss: 2.9257\n",
            "Epoch 2 Validation completed. Average Loss: 3.6054\n",
            "New best gradients stored for epoch 2 with validation loss 3.6054\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.2729\n",
            "  Batch 20/35 - Loss: 3.0488\n",
            "  Batch 30/35 - Loss: 1.8013\n",
            "  Batch 35/35 - Loss: 1.9527\n",
            "Epoch 3 Training completed. Average Loss: 2.1623\n",
            "Epoch 3 Validation completed. Average Loss: 2.9903\n",
            "New best gradients stored for epoch 3 with validation loss 2.9903\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.8860\n",
            "  Batch 20/35 - Loss: 1.0184\n",
            "  Batch 30/35 - Loss: 1.0070\n",
            "  Batch 35/35 - Loss: 0.9742\n",
            "Epoch 4 Training completed. Average Loss: 1.7195\n",
            "Epoch 4 Validation completed. Average Loss: 3.0003\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 1.3564\n",
            "  Batch 20/35 - Loss: 1.4117\n",
            "  Batch 30/35 - Loss: 0.6921\n",
            "  Batch 35/35 - Loss: 0.5968\n",
            "Epoch 5 Training completed. Average Loss: 1.1886\n",
            "Epoch 5 Validation completed. Average Loss: 2.8313\n",
            "New best gradients stored for epoch 5 with validation loss 2.8313\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 1.4475\n",
            "  Batch 20/35 - Loss: 1.3699\n",
            "  Batch 30/35 - Loss: 0.7197\n",
            "  Batch 35/35 - Loss: 0.6309\n",
            "Epoch 6 Training completed. Average Loss: 0.9439\n",
            "Epoch 6 Validation completed. Average Loss: 2.7710\n",
            "New best gradients stored for epoch 6 with validation loss 2.7710\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.8965\n",
            "  Batch 20/35 - Loss: 1.2500\n",
            "  Batch 30/35 - Loss: 0.6637\n",
            "  Batch 35/35 - Loss: 0.8120\n",
            "Epoch 7 Training completed. Average Loss: 0.7638\n",
            "Epoch 7 Validation completed. Average Loss: 3.4441\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 1.0777\n",
            "  Batch 20/35 - Loss: 1.1923\n",
            "  Batch 30/35 - Loss: 0.5992\n",
            "  Batch 35/35 - Loss: 1.2638\n",
            "Epoch 8 Training completed. Average Loss: 0.7172\n",
            "Epoch 8 Validation completed. Average Loss: 3.2810\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.7276\n",
            "  Batch 20/35 - Loss: 0.4563\n",
            "  Batch 30/35 - Loss: 0.3433\n",
            "  Batch 35/35 - Loss: 0.4205\n",
            "Epoch 9 Training completed. Average Loss: 0.5762\n",
            "Epoch 9 Validation completed. Average Loss: 3.6609\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.3302\n",
            "  Batch 20/35 - Loss: 0.5898\n",
            "  Batch 30/35 - Loss: 0.8102\n",
            "  Batch 35/35 - Loss: 0.6327\n",
            "Epoch 10 Training completed. Average Loss: 0.7089\n",
            "Epoch 10 Validation completed. Average Loss: 3.8454\n",
            "\n",
            "Training complete.\n",
            "Activation function: selu Test Loss: 3.7628, Test Accuracy: 36.91%\n",
            "\n",
            "Testing activation function: gelu\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.3145\n",
            "  Batch 20/35 - Loss: 2.4704\n",
            "  Batch 30/35 - Loss: 2.4524\n",
            "  Batch 35/35 - Loss: 2.2415\n",
            "Epoch 1 Training completed. Average Loss: 22.2179\n",
            "Epoch 1 Validation completed. Average Loss: 2.2204\n",
            "New best gradients stored for epoch 1 with validation loss 2.2204\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.3507\n",
            "  Batch 20/35 - Loss: 1.9704\n",
            "  Batch 30/35 - Loss: 1.7618\n",
            "  Batch 35/35 - Loss: 1.4983\n",
            "Epoch 2 Training completed. Average Loss: 1.8879\n",
            "Epoch 2 Validation completed. Average Loss: 2.2843\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 1.8005\n",
            "  Batch 20/35 - Loss: 1.6011\n",
            "  Batch 30/35 - Loss: 1.2406\n",
            "  Batch 35/35 - Loss: 0.8417\n",
            "Epoch 3 Training completed. Average Loss: 1.2327\n",
            "Epoch 3 Validation completed. Average Loss: 2.7764\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.0929\n",
            "  Batch 20/35 - Loss: 0.9610\n",
            "  Batch 30/35 - Loss: 0.4919\n",
            "  Batch 35/35 - Loss: 0.7871\n",
            "Epoch 4 Training completed. Average Loss: 0.7672\n",
            "Epoch 4 Validation completed. Average Loss: 3.4037\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 0.9061\n",
            "  Batch 20/35 - Loss: 0.5053\n",
            "  Batch 30/35 - Loss: 0.2770\n",
            "  Batch 35/35 - Loss: 0.3520\n",
            "Epoch 5 Training completed. Average Loss: 0.5117\n",
            "Epoch 5 Validation completed. Average Loss: 3.5534\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 0.6975\n",
            "  Batch 20/35 - Loss: 0.2894\n",
            "  Batch 30/35 - Loss: 0.2901\n",
            "  Batch 35/35 - Loss: 0.0537\n",
            "Epoch 6 Training completed. Average Loss: 0.4464\n",
            "Epoch 6 Validation completed. Average Loss: 4.8097\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.4447\n",
            "  Batch 20/35 - Loss: 1.0960\n",
            "  Batch 30/35 - Loss: 1.3118\n",
            "  Batch 35/35 - Loss: 1.1742\n",
            "Epoch 7 Training completed. Average Loss: 0.6289\n",
            "Epoch 7 Validation completed. Average Loss: 3.3159\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 1.3166\n",
            "  Batch 20/35 - Loss: 0.1982\n",
            "  Batch 30/35 - Loss: 0.6792\n",
            "  Batch 35/35 - Loss: 0.2780\n",
            "Epoch 8 Training completed. Average Loss: 0.6697\n",
            "Epoch 8 Validation completed. Average Loss: 4.7479\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 1.1971\n",
            "  Batch 20/35 - Loss: 0.4351\n",
            "  Batch 30/35 - Loss: 0.0755\n",
            "  Batch 35/35 - Loss: 1.2233\n",
            "Epoch 9 Training completed. Average Loss: 0.4682\n",
            "Epoch 9 Validation completed. Average Loss: 5.0957\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.4917\n",
            "  Batch 20/35 - Loss: 0.0742\n",
            "  Batch 30/35 - Loss: 0.0897\n",
            "  Batch 35/35 - Loss: 0.0550\n",
            "Epoch 10 Training completed. Average Loss: 0.1550\n",
            "Epoch 10 Validation completed. Average Loss: 5.0309\n",
            "\n",
            "Training complete.\n",
            "Activation function: gelu Test Loss: 6.3114, Test Accuracy: 30.20%\n",
            "\n",
            "Testing activation function: swish\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.9877\n",
            "  Batch 20/35 - Loss: 2.4516\n",
            "  Batch 30/35 - Loss: 2.3071\n",
            "  Batch 35/35 - Loss: 2.3362\n",
            "Epoch 1 Training completed. Average Loss: 34.5935\n",
            "Epoch 1 Validation completed. Average Loss: 2.1441\n",
            "New best gradients stored for epoch 1 with validation loss 2.1441\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.2932\n",
            "  Batch 20/35 - Loss: 2.2445\n",
            "  Batch 30/35 - Loss: 1.8735\n",
            "  Batch 35/35 - Loss: 1.8966\n",
            "Epoch 2 Training completed. Average Loss: 1.8658\n",
            "Epoch 2 Validation completed. Average Loss: 2.0389\n",
            "New best gradients stored for epoch 2 with validation loss 2.0389\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 1.9000\n",
            "  Batch 20/35 - Loss: 1.9567\n",
            "  Batch 30/35 - Loss: 1.8054\n",
            "  Batch 35/35 - Loss: 1.2434\n",
            "Epoch 3 Training completed. Average Loss: 1.4546\n",
            "Epoch 3 Validation completed. Average Loss: 2.2366\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 1.2274\n",
            "  Batch 20/35 - Loss: 1.5921\n",
            "  Batch 30/35 - Loss: 1.4744\n",
            "  Batch 35/35 - Loss: 1.1568\n",
            "Epoch 4 Training completed. Average Loss: 1.1632\n",
            "Epoch 4 Validation completed. Average Loss: 2.1922\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 0.7918\n",
            "  Batch 20/35 - Loss: 0.8922\n",
            "  Batch 30/35 - Loss: 1.2041\n",
            "  Batch 35/35 - Loss: 0.3253\n",
            "Epoch 5 Training completed. Average Loss: 0.7373\n",
            "Epoch 5 Validation completed. Average Loss: 2.4780\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 0.3548\n",
            "  Batch 20/35 - Loss: 0.6977\n",
            "  Batch 30/35 - Loss: 0.6879\n",
            "  Batch 35/35 - Loss: 0.8287\n",
            "Epoch 6 Training completed. Average Loss: 0.5340\n",
            "Epoch 6 Validation completed. Average Loss: 3.1922\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 0.6404\n",
            "  Batch 20/35 - Loss: 0.6113\n",
            "  Batch 30/35 - Loss: 0.2556\n",
            "  Batch 35/35 - Loss: 0.1397\n",
            "Epoch 7 Training completed. Average Loss: 0.5647\n",
            "Epoch 7 Validation completed. Average Loss: 3.6062\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 0.3180\n",
            "  Batch 20/35 - Loss: 0.1897\n",
            "  Batch 30/35 - Loss: 0.2375\n",
            "  Batch 35/35 - Loss: 0.0254\n",
            "Epoch 8 Training completed. Average Loss: 0.2644\n",
            "Epoch 8 Validation completed. Average Loss: 3.8634\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 0.1627\n",
            "  Batch 20/35 - Loss: 0.1039\n",
            "  Batch 30/35 - Loss: 0.0429\n",
            "  Batch 35/35 - Loss: 0.0912\n",
            "Epoch 9 Training completed. Average Loss: 0.2186\n",
            "Epoch 9 Validation completed. Average Loss: 3.9180\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 0.1511\n",
            "  Batch 20/35 - Loss: 0.0940\n",
            "  Batch 30/35 - Loss: 0.0403\n",
            "  Batch 35/35 - Loss: 0.2749\n",
            "Epoch 10 Training completed. Average Loss: 0.1090\n",
            "Epoch 10 Validation completed. Average Loss: 3.7273\n",
            "\n",
            "Training complete.\n",
            "Activation function: swish Test Loss: 4.3563, Test Accuracy: 30.20%\n",
            "\n",
            "Testing activation function: hard_sigmoid\n",
            "\n",
            "Starting Epoch 1/10\n",
            "  Batch 10/35 - Loss: 2.2921\n",
            "  Batch 20/35 - Loss: 2.3235\n",
            "  Batch 30/35 - Loss: 2.3077\n",
            "  Batch 35/35 - Loss: 2.3181\n",
            "Epoch 1 Training completed. Average Loss: 2.3158\n",
            "Epoch 1 Validation completed. Average Loss: 2.3022\n",
            "New best gradients stored for epoch 1 with validation loss 2.3022\n",
            "\n",
            "Starting Epoch 2/10\n",
            "  Batch 10/35 - Loss: 2.3164\n",
            "  Batch 20/35 - Loss: 2.3052\n",
            "  Batch 30/35 - Loss: 2.3067\n",
            "  Batch 35/35 - Loss: 2.3089\n",
            "Epoch 2 Training completed. Average Loss: 2.3082\n",
            "Epoch 2 Validation completed. Average Loss: 2.3021\n",
            "New best gradients stored for epoch 2 with validation loss 2.3021\n",
            "\n",
            "Starting Epoch 3/10\n",
            "  Batch 10/35 - Loss: 2.3140\n",
            "  Batch 20/35 - Loss: 2.3067\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3080\n",
            "Epoch 3 Training completed. Average Loss: 2.3082\n",
            "Epoch 3 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 4/10\n",
            "  Batch 10/35 - Loss: 2.3134\n",
            "  Batch 20/35 - Loss: 2.3063\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3081\n",
            "Epoch 4 Training completed. Average Loss: 2.3082\n",
            "Epoch 4 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 5/10\n",
            "  Batch 10/35 - Loss: 2.3133\n",
            "  Batch 20/35 - Loss: 2.3063\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3080\n",
            "Epoch 5 Training completed. Average Loss: 2.3081\n",
            "Epoch 5 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 6/10\n",
            "  Batch 10/35 - Loss: 2.3132\n",
            "  Batch 20/35 - Loss: 2.3062\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3079\n",
            "Epoch 6 Training completed. Average Loss: 2.3081\n",
            "Epoch 6 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 7/10\n",
            "  Batch 10/35 - Loss: 2.3131\n",
            "  Batch 20/35 - Loss: 2.3062\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3077\n",
            "Epoch 7 Training completed. Average Loss: 2.3081\n",
            "Epoch 7 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 8/10\n",
            "  Batch 10/35 - Loss: 2.3130\n",
            "  Batch 20/35 - Loss: 2.3061\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3076\n",
            "Epoch 8 Training completed. Average Loss: 2.3080\n",
            "Epoch 8 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 9/10\n",
            "  Batch 10/35 - Loss: 2.3129\n",
            "  Batch 20/35 - Loss: 2.3060\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3075\n",
            "Epoch 9 Training completed. Average Loss: 2.3080\n",
            "Epoch 9 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Starting Epoch 10/10\n",
            "  Batch 10/35 - Loss: 2.3127\n",
            "  Batch 20/35 - Loss: 2.3059\n",
            "  Batch 30/35 - Loss: 2.3065\n",
            "  Batch 35/35 - Loss: 2.3074\n",
            "Epoch 10 Training completed. Average Loss: 2.3080\n",
            "Epoch 10 Validation completed. Average Loss: 2.3022\n",
            "\n",
            "Training complete.\n",
            "Activation function: hard_sigmoid Test Loss: 2.3029, Test Accuracy: 10.07%\n",
            "\n",
            "Best Model Test Accuracy: 43.62% with Loss: 4.5181\n"
          ]
        }
      ],
      "source": [
        "# Assume 'test_loader' is the DataLoader for your test set, and 'criterion' is the loss function (e.g., CrossEntropyLoss)\n",
        "best_loss, best_model = test_multiple_activation_functions(train_spectogram, val_spectogram, test_spectogram, criterion, device, 10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TD6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
