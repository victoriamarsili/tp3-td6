{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad Torcuato Di Tella\n",
    "\n",
    "Licenciatura en Tecnología Digital\\\n",
    "**Tecnología Digital VI: Inteligencia Artificial**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import tarfile\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TP3: Encodeador de música\n",
    "\n",
    "\n",
    "\n",
    "## Orden de pasos\n",
    "\n",
    "0. Elijan GPU para que corra mas rapido (RAM --> change runtime type --> T4 GPU)\n",
    "1. Descargamos el dataset y lo descomprimimos en alguna carpeta en nuestro drive.\n",
    "2. Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos.\n",
    "3. Visualización de los archivos\n",
    "4. Clasificación\n",
    "5. Evaluación\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: jaimeamigo (sansonmariano-universidad-torcuato-di-tella). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\felip\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Escritorio\\TD6\\tp3-td6\\wandb\\run-20241107_175043-2mhyfp20</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/2mhyfp20' target=\"_blank\">fiery-sky-71</a></strong> to <a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6' target=\"_blank\">https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/2mhyfp20' target=\"_blank\">https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/2mhyfp20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sansonmariano-universidad-torcuato-di-tella/TP3-TD6/runs/2mhyfp20?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1e90fb7dac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name='TP3-TD6'\n",
    "username = \"sansonmariano-universidad-torcuato-di-tella\"\n",
    "wandb.login(key=\"d2875c91a36209496ee81454cccd95ebe3dc948d\")\n",
    "wandb.init(project = project_name, entity = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Definir parámetros\n",
    "samplerate = 22050\n",
    "data_dir = './genres_5sec'\n",
    "\n",
    "init_batch_size = 20\n",
    "init_num_epochs = 10\n",
    "init_lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para parsear géneros\n",
    "def parse_genres(fname):\n",
    "    parts = fname.split('/')[-1].split('.')[0]\n",
    "    return parts\n",
    "\n",
    "# Definir la clase del dataset\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files = []\n",
    "        for c in os.listdir(root):\n",
    "            self.files += [os.path.join(root, c, fname) for fname in os.listdir(os.path.join(root, c)) if fname.endswith('.wav')]\n",
    "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fpath = self.files[idx]\n",
    "        genre = parse_genres(fpath)\n",
    "        class_idx = self.classes.index(genre)\n",
    "        audio = torchaudio.load(fpath)[0]\n",
    "        return audio, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Function to normalize the audio data across the dataset\n",
    "def normalize_audio_data(dataset):\n",
    "    \"\"\"\n",
    "    Normalize the dataset by calculating the mean and standard deviation\n",
    "    of all audio samples and then applying standardization.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        audio, _ = dataset[i]\n",
    "        all_data.append(audio)\n",
    "    stacked_data = torch.cat(all_data, dim=1)  # Concatenate along the time dimension for mean/std calculation\n",
    "    mean = stacked_data.mean()\n",
    "    std = stacked_data.std()\n",
    "\n",
    "    # Apply normalization to each sample\n",
    "    normalized_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        audio, label = dataset[i]\n",
    "        normalized_audio = (audio - mean) / std\n",
    "        normalized_data.append((normalized_audio, label))\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# Function to create stratified train, validation, and test DataLoaders\n",
    "def create_dataloaders(dataset, batch_size, test_size=0.3, val_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Normalizes the dataset, splits it into train, validation, and test subsets,\n",
    "    and returns corresponding DataLoaders.\n",
    "    \"\"\"\n",
    "    # Normalize the dataset\n",
    "    normalized_data = normalize_audio_data(dataset)\n",
    "\n",
    "    # Extract labels for stratified splitting\n",
    "    labels = [label for _, label in normalized_data]\n",
    "\n",
    "    # Stratified split: train and temporary (val+test) split\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    for train_idx, temp_idx in split.split(range(len(normalized_data)), labels):\n",
    "        train_dataset = Subset(normalized_data, train_idx)\n",
    "        temp_dataset = Subset(normalized_data, temp_idx)\n",
    "\n",
    "    # Stratified split on temp data: validation and test split\n",
    "    val_test_labels = [labels[i] for i in temp_idx]\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "    for val_idx, test_idx in split.split(temp_idx, val_test_labels):\n",
    "        val_dataset = Subset(normalized_data, [temp_idx[i] for i in val_idx])\n",
    "        test_dataset = Subset(normalized_data, [temp_idx[i] for i in test_idx])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Usage example with MusicDataset\n",
    "dataset = MusicDataset(data_dir)\n",
    "batch_size = 20\n",
    "train_loader, val_loader, test_loader = create_dataloaders(dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files=os.listdir(data_dir)\n",
    "\n",
    "classes = []\n",
    "\n",
    "for file in list_files:\n",
    "\n",
    "  name='{}/{}'.format(data_dir,file)\n",
    "\n",
    "  if os.path.isdir(name):\n",
    "\n",
    "    classes.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualización de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spectrogram(waveform):\n",
    "    # Ensure the waveform is in the correct shape\n",
    "    if len(waveform.shape) == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    # Convert the waveform to a spectrogram\n",
    "    spectrogram = tt.Spectrogram()(waveform)\n",
    "    return spectrogram\n",
    "\n",
    "def process_dataloader_to_spectrograms(dataloader):\n",
    "    spectrograms = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Assuming the batch is a tuple (waveforms, labels) and waveforms are the audio data\n",
    "        waveforms, labels = batch\n",
    "        \n",
    "        # Process each waveform in the batch\n",
    "        batch_spectrograms = [audio_to_spectrogram(waveform) for waveform in waveforms]\n",
    "        \n",
    "        # Append to the list of spectrograms\n",
    "        spectrograms.append((torch.stack(batch_spectrograms), labels))\n",
    "    \n",
    "    return spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 1  # for RGB images, or 1 for grayscale\n",
    "num_classes = 10    # depends on your specific classification task\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectogram = process_dataloader_to_spectrograms(train_loader)\n",
    "val_spectogram = process_dataloader_to_spectrograms(val_loader)\n",
    "test_spectogram = process_dataloader_to_spectrograms(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo que recibe el tamaño de la capa y la cantidad de capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste en la clase del modelo\n",
    "class ExperimentNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, layer_size, layers):\n",
    "        super(ExperimentNN, self).__init__()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% probability\n",
    "\n",
    "        for layer in range(layers - 1):\n",
    "            if layer == 0:\n",
    "                self.fc_layers.append(nn.Linear(input_size, layer_size))\n",
    "            else:\n",
    "                self.fc_layers.append(nn.Linear(layer_size, layer_size))\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_layers.append(nn.Linear(layer_size, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Aplanar la onda de audio\n",
    "        x = (x - x.mean()) / x.std()  # Normalización\n",
    "        for fc in self.fc_layers[:-1]:  # Skip last layer\n",
    "            x = self.dropout(F.relu(fc(x)))  # ReLU + Dropout\n",
    "        x = self.fc_layers[-1](x)  # Output layer (no activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs, train_loader, val_loader, device):\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None  # To store the best model's state\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check if the current validation loss is the best\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # Save model state\n",
    "            print(f\"New best validation loss: {best_loss:.4f}\")\n",
    "\n",
    "        # Update learning rate with scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Clear cache and collect garbage\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the best model's state into the model and return it\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return best_loss, model  # Return the final best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimentando con 2 capas y 32 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 3.5379\n",
      "Validation loss: 4.0607\n",
      "New best validation loss: 4.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Train loss: 2.8485\n",
      "Validation loss: 4.6007\n",
      "Epoch 3/15 - Train loss: 1.9416\n",
      "Validation loss: 4.9296\n",
      "Epoch 4/15 - Train loss: 1.6447\n",
      "Validation loss: 5.3143\n",
      "Epoch 5/15 - Train loss: 1.4859\n",
      "Validation loss: 5.4273\n",
      "Epoch 6/15 - Train loss: 1.2327\n",
      "Validation loss: 5.2674\n",
      "Epoch 7/15 - Train loss: 1.1366\n",
      "Validation loss: 5.2504\n",
      "Epoch 8/15 - Train loss: 0.9605\n",
      "Validation loss: 5.3706\n",
      "Epoch 9/15 - Train loss: 0.8903\n",
      "Validation loss: 5.1963\n",
      "Epoch 10/15 - Train loss: 1.0175\n",
      "Validation loss: 5.7369\n",
      "Epoch 11/15 - Train loss: 0.9583\n",
      "Validation loss: 5.6599\n",
      "Epoch 12/15 - Train loss: 0.8215\n",
      "Validation loss: 5.1936\n",
      "Epoch 13/15 - Train loss: 0.7246\n",
      "Validation loss: 5.3675\n",
      "Epoch 14/15 - Train loss: 0.6201\n",
      "Validation loss: 5.3349\n",
      "Epoch 15/15 - Train loss: 0.6945\n",
      "Validation loss: 5.3079\n",
      "Loss de validación para 2 capas y 32 unidades: 4.060677021741867\n",
      "Precisión de validación: 8.78%\n",
      "Nuevo mejor modelo encontrado con 2 capas y 32 unidades.\n",
      "Experimentando con 2 capas y 64 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 3.5062\n",
      "Validation loss: 3.9468\n",
      "New best validation loss: 3.9468\n",
      "Epoch 2/15 - Train loss: 2.1783\n",
      "Validation loss: 4.4562\n",
      "Epoch 3/15 - Train loss: 1.4351\n",
      "Validation loss: 5.0825\n",
      "Epoch 4/15 - Train loss: 1.0988\n",
      "Validation loss: 5.1436\n",
      "Epoch 5/15 - Train loss: 0.8826\n",
      "Validation loss: 5.0516\n",
      "Epoch 6/15 - Train loss: 0.6649\n",
      "Validation loss: 5.2836\n",
      "Epoch 7/15 - Train loss: 0.6343\n",
      "Validation loss: 5.0856\n",
      "Epoch 8/15 - Train loss: 0.5340\n",
      "Validation loss: 5.1048\n",
      "Epoch 9/15 - Train loss: 0.6303\n",
      "Validation loss: 5.1442\n",
      "Epoch 10/15 - Train loss: 0.5646\n",
      "Validation loss: 5.2621\n",
      "Epoch 11/15 - Train loss: 0.5589\n",
      "Validation loss: 5.3353\n",
      "Epoch 12/15 - Train loss: 0.4223\n",
      "Validation loss: 4.9672\n",
      "Epoch 13/15 - Train loss: 0.4857\n",
      "Validation loss: 5.4700\n",
      "Epoch 14/15 - Train loss: 0.4765\n",
      "Validation loss: 5.4975\n",
      "Epoch 15/15 - Train loss: 0.4857\n",
      "Validation loss: 5.4676\n",
      "Loss de validación para 2 capas y 64 unidades: 3.9467813968658447\n",
      "Precisión de validación: 12.16%\n",
      "Nuevo mejor modelo encontrado con 2 capas y 64 unidades.\n",
      "Experimentando con 2 capas y 128 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 4.0455\n",
      "Validation loss: 4.6170\n",
      "New best validation loss: 4.6170\n",
      "Epoch 2/15 - Train loss: 1.2333\n",
      "Validation loss: 5.2602\n",
      "Epoch 3/15 - Train loss: 0.7730\n",
      "Validation loss: 5.3901\n",
      "Epoch 4/15 - Train loss: 0.4618\n",
      "Validation loss: 5.2562\n",
      "Epoch 5/15 - Train loss: 0.4273\n",
      "Validation loss: 5.0532\n",
      "Epoch 6/15 - Train loss: 0.3539\n",
      "Validation loss: 5.2591\n",
      "Epoch 7/15 - Train loss: 0.3752\n",
      "Validation loss: 5.2656\n",
      "Epoch 8/15 - Train loss: 0.3101\n",
      "Validation loss: 5.2550\n",
      "Epoch 9/15 - Train loss: 0.2750\n",
      "Validation loss: 5.1344\n",
      "Epoch 10/15 - Train loss: 0.2379\n",
      "Validation loss: 5.3412\n",
      "Epoch 11/15 - Train loss: 0.3148\n",
      "Validation loss: 5.4204\n",
      "Epoch 12/15 - Train loss: 0.2558\n",
      "Validation loss: 5.4129\n",
      "Epoch 13/15 - Train loss: 0.2086\n",
      "Validation loss: 5.5965\n",
      "Epoch 14/15 - Train loss: 0.2114\n",
      "Validation loss: 5.4771\n",
      "Epoch 15/15 - Train loss: 0.2236\n",
      "Validation loss: 5.2365\n",
      "Loss de validación para 2 capas y 128 unidades: 4.617018342018127\n",
      "Precisión de validación: 12.16%\n",
      "Experimentando con 2 capas y 256 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 4.4979\n",
      "Validation loss: 5.5302\n",
      "New best validation loss: 5.5302\n",
      "Epoch 2/15 - Train loss: 0.7376\n",
      "Validation loss: 5.2563\n",
      "New best validation loss: 5.2563\n",
      "Epoch 3/15 - Train loss: 0.4615\n",
      "Validation loss: 5.3418\n",
      "Epoch 4/15 - Train loss: 0.3677\n",
      "Validation loss: 5.3428\n",
      "Epoch 5/15 - Train loss: 0.2939\n",
      "Validation loss: 5.3448\n",
      "Epoch 6/15 - Train loss: 0.2239\n",
      "Validation loss: 5.3836\n",
      "Epoch 7/15 - Train loss: 0.2260\n",
      "Validation loss: 5.3153\n",
      "Epoch 8/15 - Train loss: 0.1920\n",
      "Validation loss: 5.3496\n",
      "Epoch 9/15 - Train loss: 0.1921\n",
      "Validation loss: 5.4375\n",
      "Epoch 10/15 - Train loss: 0.1399\n",
      "Validation loss: 5.6476\n",
      "Epoch 11/15 - Train loss: 0.1463\n",
      "Validation loss: 5.6291\n",
      "Epoch 12/15 - Train loss: 0.1253\n",
      "Validation loss: 5.4747\n",
      "Epoch 13/15 - Train loss: 0.1032\n",
      "Validation loss: 5.4029\n",
      "Epoch 14/15 - Train loss: 0.1237\n",
      "Validation loss: 5.3935\n",
      "Epoch 15/15 - Train loss: 0.1122\n",
      "Validation loss: 5.3210\n",
      "Loss de validación para 2 capas y 256 unidades: 5.256320387125015\n",
      "Precisión de validación: 14.19%\n",
      "Nuevo mejor modelo encontrado con 2 capas y 256 unidades.\n",
      "Experimentando con 3 capas y 32 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.9790\n",
      "Validation loss: 2.6348\n",
      "New best validation loss: 2.6348\n",
      "Epoch 2/15 - Train loss: 4.0298\n",
      "Validation loss: 2.5726\n",
      "New best validation loss: 2.5726\n",
      "Epoch 3/15 - Train loss: 4.2362\n",
      "Validation loss: 2.5186\n",
      "New best validation loss: 2.5186\n",
      "Epoch 4/15 - Train loss: 3.7253\n",
      "Validation loss: 2.4286\n",
      "New best validation loss: 2.4286\n",
      "Epoch 5/15 - Train loss: 3.7426\n",
      "Validation loss: 2.4158\n",
      "New best validation loss: 2.4158\n",
      "Epoch 6/15 - Train loss: 3.4139\n",
      "Validation loss: 2.3788\n",
      "New best validation loss: 2.3788\n",
      "Epoch 7/15 - Train loss: 3.4903\n",
      "Validation loss: 2.3924\n",
      "Epoch 8/15 - Train loss: 3.1379\n",
      "Validation loss: 2.3394\n",
      "New best validation loss: 2.3394\n",
      "Epoch 9/15 - Train loss: 2.9467\n",
      "Validation loss: 2.3317\n",
      "New best validation loss: 2.3317\n",
      "Epoch 10/15 - Train loss: 2.9094\n",
      "Validation loss: 2.3272\n",
      "New best validation loss: 2.3272\n",
      "Epoch 11/15 - Train loss: 2.6516\n",
      "Validation loss: 2.3373\n",
      "Epoch 12/15 - Train loss: 2.6312\n",
      "Validation loss: 2.3549\n",
      "Epoch 13/15 - Train loss: 2.5200\n",
      "Validation loss: 2.3179\n",
      "New best validation loss: 2.3179\n",
      "Epoch 14/15 - Train loss: 2.5832\n",
      "Validation loss: 2.3616\n",
      "Epoch 15/15 - Train loss: 2.2885\n",
      "Validation loss: 2.3197\n",
      "Loss de validación para 3 capas y 32 unidades: 2.3178512156009674\n",
      "Precisión de validación: 10.81%\n",
      "Experimentando con 3 capas y 64 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.7227\n",
      "Validation loss: 2.3836\n",
      "New best validation loss: 2.3836\n",
      "Epoch 2/15 - Train loss: 4.2077\n",
      "Validation loss: 2.3945\n",
      "Epoch 3/15 - Train loss: 3.9513\n",
      "Validation loss: 2.3980\n",
      "Epoch 4/15 - Train loss: 3.8431\n",
      "Validation loss: 2.3819\n",
      "New best validation loss: 2.3819\n",
      "Epoch 5/15 - Train loss: 3.5385\n",
      "Validation loss: 2.3708\n",
      "New best validation loss: 2.3708\n",
      "Epoch 6/15 - Train loss: 3.3002\n",
      "Validation loss: 2.3446\n",
      "New best validation loss: 2.3446\n",
      "Epoch 7/15 - Train loss: 3.1461\n",
      "Validation loss: 2.3368\n",
      "New best validation loss: 2.3368\n",
      "Epoch 8/15 - Train loss: 2.6958\n",
      "Validation loss: 2.3677\n",
      "Epoch 9/15 - Train loss: 2.7458\n",
      "Validation loss: 2.3483\n",
      "Epoch 10/15 - Train loss: 2.5083\n",
      "Validation loss: 2.3547\n",
      "Epoch 11/15 - Train loss: 2.3392\n",
      "Validation loss: 2.3872\n",
      "Epoch 12/15 - Train loss: 2.3537\n",
      "Validation loss: 2.3882\n",
      "Epoch 13/15 - Train loss: 2.2535\n",
      "Validation loss: 2.3359\n",
      "New best validation loss: 2.3359\n",
      "Epoch 14/15 - Train loss: 2.0016\n",
      "Validation loss: 2.3439\n",
      "Epoch 15/15 - Train loss: 1.8689\n",
      "Validation loss: 2.3866\n",
      "Loss de validación para 3 capas y 64 unidades: 2.3359024822711945\n",
      "Precisión de validación: 17.57%\n",
      "Nuevo mejor modelo encontrado con 3 capas y 64 unidades.\n",
      "Experimentando con 3 capas y 128 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.6860\n",
      "Validation loss: 2.4558\n",
      "New best validation loss: 2.4558\n",
      "Epoch 2/15 - Train loss: 3.3684\n",
      "Validation loss: 2.4894\n",
      "Epoch 3/15 - Train loss: 3.5440\n",
      "Validation loss: 2.5199\n",
      "Epoch 4/15 - Train loss: 3.4462\n",
      "Validation loss: 2.5881\n",
      "Epoch 5/15 - Train loss: 2.9958\n",
      "Validation loss: 2.5196\n",
      "Epoch 6/15 - Train loss: 2.7012\n",
      "Validation loss: 2.5087\n",
      "Epoch 7/15 - Train loss: 2.4641\n",
      "Validation loss: 2.5167\n",
      "Epoch 8/15 - Train loss: 2.0521\n",
      "Validation loss: 2.5435\n",
      "Epoch 9/15 - Train loss: 1.9628\n",
      "Validation loss: 2.5667\n",
      "Epoch 10/15 - Train loss: 1.9997\n",
      "Validation loss: 2.6061\n",
      "Epoch 11/15 - Train loss: 2.0260\n",
      "Validation loss: 2.6592\n",
      "Epoch 12/15 - Train loss: 1.4131\n",
      "Validation loss: 2.6005\n",
      "Epoch 13/15 - Train loss: 1.4628\n",
      "Validation loss: 2.6127\n",
      "Epoch 14/15 - Train loss: 1.3709\n",
      "Validation loss: 2.6203\n",
      "Epoch 15/15 - Train loss: 1.2227\n",
      "Validation loss: 2.6857\n",
      "Loss de validación para 3 capas y 128 unidades: 2.455779016017914\n",
      "Precisión de validación: 11.49%\n",
      "Experimentando con 3 capas y 256 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.7168\n",
      "Validation loss: 2.4372\n",
      "New best validation loss: 2.4372\n",
      "Epoch 2/15 - Train loss: 3.1540\n",
      "Validation loss: 2.6594\n",
      "Epoch 3/15 - Train loss: 2.6472\n",
      "Validation loss: 2.6933\n",
      "Epoch 4/15 - Train loss: 2.0598\n",
      "Validation loss: 2.7515\n",
      "Epoch 5/15 - Train loss: 1.7771\n",
      "Validation loss: 2.7546\n",
      "Epoch 6/15 - Train loss: 1.5324\n",
      "Validation loss: 2.8102\n",
      "Epoch 7/15 - Train loss: 1.2117\n",
      "Validation loss: 2.9199\n",
      "Epoch 8/15 - Train loss: 1.1441\n",
      "Validation loss: 3.0845\n",
      "Epoch 9/15 - Train loss: 1.0579\n",
      "Validation loss: 3.0437\n",
      "Epoch 10/15 - Train loss: 0.7696\n",
      "Validation loss: 3.1629\n",
      "Epoch 11/15 - Train loss: 0.9726\n",
      "Validation loss: 3.3174\n",
      "Epoch 12/15 - Train loss: 0.7293\n",
      "Validation loss: 3.3457\n",
      "Epoch 13/15 - Train loss: 0.7681\n",
      "Validation loss: 3.3047\n",
      "Epoch 14/15 - Train loss: 0.5912\n",
      "Validation loss: 3.3278\n",
      "Epoch 15/15 - Train loss: 0.5647\n",
      "Validation loss: 3.3882\n",
      "Loss de validación para 3 capas y 256 unidades: 2.4371513724327087\n",
      "Precisión de validación: 10.14%\n",
      "Experimentando con 5 capas y 32 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3369\n",
      "Validation loss: 2.3083\n",
      "New best validation loss: 2.3083\n",
      "Epoch 2/15 - Train loss: 2.6361\n",
      "Validation loss: 2.3107\n",
      "Epoch 3/15 - Train loss: 2.6289\n",
      "Validation loss: 2.3007\n",
      "New best validation loss: 2.3007\n",
      "Epoch 4/15 - Train loss: 2.5608\n",
      "Validation loss: 2.2955\n",
      "New best validation loss: 2.2955\n",
      "Epoch 5/15 - Train loss: 2.5709\n",
      "Validation loss: 2.2997\n",
      "Epoch 6/15 - Train loss: 2.5202\n",
      "Validation loss: 2.2979\n",
      "Epoch 7/15 - Train loss: 2.4207\n",
      "Validation loss: 2.2997\n",
      "Epoch 8/15 - Train loss: 2.4099\n",
      "Validation loss: 2.2947\n",
      "New best validation loss: 2.2947\n",
      "Epoch 9/15 - Train loss: 2.3806\n",
      "Validation loss: 2.2978\n",
      "Epoch 10/15 - Train loss: 2.3506\n",
      "Validation loss: 2.2982\n",
      "Epoch 11/15 - Train loss: 2.3323\n",
      "Validation loss: 2.2962\n",
      "Epoch 12/15 - Train loss: 2.3165\n",
      "Validation loss: 2.2926\n",
      "New best validation loss: 2.2926\n",
      "Epoch 13/15 - Train loss: 2.3203\n",
      "Validation loss: 2.2952\n",
      "Epoch 14/15 - Train loss: 2.3391\n",
      "Validation loss: 2.3030\n",
      "Epoch 15/15 - Train loss: 2.3485\n",
      "Validation loss: 2.2984\n",
      "Loss de validación para 5 capas y 32 unidades: 2.2926401793956757\n",
      "Precisión de validación: 12.84%\n",
      "Experimentando con 5 capas y 64 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3948\n",
      "Validation loss: 2.3058\n",
      "New best validation loss: 2.3058\n",
      "Epoch 2/15 - Train loss: 2.7673\n",
      "Validation loss: 2.2848\n",
      "New best validation loss: 2.2848\n",
      "Epoch 3/15 - Train loss: 2.7624\n",
      "Validation loss: 2.2853\n",
      "Epoch 4/15 - Train loss: 2.6523\n",
      "Validation loss: 2.2964\n",
      "Epoch 5/15 - Train loss: 2.5419\n",
      "Validation loss: 2.2904\n",
      "Epoch 6/15 - Train loss: 2.4762\n",
      "Validation loss: 2.2867\n",
      "Epoch 7/15 - Train loss: 2.4400\n",
      "Validation loss: 2.2882\n",
      "Epoch 8/15 - Train loss: 2.4407\n",
      "Validation loss: 2.2902\n",
      "Epoch 9/15 - Train loss: 2.3544\n",
      "Validation loss: 2.2950\n",
      "Epoch 10/15 - Train loss: 2.3571\n",
      "Validation loss: 2.2941\n",
      "Epoch 11/15 - Train loss: 2.3402\n",
      "Validation loss: 2.2962\n",
      "Epoch 12/15 - Train loss: 2.3091\n",
      "Validation loss: 2.3016\n",
      "Epoch 13/15 - Train loss: 2.3142\n",
      "Validation loss: 2.2943\n",
      "Epoch 14/15 - Train loss: 2.3133\n",
      "Validation loss: 2.2996\n",
      "Epoch 15/15 - Train loss: 2.2670\n",
      "Validation loss: 2.3021\n",
      "Loss de validación para 5 capas y 64 unidades: 2.2848425209522247\n",
      "Precisión de validación: 8.78%\n",
      "Experimentando con 5 capas y 128 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3637\n",
      "Validation loss: 2.2613\n",
      "New best validation loss: 2.2613\n",
      "Epoch 2/15 - Train loss: 2.9427\n",
      "Validation loss: 2.2817\n",
      "Epoch 3/15 - Train loss: 2.7949\n",
      "Validation loss: 2.2891\n",
      "Epoch 4/15 - Train loss: 2.7102\n",
      "Validation loss: 2.2954\n",
      "Epoch 5/15 - Train loss: 2.5258\n",
      "Validation loss: 2.2958\n",
      "Epoch 6/15 - Train loss: 2.4662\n",
      "Validation loss: 2.2909\n",
      "Epoch 7/15 - Train loss: 2.4309\n",
      "Validation loss: 2.2877\n",
      "Epoch 8/15 - Train loss: 2.4125\n",
      "Validation loss: 2.2890\n",
      "Epoch 9/15 - Train loss: 2.3964\n",
      "Validation loss: 2.2956\n",
      "Epoch 10/15 - Train loss: 2.3222\n",
      "Validation loss: 2.2911\n",
      "Epoch 11/15 - Train loss: 2.2653\n",
      "Validation loss: 2.2888\n",
      "Epoch 12/15 - Train loss: 2.3410\n",
      "Validation loss: 2.2914\n",
      "Epoch 13/15 - Train loss: 2.2010\n",
      "Validation loss: 2.2944\n",
      "Epoch 14/15 - Train loss: 2.2300\n",
      "Validation loss: 2.2932\n",
      "Epoch 15/15 - Train loss: 2.2620\n",
      "Validation loss: 2.2899\n",
      "Loss de validación para 5 capas y 128 unidades: 2.261313319206238\n",
      "Precisión de validación: 8.78%\n",
      "Experimentando con 5 capas y 256 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3377\n",
      "Validation loss: 2.2566\n",
      "New best validation loss: 2.2566\n",
      "Epoch 2/15 - Train loss: 2.8500\n",
      "Validation loss: 2.2839\n",
      "Epoch 3/15 - Train loss: 2.6385\n",
      "Validation loss: 2.2708\n",
      "Epoch 4/15 - Train loss: 2.6586\n",
      "Validation loss: 2.2875\n",
      "Epoch 5/15 - Train loss: 2.4832\n",
      "Validation loss: 2.2856\n",
      "Epoch 6/15 - Train loss: 2.4318\n",
      "Validation loss: 2.2897\n",
      "Epoch 7/15 - Train loss: 2.3486\n",
      "Validation loss: 2.2938\n",
      "Epoch 8/15 - Train loss: 2.2797\n",
      "Validation loss: 2.2933\n",
      "Epoch 9/15 - Train loss: 2.1956\n",
      "Validation loss: 2.2875\n",
      "Epoch 10/15 - Train loss: 2.2161\n",
      "Validation loss: 2.2874\n",
      "Epoch 11/15 - Train loss: 2.1380\n",
      "Validation loss: 2.2787\n",
      "Epoch 12/15 - Train loss: 2.1562\n",
      "Validation loss: 2.2821\n",
      "Epoch 13/15 - Train loss: 2.0917\n",
      "Validation loss: 2.2805\n",
      "Epoch 14/15 - Train loss: 2.0215\n",
      "Validation loss: 2.2829\n",
      "Epoch 15/15 - Train loss: 1.9607\n",
      "Validation loss: 2.2779\n",
      "Loss de validación para 5 capas y 256 unidades: 2.2565984427928925\n",
      "Precisión de validación: 8.11%\n",
      "Experimentando con 7 capas y 32 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3301\n",
      "Validation loss: 2.3065\n",
      "New best validation loss: 2.3065\n",
      "Epoch 2/15 - Train loss: 2.4207\n",
      "Validation loss: 2.3078\n",
      "Epoch 3/15 - Train loss: 2.3923\n",
      "Validation loss: 2.3082\n",
      "Epoch 4/15 - Train loss: 2.3723\n",
      "Validation loss: 2.3101\n",
      "Epoch 5/15 - Train loss: 2.3523\n",
      "Validation loss: 2.3083\n",
      "Epoch 6/15 - Train loss: 2.3108\n",
      "Validation loss: 2.3049\n",
      "New best validation loss: 2.3049\n",
      "Epoch 7/15 - Train loss: 2.3127\n",
      "Validation loss: 2.3040\n",
      "New best validation loss: 2.3040\n",
      "Epoch 8/15 - Train loss: 2.3304\n",
      "Validation loss: 2.3036\n",
      "New best validation loss: 2.3036\n",
      "Epoch 9/15 - Train loss: 2.2959\n",
      "Validation loss: 2.3068\n",
      "Epoch 10/15 - Train loss: 2.2743\n",
      "Validation loss: 2.3058\n",
      "Epoch 11/15 - Train loss: 2.2791\n",
      "Validation loss: 2.3023\n",
      "New best validation loss: 2.3023\n",
      "Epoch 12/15 - Train loss: 2.2746\n",
      "Validation loss: 2.3064\n",
      "Epoch 13/15 - Train loss: 2.2830\n",
      "Validation loss: 2.3041\n",
      "Epoch 14/15 - Train loss: 2.2717\n",
      "Validation loss: 2.3048\n",
      "Epoch 15/15 - Train loss: 2.2291\n",
      "Validation loss: 2.3057\n",
      "Loss de validación para 7 capas y 32 unidades: 2.3022977113723755\n",
      "Precisión de validación: 10.14%\n",
      "Experimentando con 7 capas y 64 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3067\n",
      "Validation loss: 2.2970\n",
      "New best validation loss: 2.2970\n",
      "Epoch 2/15 - Train loss: 2.3845\n",
      "Validation loss: 2.2974\n",
      "Epoch 3/15 - Train loss: 2.3666\n",
      "Validation loss: 2.3025\n",
      "Epoch 4/15 - Train loss: 2.3304\n",
      "Validation loss: 2.3025\n",
      "Epoch 5/15 - Train loss: 2.2910\n",
      "Validation loss: 2.3021\n",
      "Epoch 6/15 - Train loss: 2.2792\n",
      "Validation loss: 2.2984\n",
      "Epoch 7/15 - Train loss: 2.2670\n",
      "Validation loss: 2.3025\n",
      "Epoch 8/15 - Train loss: 2.2534\n",
      "Validation loss: 2.2994\n",
      "Epoch 9/15 - Train loss: 2.2518\n",
      "Validation loss: 2.3007\n",
      "Epoch 10/15 - Train loss: 2.2605\n",
      "Validation loss: 2.3035\n",
      "Epoch 11/15 - Train loss: 2.2604\n",
      "Validation loss: 2.3062\n",
      "Epoch 12/15 - Train loss: 2.2374\n",
      "Validation loss: 2.3042\n",
      "Epoch 13/15 - Train loss: 2.2288\n",
      "Validation loss: 2.3052\n",
      "Epoch 14/15 - Train loss: 2.2025\n",
      "Validation loss: 2.3064\n",
      "Epoch 15/15 - Train loss: 2.2237\n",
      "Validation loss: 2.3021\n",
      "Loss de validación para 7 capas y 64 unidades: 2.2970449924468994\n",
      "Precisión de validación: 13.51%\n",
      "Experimentando con 7 capas y 128 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3013\n",
      "Validation loss: 2.2906\n",
      "New best validation loss: 2.2906\n",
      "Epoch 2/15 - Train loss: 2.3722\n",
      "Validation loss: 2.3009\n",
      "Epoch 3/15 - Train loss: 2.3429\n",
      "Validation loss: 2.3004\n",
      "Epoch 4/15 - Train loss: 2.3190\n",
      "Validation loss: 2.3010\n",
      "Epoch 5/15 - Train loss: 2.2788\n",
      "Validation loss: 2.3022\n",
      "Epoch 6/15 - Train loss: 2.2457\n",
      "Validation loss: 2.3045\n",
      "Epoch 7/15 - Train loss: 2.2358\n",
      "Validation loss: 2.3038\n",
      "Epoch 8/15 - Train loss: 2.2467\n",
      "Validation loss: 2.3008\n",
      "Epoch 9/15 - Train loss: 2.2398\n",
      "Validation loss: 2.3024\n",
      "Epoch 10/15 - Train loss: 2.2466\n",
      "Validation loss: 2.3035\n",
      "Epoch 11/15 - Train loss: 2.1798\n",
      "Validation loss: 2.3032\n",
      "Epoch 12/15 - Train loss: 2.1909\n",
      "Validation loss: 2.3075\n",
      "Epoch 13/15 - Train loss: 2.1866\n",
      "Validation loss: 2.3104\n",
      "Epoch 14/15 - Train loss: 2.2004\n",
      "Validation loss: 2.3102\n",
      "Epoch 15/15 - Train loss: 2.1964\n",
      "Validation loss: 2.3157\n",
      "Loss de validación para 7 capas y 128 unidades: 2.290589213371277\n",
      "Precisión de validación: 12.84%\n",
      "Experimentando con 7 capas y 256 unidades por capa...\n",
      "Epoch 1/15 - Train loss: 2.3124\n",
      "Validation loss: 2.2709\n",
      "New best validation loss: 2.2709\n",
      "Epoch 2/15 - Train loss: 2.3618\n",
      "Validation loss: 2.2960\n",
      "Epoch 3/15 - Train loss: 2.3257\n",
      "Validation loss: 2.3001\n",
      "Epoch 4/15 - Train loss: 2.2600\n",
      "Validation loss: 2.2979\n",
      "Epoch 5/15 - Train loss: 2.2692\n",
      "Validation loss: 2.2992\n",
      "Epoch 6/15 - Train loss: 2.2208\n",
      "Validation loss: 2.2995\n",
      "Epoch 7/15 - Train loss: 2.2164\n",
      "Validation loss: 2.2942\n",
      "Epoch 8/15 - Train loss: 2.2197\n",
      "Validation loss: 2.3005\n",
      "Epoch 9/15 - Train loss: 2.1925\n",
      "Validation loss: 2.2972\n",
      "Epoch 10/15 - Train loss: 2.2014\n",
      "Validation loss: 2.3026\n",
      "Epoch 11/15 - Train loss: 2.1734\n",
      "Validation loss: 2.3037\n",
      "Epoch 12/15 - Train loss: 2.1615\n",
      "Validation loss: 2.3107\n",
      "Epoch 13/15 - Train loss: 2.1396\n",
      "Validation loss: 2.3135\n",
      "Epoch 14/15 - Train loss: 2.1186\n",
      "Validation loss: 2.3271\n",
      "Epoch 15/15 - Train loss: 2.1121\n",
      "Validation loss: 2.3452\n",
      "Loss de validación para 7 capas y 256 unidades: 2.2708586156368256\n",
      "Precisión de validación: 11.49%\n",
      "Mejor modelo: 7 capas y 256 unidades, con precisión de validación de 17.57% y pérdida de validación de 2.3359\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = samplerate * 5\n",
    "\n",
    "num_classes = len(dataset.classes)  # Número de clases (géneros musicales)\n",
    "\n",
    "# Define nuevos valores de hiperparámetros para experimentar\n",
    "layers_list = [2, 3, 5, 7]       # Pruebas con más capas\n",
    "sizes_list = [32, 64, 128, 256]  # Pruebas con más unidades en cada capa\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_accuracy = 0\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 1e-4\n",
    "best_model = None\n",
    "\n",
    "# Loop de experimentación\n",
    "for layers in layers_list:\n",
    "    for size in sizes_list:\n",
    "        print(f\"Experimentando con {layers} capas y {size} unidades por capa...\")\n",
    "        \n",
    "        # Inicializar el modelo con la configuración actual\n",
    "        model = ExperimentNN(input_size,\n",
    "                             num_classes,\n",
    "                             size,\n",
    "                             layers).to(device)\n",
    "\n",
    "        # Definir el criterio y optimizador con los pesos de las clases\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Define el scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # Llama a train_model pasándole el scheduler\n",
    "        validation_loss, temp_model = train_model(\n",
    "            model, criterion, optimizer, scheduler=scheduler, epochs=15,\n",
    "            train_loader=train_loader, val_loader=val_loader, device=device\n",
    "        )\n",
    "\n",
    "        # Calcular precisión de validación\n",
    "        temp_model.eval()\n",
    "        correct = 0\n",
    "        total = len(val_loader.dataset)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = temp_model(inputs)  # Use temp_model here\n",
    "                predicted = outputs.argmax(dim=1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Loss de validación para {layers} capas y {size} unidades: {validation_loss}\")\n",
    "        print(f\"Precisión de validación: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Guardar el modelo con mejor precisión y menor pérdida\n",
    "        if val_accuracy > best_val_accuracy or (val_accuracy == best_val_accuracy and validation_loss < best_val_loss):\n",
    "            best_val_loss = validation_loss\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = temp_model  # Store the model with the best configuration\n",
    "            print(f\"Nuevo mejor modelo encontrado con {layers} capas y {size} unidades.\")\n",
    "\n",
    "print(f\"Mejor modelo: {layers} capas y {size} unidades, con precisión de validación de {best_val_accuracy:.2f}% y pérdida de validación de {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando el mejor modelo en el conjunto de prueba...\n",
      "Loss en el conjunto de prueba: 2.4154\n",
      "Precisión en el conjunto de prueba: 15.44%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación final en el conjunto de prueba\n",
    "print(\"Evaluando el mejor modelo en el conjunto de prueba...\")\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = len(test_loader.dataset)\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = best_model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss:.4f}\")\n",
    "print(f\"Precisión en el conjunto de prueba: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, conv_layers_config):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Initialize the list to hold the convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Initialize the number of input channels for the first layer\n",
    "        in_channels = input_channels\n",
    "\n",
    "        # Dynamically create convolutional layers based on the configuration\n",
    "        for (out_channels, kernel_size, stride, padding) in conv_layers_config:\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            in_channels = out_channels  # Update in_channels for the next layer\n",
    "\n",
    "        # Calculate the size after convolution and pooling to define the fully connected layer\n",
    "        # Assuming pooling reduces the size by a factor of 2 at each layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Get the final feature map size (after all conv and pooling layers)\n",
    "        self.final_feature_map_size = self._get_conv_output_size(input_channels, conv_layers_config)\n",
    "        \n",
    "        # Define 9 fully connected layers with 256 nodes each\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(nn.Linear(self.final_feature_map_size, 64))  # First fully connected layer\n",
    "        for _ in range(2):  # Add 8 more fully connected layers with 256 nodes\n",
    "            self.fc_layers.append(nn.Linear(64, 64))\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(64, num_classes)  # Output layer for classification\n",
    "        \n",
    "    def _get_conv_output_size(self, input_channels, conv_layers_config):\n",
    "        # Sample input size (height x width) to calculate the final feature map size\n",
    "        # You can adjust these values based on your actual input size\n",
    "        height = 201  # Replace with your actual input height\n",
    "        width = 552   # Replace with your actual input width\n",
    "        \n",
    "        # Apply each convolutional and pooling layer\n",
    "        for (out_channels, kernel_size, stride, padding) in conv_layers_config:\n",
    "            height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "            width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "            height = height // 2  # Max pooling halves the height\n",
    "            width = width // 2    # Max pooling halves the width\n",
    "        \n",
    "        # Return the total number of features after all convolutional and pooling layers\n",
    "        return out_channels * height * width\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each convolutional layer followed by ReLU and pooling\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = F.relu(conv_layer(x))\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output before passing it to the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "\n",
    "        # Apply the fully connected layers\n",
    "        for fc in self.fc_layers:\n",
    "            x = F.relu(fc(x))\n",
    "        \n",
    "        # Output layer (classification)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Train the model and select the best gradients based on validation loss.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_gradients = None  # To store gradients with the lowest validation loss\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training loop\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate training loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Print loss for every batch\n",
    "            if (i + 1) % 10 == 0 or (i + 1) == len(train_loader):\n",
    "                print(f\"  Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Training completed. Average Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = len(val_loader)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        avg_val_loss = val_loss / total\n",
    "        print(f\"Epoch {epoch+1} Validation completed. Average Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # If this epoch has the best (lowest) validation loss, save the gradients\n",
    "        if (accuracy > best_accuracy) or ((accuracy == best_accuracy) and (avg_val_loss < best_val_loss)):\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_accuracy = accuracy\n",
    "            # Capture gradients\n",
    "            best_gradients = {name: param.grad.clone() for name, param in model.named_parameters() if param.grad is not None}\n",
    "            print(f\"New best gradients stored for epoch {epoch+1} with validation loss {best_val_loss:.4f} and accuracy {accuracy}%\")\n",
    "        \n",
    "        # Set model back to training mode\n",
    "        model.train()\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    \n",
    "    return best_gradients\n",
    "\n",
    "def test_model_configuration(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0  # Total number of samples processed\n",
    "\n",
    "    # Ensure no gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:  # Assuming test_loader is the correct DataLoader for your test set\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class labels (the one with the highest logit)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Accumulate total samples\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Accumulate correct predictions\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def test_multiple_configurations(train, val, test, criterion, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Test multiple model configurations, applying the best gradients for evaluation,\n",
    "    and save the best model based on accuracy or validation loss.\n",
    "    \"\"\"\n",
    "    # Different configurations for the CNN model\n",
    "    configurations = [\n",
    "        [(32, 3, 1, 1), (64, 3, 1, 1), (128, 3, 1, 1)],  # Configuration 1\n",
    "        [(32, 5, 1, 2), (64, 5, 1, 2)],                  # Configuration 2\n",
    "        [(16, 3, 1, 1), (32, 3, 1, 1), (64, 3, 1, 1)],   # Configuration 3\n",
    "        [(64, 3, 1, 1), (128, 3, 1, 1), (256, 3, 1, 1)]  # Configuration 4\n",
    "    ]\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = float(\"inf\")\n",
    "    \n",
    "    for idx, conv_layers_config in enumerate(configurations):\n",
    "        print(f\"\\nTesting Configuration {idx + 1} with convolutional layers: {conv_layers_config}\")\n",
    "        \n",
    "        # Initialize the model with the current configuration\n",
    "        model = CNN(input_channels=1, num_classes=10, conv_layers_config=conv_layers_config)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Initialize optimizer (e.g., Adam) and train the model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train the model on the training set\n",
    "        best_gradients = train_model(model, train, val, criterion, optimizer, num_epochs, device)\n",
    "        \n",
    "        # Apply the best gradients to the model parameters\n",
    "        if best_gradients is not None:\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in best_gradients and param.grad is not None:\n",
    "                        param.grad.copy_(best_gradients[name])\n",
    "        \n",
    "        # Evaluate the model on the test set with best gradients applied\n",
    "        test_loss, accuracy = test_model_configuration(model, test, criterion, device)\n",
    "        \n",
    "        # Print the results for the current configuration\n",
    "        print(f\"Configuration {idx + 1} Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save the best model based on accuracy; if accuracy is the same, use loss as the tie-breaker\n",
    "        if accuracy > best_accuracy or (accuracy == best_accuracy and test_loss < best_loss):\n",
    "            best_accuracy = accuracy\n",
    "            best_loss = test_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(f\"New best model found with accuracy: {best_accuracy:.2f}% and loss: {best_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nBest Model Test Accuracy: {best_accuracy:.2f}% with Loss: {best_loss:.4f}\")\n",
    "    return best_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Configuration 1 with convolutional layers: [(32, 3, 1, 1), (64, 3, 1, 1), (128, 3, 1, 1)]\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 2.4088\n",
      "  Batch 20/35 - Loss: 2.3754\n",
      "  Batch 30/35 - Loss: 2.2925\n",
      "  Batch 35/35 - Loss: 2.3715\n",
      "Epoch 1 Training completed. Average Loss: 30.7698\n",
      "Epoch 1 Validation completed. Average Loss: 2.3298\n",
      "New best gradients stored for epoch 1 with validation loss 2.3298 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.2604\n",
      "  Batch 20/35 - Loss: 2.1988\n",
      "  Batch 30/35 - Loss: 2.1564\n",
      "  Batch 35/35 - Loss: 1.9377\n",
      "Epoch 2 Training completed. Average Loss: 2.2236\n",
      "Epoch 2 Validation completed. Average Loss: 2.6260\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.2124\n",
      "  Batch 20/35 - Loss: 2.2867\n",
      "  Batch 30/35 - Loss: 2.2275\n",
      "  Batch 35/35 - Loss: 2.1500\n",
      "Epoch 3 Training completed. Average Loss: 2.2205\n",
      "Epoch 3 Validation completed. Average Loss: 2.2919\n",
      "New best gradients stored for epoch 3 with validation loss 2.2919 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 2.3097\n",
      "  Batch 20/35 - Loss: 2.2344\n",
      "  Batch 30/35 - Loss: 2.0640\n",
      "  Batch 35/35 - Loss: 1.8509\n",
      "Epoch 4 Training completed. Average Loss: 2.1821\n",
      "Epoch 4 Validation completed. Average Loss: 2.5145\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 2.1010\n",
      "  Batch 20/35 - Loss: 2.0482\n",
      "  Batch 30/35 - Loss: 1.7500\n",
      "  Batch 35/35 - Loss: 1.3531\n",
      "Epoch 5 Training completed. Average Loss: 1.9722\n",
      "Epoch 5 Validation completed. Average Loss: 2.6217\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 2.3711\n",
      "  Batch 20/35 - Loss: 1.8272\n",
      "  Batch 30/35 - Loss: 1.9198\n",
      "  Batch 35/35 - Loss: 1.1642\n",
      "Epoch 6 Training completed. Average Loss: 1.7435\n",
      "Epoch 6 Validation completed. Average Loss: 2.6053\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 1.8394\n",
      "  Batch 20/35 - Loss: 1.7349\n",
      "  Batch 30/35 - Loss: 1.4563\n",
      "  Batch 35/35 - Loss: 1.1622\n",
      "Epoch 7 Training completed. Average Loss: 1.7247\n",
      "Epoch 7 Validation completed. Average Loss: 3.1794\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.9867\n",
      "  Batch 20/35 - Loss: 1.4865\n",
      "  Batch 30/35 - Loss: 1.1504\n",
      "  Batch 35/35 - Loss: 0.6395\n",
      "Epoch 8 Training completed. Average Loss: 1.1960\n",
      "Epoch 8 Validation completed. Average Loss: 4.8233\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.3589\n",
      "  Batch 20/35 - Loss: 1.3858\n",
      "  Batch 30/35 - Loss: 0.9966\n",
      "  Batch 35/35 - Loss: 0.2559\n",
      "Epoch 9 Training completed. Average Loss: 1.1410\n",
      "Epoch 9 Validation completed. Average Loss: 4.1400\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.3775\n",
      "  Batch 20/35 - Loss: 1.2441\n",
      "  Batch 30/35 - Loss: 0.7576\n",
      "  Batch 35/35 - Loss: 0.1595\n",
      "Epoch 10 Training completed. Average Loss: 0.6731\n",
      "Epoch 10 Validation completed. Average Loss: 4.5160\n",
      "\n",
      "Training complete.\n",
      "Configuration 1 Test Loss: 4.1936, Test Accuracy: 18.12%\n",
      "New best model found with accuracy: 18.12% and loss: 4.1936\n",
      "\n",
      "Testing Configuration 2 with convolutional layers: [(32, 5, 1, 2), (64, 5, 1, 2)]\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 3.5913\n",
      "  Batch 20/35 - Loss: 2.4689\n",
      "  Batch 30/35 - Loss: 2.2596\n",
      "  Batch 35/35 - Loss: 2.2714\n",
      "Epoch 1 Training completed. Average Loss: 62.9120\n",
      "Epoch 1 Validation completed. Average Loss: 2.3016\n",
      "New best gradients stored for epoch 1 with validation loss 2.3016 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.2745\n",
      "  Batch 20/35 - Loss: 2.2868\n",
      "  Batch 30/35 - Loss: 2.2812\n",
      "  Batch 35/35 - Loss: 2.1474\n",
      "Epoch 2 Training completed. Average Loss: 2.2137\n",
      "Epoch 2 Validation completed. Average Loss: 2.3337\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.1652\n",
      "  Batch 20/35 - Loss: 1.9436\n",
      "  Batch 30/35 - Loss: 1.5324\n",
      "  Batch 35/35 - Loss: 1.5476\n",
      "Epoch 3 Training completed. Average Loss: 1.9290\n",
      "Epoch 3 Validation completed. Average Loss: 2.5625\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 1.5794\n",
      "  Batch 20/35 - Loss: 1.5266\n",
      "  Batch 30/35 - Loss: 1.1697\n",
      "  Batch 35/35 - Loss: 1.0439\n",
      "Epoch 4 Training completed. Average Loss: 1.3977\n",
      "Epoch 4 Validation completed. Average Loss: 3.3072\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 1.2914\n",
      "  Batch 20/35 - Loss: 1.0417\n",
      "  Batch 30/35 - Loss: 0.8704\n",
      "  Batch 35/35 - Loss: 0.3759\n",
      "Epoch 5 Training completed. Average Loss: 0.9700\n",
      "Epoch 5 Validation completed. Average Loss: 4.7309\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 1.4970\n",
      "  Batch 20/35 - Loss: 0.6067\n",
      "  Batch 30/35 - Loss: 1.1851\n",
      "  Batch 35/35 - Loss: 0.3382\n",
      "Epoch 6 Training completed. Average Loss: 0.7195\n",
      "Epoch 6 Validation completed. Average Loss: 6.9143\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 1.1898\n",
      "  Batch 20/35 - Loss: 0.4332\n",
      "  Batch 30/35 - Loss: 0.6951\n",
      "  Batch 35/35 - Loss: 0.1657\n",
      "Epoch 7 Training completed. Average Loss: 0.6495\n",
      "Epoch 7 Validation completed. Average Loss: 4.6306\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.3045\n",
      "  Batch 20/35 - Loss: 0.2727\n",
      "  Batch 30/35 - Loss: 0.4481\n",
      "  Batch 35/35 - Loss: 0.1429\n",
      "Epoch 8 Training completed. Average Loss: 0.4374\n",
      "Epoch 8 Validation completed. Average Loss: 5.8327\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 3.1191\n",
      "  Batch 20/35 - Loss: 0.2694\n",
      "  Batch 30/35 - Loss: 0.2829\n",
      "  Batch 35/35 - Loss: 0.1554\n",
      "Epoch 9 Training completed. Average Loss: 0.3881\n",
      "Epoch 9 Validation completed. Average Loss: 7.4551\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.0370\n",
      "  Batch 20/35 - Loss: 0.1918\n",
      "  Batch 30/35 - Loss: 0.3151\n",
      "  Batch 35/35 - Loss: 0.8174\n",
      "Epoch 10 Training completed. Average Loss: 0.3387\n",
      "Epoch 10 Validation completed. Average Loss: 7.3342\n",
      "\n",
      "Training complete.\n",
      "Configuration 2 Test Loss: 10.1080, Test Accuracy: 20.81%\n",
      "New best model found with accuracy: 20.81% and loss: 10.1080\n",
      "\n",
      "Testing Configuration 3 with convolutional layers: [(16, 3, 1, 1), (32, 3, 1, 1), (64, 3, 1, 1)]\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 14.9619\n",
      "  Batch 20/35 - Loss: 2.5757\n",
      "  Batch 30/35 - Loss: 2.2579\n",
      "  Batch 35/35 - Loss: 2.2158\n",
      "Epoch 1 Training completed. Average Loss: 21.9689\n",
      "Epoch 1 Validation completed. Average Loss: 2.2796\n",
      "New best gradients stored for epoch 1 with validation loss 2.2796 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.2393\n",
      "  Batch 20/35 - Loss: 2.2100\n",
      "  Batch 30/35 - Loss: 2.0942\n",
      "  Batch 35/35 - Loss: 2.1663\n",
      "Epoch 2 Training completed. Average Loss: 2.1976\n",
      "Epoch 2 Validation completed. Average Loss: 2.3154\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 1.8972\n",
      "  Batch 20/35 - Loss: 2.0330\n",
      "  Batch 30/35 - Loss: 1.8619\n",
      "  Batch 35/35 - Loss: 1.2643\n",
      "Epoch 3 Training completed. Average Loss: 1.9393\n",
      "Epoch 3 Validation completed. Average Loss: 2.5371\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 1.6379\n",
      "  Batch 20/35 - Loss: 1.3599\n",
      "  Batch 30/35 - Loss: 1.3110\n",
      "  Batch 35/35 - Loss: 0.9642\n",
      "Epoch 4 Training completed. Average Loss: 1.5726\n",
      "Epoch 4 Validation completed. Average Loss: 3.4747\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 1.3697\n",
      "  Batch 20/35 - Loss: 1.2795\n",
      "  Batch 30/35 - Loss: 1.0774\n",
      "  Batch 35/35 - Loss: 1.0891\n",
      "Epoch 5 Training completed. Average Loss: 1.4272\n",
      "Epoch 5 Validation completed. Average Loss: 4.0916\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 0.9730\n",
      "  Batch 20/35 - Loss: 0.8062\n",
      "  Batch 30/35 - Loss: 1.4601\n",
      "  Batch 35/35 - Loss: 0.4618\n",
      "Epoch 6 Training completed. Average Loss: 1.1617\n",
      "Epoch 6 Validation completed. Average Loss: 4.0499\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.6534\n",
      "  Batch 20/35 - Loss: 0.8779\n",
      "  Batch 30/35 - Loss: 0.8772\n",
      "  Batch 35/35 - Loss: 0.2820\n",
      "Epoch 7 Training completed. Average Loss: 1.0430\n",
      "Epoch 7 Validation completed. Average Loss: 4.8591\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.3253\n",
      "  Batch 20/35 - Loss: 0.4285\n",
      "  Batch 30/35 - Loss: 0.7193\n",
      "  Batch 35/35 - Loss: 0.4337\n",
      "Epoch 8 Training completed. Average Loss: 0.6361\n",
      "Epoch 8 Validation completed. Average Loss: 4.2161\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.2022\n",
      "  Batch 20/35 - Loss: 0.4134\n",
      "  Batch 30/35 - Loss: 0.5416\n",
      "  Batch 35/35 - Loss: 0.1405\n",
      "Epoch 9 Training completed. Average Loss: 0.4129\n",
      "Epoch 9 Validation completed. Average Loss: 5.9095\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.3757\n",
      "  Batch 20/35 - Loss: 1.4173\n",
      "  Batch 30/35 - Loss: 0.4354\n",
      "  Batch 35/35 - Loss: 0.6941\n",
      "Epoch 10 Training completed. Average Loss: 0.4645\n",
      "Epoch 10 Validation completed. Average Loss: 5.3936\n",
      "\n",
      "Training complete.\n",
      "Configuration 3 Test Loss: 5.6470, Test Accuracy: 24.16%\n",
      "New best model found with accuracy: 24.16% and loss: 5.6470\n",
      "\n",
      "Testing Configuration 4 with convolutional layers: [(64, 3, 1, 1), (128, 3, 1, 1), (256, 3, 1, 1)]\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 2.2190\n",
      "  Batch 20/35 - Loss: 2.3018\n",
      "  Batch 30/35 - Loss: 2.3321\n",
      "  Batch 35/35 - Loss: 2.3153\n",
      "Epoch 1 Training completed. Average Loss: 97.5673\n",
      "Epoch 1 Validation completed. Average Loss: 2.3189\n",
      "New best gradients stored for epoch 1 with validation loss 2.3189 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.2282\n",
      "  Batch 20/35 - Loss: 2.2963\n",
      "  Batch 30/35 - Loss: 2.2338\n",
      "  Batch 35/35 - Loss: 2.0849\n",
      "Epoch 2 Training completed. Average Loss: 2.2733\n",
      "Epoch 2 Validation completed. Average Loss: 2.3161\n",
      "New best gradients stored for epoch 2 with validation loss 2.3161 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.0768\n",
      "  Batch 20/35 - Loss: 2.2461\n",
      "  Batch 30/35 - Loss: 2.1971\n",
      "  Batch 35/35 - Loss: 2.2955\n",
      "Epoch 3 Training completed. Average Loss: 2.2073\n",
      "Epoch 3 Validation completed. Average Loss: 2.2937\n",
      "New best gradients stored for epoch 3 with validation loss 2.2937 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 2.2958\n",
      "  Batch 20/35 - Loss: 1.7131\n",
      "  Batch 30/35 - Loss: 2.1468\n",
      "  Batch 35/35 - Loss: 2.2255\n",
      "Epoch 4 Training completed. Average Loss: 2.1791\n",
      "Epoch 4 Validation completed. Average Loss: 2.2848\n",
      "New best gradients stored for epoch 4 with validation loss 2.2848 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 2.3167\n",
      "  Batch 20/35 - Loss: 2.1773\n",
      "  Batch 30/35 - Loss: 1.9777\n",
      "  Batch 35/35 - Loss: 2.0165\n",
      "Epoch 5 Training completed. Average Loss: 2.2159\n",
      "Epoch 5 Validation completed. Average Loss: 2.4100\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 2.1631\n",
      "  Batch 20/35 - Loss: 2.0772\n",
      "  Batch 30/35 - Loss: 1.8935\n",
      "  Batch 35/35 - Loss: 1.0326\n",
      "Epoch 6 Training completed. Average Loss: 1.9529\n",
      "Epoch 6 Validation completed. Average Loss: 2.7054\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 1.8213\n",
      "  Batch 20/35 - Loss: 1.8132\n",
      "  Batch 30/35 - Loss: 1.6329\n",
      "  Batch 35/35 - Loss: 0.8085\n",
      "Epoch 7 Training completed. Average Loss: 1.7522\n",
      "Epoch 7 Validation completed. Average Loss: 2.8862\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 1.4747\n",
      "  Batch 20/35 - Loss: 1.6213\n",
      "  Batch 30/35 - Loss: 1.2911\n",
      "  Batch 35/35 - Loss: 0.7351\n",
      "Epoch 8 Training completed. Average Loss: 1.4560\n",
      "Epoch 8 Validation completed. Average Loss: 3.5435\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.8616\n",
      "  Batch 20/35 - Loss: 1.5501\n",
      "  Batch 30/35 - Loss: 1.1140\n",
      "  Batch 35/35 - Loss: 0.7223\n",
      "Epoch 9 Training completed. Average Loss: 1.2828\n",
      "Epoch 9 Validation completed. Average Loss: 3.7855\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.6579\n",
      "  Batch 20/35 - Loss: 1.3027\n",
      "  Batch 30/35 - Loss: 1.0525\n",
      "  Batch 35/35 - Loss: 0.6074\n",
      "Epoch 10 Training completed. Average Loss: 1.1238\n",
      "Epoch 10 Validation completed. Average Loss: 5.1092\n",
      "\n",
      "Training complete.\n",
      "Configuration 4 Test Loss: 4.8880, Test Accuracy: 20.13%\n",
      "\n",
      "Best Model Test Accuracy: 24.16% with Loss: 5.6470\n"
     ]
    }
   ],
   "source": [
    "# Assume 'test_loader' is the DataLoader for your test set, and 'criterion' is the loss function (e.g., CrossEntropyLoss)\n",
    "best_model = test_multiple_configurations(train_spectogram, val_spectogram, test_spectogram, criterion, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.647017657756805 - Model: CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=110400, out_features=64, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss: {best_model[0]} - Model: {best_model[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.647017657756805, 24.161073825503355)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_best_model = best_model[1]\n",
    "\n",
    "test_model_configuration(cnn_best_model,test_spectogram,criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class different_act_CNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=10, activation_function=\"relu\"):\n",
    "        super(different_act_CNN, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers (fixed as per example)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),  # Conv layer 1\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # Conv layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)              # Conv layer 3\n",
    "        ])\n",
    "        \n",
    "        # Max Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the output feature size after conv layers and pooling\n",
    "        self.final_feature_map_size = self._get_conv_output_size(input_channels)\n",
    "\n",
    "        # Define fully connected layers (fixed as per example)\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(self.final_feature_map_size, 64),  # First fully connected layer\n",
    "            nn.Linear(64, 64),                           # Second fully connected layer\n",
    "            nn.Linear(64, 64)                            # Third fully connected layer\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Set activation function\n",
    "        self.activation = self._get_activation_function(activation_function)\n",
    "\n",
    "    def _get_conv_output_size(self, input_channels):\n",
    "        # Sample input size (height x width)\n",
    "        height, width = 201, 552  # Replace with actual input height and width\n",
    "\n",
    "        # Pass through convolutional and pooling layers to determine final size\n",
    "        for layer in self.conv_layers:\n",
    "            height = (height + 2 * layer.padding[0] - layer.kernel_size[0]) // layer.stride[0] + 1\n",
    "            width = (width + 2 * layer.padding[1] - layer.kernel_size[1]) // layer.stride[1] + 1\n",
    "            height //= 2  # Max pooling halves the height\n",
    "            width //= 2   # Max pooling halves the width\n",
    "\n",
    "        # Output feature map size\n",
    "        return height * width * 128\n",
    "\n",
    "    def _get_activation_function(self, activation_function):\n",
    "        # Map the string to the appropriate activation function\n",
    "        if activation_function == \"relu\":\n",
    "            return F.relu\n",
    "        elif activation_function == \"leaky_relu\":\n",
    "            return F.leaky_relu\n",
    "        elif activation_function == \"tanh\":\n",
    "            return torch.tanh\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            return torch.sigmoid\n",
    "        elif activation_function == \"softmax\":\n",
    "            return F.softmax\n",
    "        elif activation_function == \"elu\":\n",
    "            return F.elu\n",
    "        elif activation_function == \"selu\":\n",
    "            return F.selu\n",
    "        elif activation_function == \"gelu\":\n",
    "            return F.gelu\n",
    "        elif activation_function == \"swish\":\n",
    "            return lambda x: x * torch.sigmoid(x)  # Swish activation: x * sigmoid(x)\n",
    "        elif activation_function == \"hard_sigmoid\":\n",
    "            return F.hardsigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_function}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through conv layers with the selected activation function and pooling\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = self.activation(conv_layer(x))\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output from conv layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Forward pass through fully connected layers with the selected activation function\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = self.activation(fc_layer(x))\n",
    "        \n",
    "        # Final output layer\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test different activation functions\n",
    "def test_multiple_activation_functions(train, val, test, criterion, device, num_epochs=10):\n",
    "    activation_functions = [\"relu\", \"leaky_relu\", \"tanh\", \"sigmoid\", \"softmax\", \"elu\", \"selu\", \"gelu\", \"swish\", \"hard_sigmoid\"]\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = float(\"inf\")\n",
    "    \n",
    "    for activation_function in activation_functions:\n",
    "        print(f\"\\nTesting activation function: {activation_function}\")\n",
    "        \n",
    "        model = different_act_CNN(input_channels=1, num_classes=10, activation_function=activation_function)\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        best_gradients = train_model(model, train, val, criterion, optimizer, num_epochs, device)\n",
    "        \n",
    "        if best_gradients is not None:\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in best_gradients and param.grad is not None:\n",
    "                        param.grad.copy_(best_gradients[name])\n",
    "        \n",
    "        test_loss, accuracy = test_model_configuration(model, test, criterion, device)\n",
    "        \n",
    "        print(f\"Activation function: {activation_function} Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        if accuracy > best_accuracy or (accuracy == best_accuracy and test_loss < best_loss):\n",
    "            best_accuracy = accuracy\n",
    "            best_loss = test_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(f\"New best model found with accuracy: {best_accuracy:.2f}% and loss: {best_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nBest Model Test Accuracy: {best_accuracy:.2f}% with Loss: {best_loss:.4f}\")\n",
    "    return best_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing activation function: relu\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 3.7923\n",
      "  Batch 20/35 - Loss: 2.1062\n",
      "  Batch 30/35 - Loss: 2.2496\n",
      "  Batch 35/35 - Loss: 2.1477\n",
      "Epoch 1 Training completed. Average Loss: 26.8719\n",
      "Epoch 1 Validation completed. Average Loss: 2.2374\n",
      "New best gradients stored for epoch 1 with validation loss 2.2374 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 1.9306\n",
      "  Batch 20/35 - Loss: 2.1099\n",
      "  Batch 30/35 - Loss: 2.0890\n",
      "  Batch 35/35 - Loss: 2.0647\n",
      "Epoch 2 Training completed. Average Loss: 2.1597\n",
      "Epoch 2 Validation completed. Average Loss: 2.2873\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 1.9281\n",
      "  Batch 20/35 - Loss: 1.9140\n",
      "  Batch 30/35 - Loss: 1.9856\n",
      "  Batch 35/35 - Loss: 1.0595\n",
      "Epoch 3 Training completed. Average Loss: 1.9783\n",
      "Epoch 3 Validation completed. Average Loss: 2.6419\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 1.4790\n",
      "  Batch 20/35 - Loss: 1.7335\n",
      "  Batch 30/35 - Loss: 1.5896\n",
      "  Batch 35/35 - Loss: 0.8898\n",
      "Epoch 4 Training completed. Average Loss: 1.7025\n",
      "Epoch 4 Validation completed. Average Loss: 2.4510\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 1.2404\n",
      "  Batch 20/35 - Loss: 2.1872\n",
      "  Batch 30/35 - Loss: 1.7079\n",
      "  Batch 35/35 - Loss: 0.7836\n",
      "Epoch 5 Training completed. Average Loss: 1.4306\n",
      "Epoch 5 Validation completed. Average Loss: 2.7713\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 0.8001\n",
      "  Batch 20/35 - Loss: 1.5508\n",
      "  Batch 30/35 - Loss: 1.1065\n",
      "  Batch 35/35 - Loss: 0.8646\n",
      "Epoch 6 Training completed. Average Loss: 1.2517\n",
      "Epoch 6 Validation completed. Average Loss: 3.2873\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.3989\n",
      "  Batch 20/35 - Loss: 1.2879\n",
      "  Batch 30/35 - Loss: 0.9084\n",
      "  Batch 35/35 - Loss: 2.1263\n",
      "Epoch 7 Training completed. Average Loss: 1.1599\n",
      "Epoch 7 Validation completed. Average Loss: 3.5220\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.5647\n",
      "  Batch 20/35 - Loss: 0.7495\n",
      "  Batch 30/35 - Loss: 2.8392\n",
      "  Batch 35/35 - Loss: 0.4742\n",
      "Epoch 8 Training completed. Average Loss: 0.9778\n",
      "Epoch 8 Validation completed. Average Loss: 5.9715\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.6110\n",
      "  Batch 20/35 - Loss: 0.5696\n",
      "  Batch 30/35 - Loss: 0.9748\n",
      "  Batch 35/35 - Loss: 0.2307\n",
      "Epoch 9 Training completed. Average Loss: 0.7857\n",
      "Epoch 9 Validation completed. Average Loss: 5.2690\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.0999\n",
      "  Batch 20/35 - Loss: 0.6516\n",
      "  Batch 30/35 - Loss: 0.7691\n",
      "  Batch 35/35 - Loss: 0.4274\n",
      "Epoch 10 Training completed. Average Loss: 0.6566\n",
      "Epoch 10 Validation completed. Average Loss: 4.6769\n",
      "\n",
      "Training complete.\n",
      "Activation function: relu Test Loss: 5.3046, Test Accuracy: 24.83%\n",
      "New best model found with accuracy: 24.83% and loss: 5.3046\n",
      "\n",
      "Testing activation function: leaky_relu\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 8.7988\n",
      "  Batch 20/35 - Loss: 4.2011\n",
      "  Batch 30/35 - Loss: 3.2756\n",
      "  Batch 35/35 - Loss: 2.6578\n",
      "Epoch 1 Training completed. Average Loss: 21.3344\n",
      "Epoch 1 Validation completed. Average Loss: 2.6511\n",
      "New best gradients stored for epoch 1 with validation loss 2.6511 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.4474\n",
      "  Batch 20/35 - Loss: 2.1955\n",
      "  Batch 30/35 - Loss: 2.2783\n",
      "  Batch 35/35 - Loss: 2.1841\n",
      "Epoch 2 Training completed. Average Loss: 2.3320\n",
      "Epoch 2 Validation completed. Average Loss: 2.2252\n",
      "New best gradients stored for epoch 2 with validation loss 2.2252 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.3312\n",
      "  Batch 20/35 - Loss: 2.2542\n",
      "  Batch 30/35 - Loss: 2.4970\n",
      "  Batch 35/35 - Loss: 2.1800\n",
      "Epoch 3 Training completed. Average Loss: 2.1108\n",
      "Epoch 3 Validation completed. Average Loss: 2.4387\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 1.9175\n",
      "  Batch 20/35 - Loss: 2.0090\n",
      "  Batch 30/35 - Loss: 1.8922\n",
      "  Batch 35/35 - Loss: 1.8949\n",
      "Epoch 4 Training completed. Average Loss: 1.9626\n",
      "Epoch 4 Validation completed. Average Loss: 2.0770\n",
      "New best gradients stored for epoch 4 with validation loss 2.0770 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 1.5523\n",
      "  Batch 20/35 - Loss: 1.7411\n",
      "  Batch 30/35 - Loss: 1.6244\n",
      "  Batch 35/35 - Loss: 1.5998\n",
      "Epoch 5 Training completed. Average Loss: 1.4914\n",
      "Epoch 5 Validation completed. Average Loss: 2.0933\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 1.3934\n",
      "  Batch 20/35 - Loss: 1.7554\n",
      "  Batch 30/35 - Loss: 1.2311\n",
      "  Batch 35/35 - Loss: 0.8086\n",
      "Epoch 6 Training completed. Average Loss: 1.2015\n",
      "Epoch 6 Validation completed. Average Loss: 2.1698\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.9766\n",
      "  Batch 20/35 - Loss: 1.4726\n",
      "  Batch 30/35 - Loss: 1.0739\n",
      "  Batch 35/35 - Loss: 0.8039\n",
      "Epoch 7 Training completed. Average Loss: 0.9994\n",
      "Epoch 7 Validation completed. Average Loss: 2.1198\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.8936\n",
      "  Batch 20/35 - Loss: 0.8336\n",
      "  Batch 30/35 - Loss: 1.2698\n",
      "  Batch 35/35 - Loss: 0.3625\n",
      "Epoch 8 Training completed. Average Loss: 0.8813\n",
      "Epoch 8 Validation completed. Average Loss: 3.0412\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 2.5067\n",
      "  Batch 20/35 - Loss: 0.9376\n",
      "  Batch 30/35 - Loss: 0.9451\n",
      "  Batch 35/35 - Loss: 0.0833\n",
      "Epoch 9 Training completed. Average Loss: 0.8792\n",
      "Epoch 9 Validation completed. Average Loss: 2.3592\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.7738\n",
      "  Batch 20/35 - Loss: 1.1766\n",
      "  Batch 30/35 - Loss: 0.5315\n",
      "  Batch 35/35 - Loss: 0.1436\n",
      "Epoch 10 Training completed. Average Loss: 0.6397\n",
      "Epoch 10 Validation completed. Average Loss: 3.2950\n",
      "\n",
      "Training complete.\n",
      "Activation function: leaky_relu Test Loss: 3.7380, Test Accuracy: 35.57%\n",
      "New best model found with accuracy: 35.57% and loss: 3.7380\n",
      "\n",
      "Testing activation function: tanh\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 2.1363\n",
      "  Batch 20/35 - Loss: 2.1807\n",
      "  Batch 30/35 - Loss: 2.2001\n",
      "  Batch 35/35 - Loss: 2.0616\n",
      "Epoch 1 Training completed. Average Loss: 2.1832\n",
      "Epoch 1 Validation completed. Average Loss: 2.0894\n",
      "New best gradients stored for epoch 1 with validation loss 2.0894 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.0730\n",
      "  Batch 20/35 - Loss: 2.0757\n",
      "  Batch 30/35 - Loss: 2.0647\n",
      "  Batch 35/35 - Loss: 2.0206\n",
      "Epoch 2 Training completed. Average Loss: 2.0466\n",
      "Epoch 2 Validation completed. Average Loss: 2.0524\n",
      "New best gradients stored for epoch 2 with validation loss 2.0524 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.0726\n",
      "  Batch 20/35 - Loss: 2.0868\n",
      "  Batch 30/35 - Loss: 2.0649\n",
      "  Batch 35/35 - Loss: 1.9201\n",
      "Epoch 3 Training completed. Average Loss: 2.0279\n",
      "Epoch 3 Validation completed. Average Loss: 2.0185\n",
      "New best gradients stored for epoch 3 with validation loss 2.0185 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 2.0234\n",
      "  Batch 20/35 - Loss: 1.9359\n",
      "  Batch 30/35 - Loss: 1.9899\n",
      "  Batch 35/35 - Loss: 2.0941\n",
      "Epoch 4 Training completed. Average Loss: 2.0718\n",
      "Epoch 4 Validation completed. Average Loss: 2.0805\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 2.1131\n",
      "  Batch 20/35 - Loss: 2.1312\n",
      "  Batch 30/35 - Loss: 2.0804\n",
      "  Batch 35/35 - Loss: 2.1219\n",
      "Epoch 5 Training completed. Average Loss: 2.0610\n",
      "Epoch 5 Validation completed. Average Loss: 2.0293\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 2.0538\n",
      "  Batch 20/35 - Loss: 2.0916\n",
      "  Batch 30/35 - Loss: 2.0969\n",
      "  Batch 35/35 - Loss: 2.0314\n",
      "Epoch 6 Training completed. Average Loss: 2.0675\n",
      "Epoch 6 Validation completed. Average Loss: 2.0498\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 2.0526\n",
      "  Batch 20/35 - Loss: 1.8897\n",
      "  Batch 30/35 - Loss: 2.0146\n",
      "  Batch 35/35 - Loss: 1.9663\n",
      "Epoch 7 Training completed. Average Loss: 2.0019\n",
      "Epoch 7 Validation completed. Average Loss: 1.9855\n",
      "New best gradients stored for epoch 7 with validation loss 1.9855 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 2.0438\n",
      "  Batch 20/35 - Loss: 1.9291\n",
      "  Batch 30/35 - Loss: 1.9993\n",
      "  Batch 35/35 - Loss: 1.9113\n",
      "Epoch 8 Training completed. Average Loss: 1.9758\n",
      "Epoch 8 Validation completed. Average Loss: 2.0017\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 2.0860\n",
      "  Batch 20/35 - Loss: 1.9348\n",
      "  Batch 30/35 - Loss: 2.0566\n",
      "  Batch 35/35 - Loss: 1.9268\n",
      "Epoch 9 Training completed. Average Loss: 1.9913\n",
      "Epoch 9 Validation completed. Average Loss: 2.0647\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 2.1262\n",
      "  Batch 20/35 - Loss: 2.3298\n",
      "  Batch 30/35 - Loss: 2.2130\n",
      "  Batch 35/35 - Loss: 2.2223\n",
      "Epoch 10 Training completed. Average Loss: 2.1637\n",
      "Epoch 10 Validation completed. Average Loss: 2.1605\n",
      "\n",
      "Training complete.\n",
      "Activation function: tanh Test Loss: 2.1557, Test Accuracy: 20.81%\n",
      "\n",
      "Testing activation function: sigmoid\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 2.3057\n",
      "  Batch 20/35 - Loss: 2.3786\n",
      "  Batch 30/35 - Loss: 2.3125\n",
      "  Batch 35/35 - Loss: 2.3123\n",
      "Epoch 1 Training completed. Average Loss: 2.3251\n",
      "Epoch 1 Validation completed. Average Loss: 2.3027\n",
      "New best gradients stored for epoch 1 with validation loss 2.3027 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.3129\n",
      "  Batch 20/35 - Loss: 2.2961\n",
      "  Batch 30/35 - Loss: 2.3052\n",
      "  Batch 35/35 - Loss: 2.3083\n",
      "Epoch 2 Training completed. Average Loss: 2.3087\n",
      "Epoch 2 Validation completed. Average Loss: 2.3019\n",
      "New best gradients stored for epoch 2 with validation loss 2.3019 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.3133\n",
      "  Batch 20/35 - Loss: 2.3081\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3102\n",
      "Epoch 3 Training completed. Average Loss: 2.3086\n",
      "Epoch 3 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 2.3126\n",
      "  Batch 20/35 - Loss: 2.3074\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3096\n",
      "Epoch 4 Training completed. Average Loss: 2.3085\n",
      "Epoch 4 Validation completed. Average Loss: 2.3021\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 2.3125\n",
      "  Batch 20/35 - Loss: 2.3072\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3094\n",
      "Epoch 5 Training completed. Average Loss: 2.3084\n",
      "Epoch 5 Validation completed. Average Loss: 2.3021\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 2.3123\n",
      "  Batch 20/35 - Loss: 2.3071\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3092\n",
      "Epoch 6 Training completed. Average Loss: 2.3084\n",
      "Epoch 6 Validation completed. Average Loss: 2.3021\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 2.3121\n",
      "  Batch 20/35 - Loss: 2.3070\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3090\n",
      "Epoch 7 Training completed. Average Loss: 2.3083\n",
      "Epoch 7 Validation completed. Average Loss: 2.3021\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 2.3120\n",
      "  Batch 20/35 - Loss: 2.3069\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3088\n",
      "Epoch 8 Training completed. Average Loss: 2.3083\n",
      "Epoch 8 Validation completed. Average Loss: 2.3021\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 2.3118\n",
      "  Batch 20/35 - Loss: 2.3068\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3086\n",
      "Epoch 9 Training completed. Average Loss: 2.3082\n",
      "Epoch 9 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 2.3117\n",
      "  Batch 20/35 - Loss: 2.3067\n",
      "  Batch 30/35 - Loss: 2.3060\n",
      "  Batch 35/35 - Loss: 2.3084\n",
      "Epoch 10 Training completed. Average Loss: 2.3082\n",
      "Epoch 10 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Training complete.\n",
      "Activation function: sigmoid Test Loss: 2.3028, Test Accuracy: 10.07%\n",
      "\n",
      "Testing activation function: softmax\n",
      "\n",
      "Starting Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_11576\\3811288260.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.activation(conv_layer(x))\n",
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_11576\\3811288260.py:81: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.activation(fc_layer(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/35 - Loss: 2.2922\n",
      "  Batch 20/35 - Loss: 2.3140\n",
      "  Batch 30/35 - Loss: 2.2875\n",
      "  Batch 35/35 - Loss: 2.3146\n",
      "Epoch 1 Training completed. Average Loss: 2.3056\n",
      "Epoch 1 Validation completed. Average Loss: 2.3068\n",
      "New best gradients stored for epoch 1 with validation loss 2.3068 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.2926\n",
      "  Batch 20/35 - Loss: 2.3122\n",
      "  Batch 30/35 - Loss: 2.2891\n",
      "  Batch 35/35 - Loss: 2.3119\n",
      "Epoch 2 Training completed. Average Loss: 2.3048\n",
      "Epoch 2 Validation completed. Average Loss: 2.3061\n",
      "New best gradients stored for epoch 2 with validation loss 2.3061 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.2934\n",
      "  Batch 20/35 - Loss: 2.3104\n",
      "  Batch 30/35 - Loss: 2.2906\n",
      "  Batch 35/35 - Loss: 2.3094\n",
      "Epoch 3 Training completed. Average Loss: 2.3044\n",
      "Epoch 3 Validation completed. Average Loss: 2.3055\n",
      "New best gradients stored for epoch 3 with validation loss 2.3055 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 2.2942\n",
      "  Batch 20/35 - Loss: 2.3089\n",
      "  Batch 30/35 - Loss: 2.2920\n",
      "  Batch 35/35 - Loss: 2.3073\n",
      "Epoch 4 Training completed. Average Loss: 2.3040\n",
      "Epoch 4 Validation completed. Average Loss: 2.3050\n",
      "New best gradients stored for epoch 4 with validation loss 2.3050 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 2.2949\n",
      "  Batch 20/35 - Loss: 2.3075\n",
      "  Batch 30/35 - Loss: 2.2933\n",
      "  Batch 35/35 - Loss: 2.3054\n",
      "Epoch 5 Training completed. Average Loss: 2.3037\n",
      "Epoch 5 Validation completed. Average Loss: 2.3046\n",
      "New best gradients stored for epoch 5 with validation loss 2.3046 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 2.2955\n",
      "  Batch 20/35 - Loss: 2.3063\n",
      "  Batch 30/35 - Loss: 2.2944\n",
      "  Batch 35/35 - Loss: 2.3038\n",
      "Epoch 6 Training completed. Average Loss: 2.3035\n",
      "Epoch 6 Validation completed. Average Loss: 2.3043\n",
      "New best gradients stored for epoch 6 with validation loss 2.3043 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 2.2960\n",
      "  Batch 20/35 - Loss: 2.3053\n",
      "  Batch 30/35 - Loss: 2.2954\n",
      "  Batch 35/35 - Loss: 2.3023\n",
      "Epoch 7 Training completed. Average Loss: 2.3033\n",
      "Epoch 7 Validation completed. Average Loss: 2.3040\n",
      "New best gradients stored for epoch 7 with validation loss 2.3040 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 2.2964\n",
      "  Batch 20/35 - Loss: 2.3044\n",
      "  Batch 30/35 - Loss: 2.2963\n",
      "  Batch 35/35 - Loss: 2.3011\n",
      "Epoch 8 Training completed. Average Loss: 2.3032\n",
      "Epoch 8 Validation completed. Average Loss: 2.3038\n",
      "New best gradients stored for epoch 8 with validation loss 2.3038 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 2.2969\n",
      "  Batch 20/35 - Loss: 2.3036\n",
      "  Batch 30/35 - Loss: 2.2972\n",
      "  Batch 35/35 - Loss: 2.2999\n",
      "Epoch 9 Training completed. Average Loss: 2.3031\n",
      "Epoch 9 Validation completed. Average Loss: 2.3036\n",
      "New best gradients stored for epoch 9 with validation loss 2.3036 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 2.2972\n",
      "  Batch 20/35 - Loss: 2.3029\n",
      "  Batch 30/35 - Loss: 2.2979\n",
      "  Batch 35/35 - Loss: 2.2990\n",
      "Epoch 10 Training completed. Average Loss: 2.3030\n",
      "Epoch 10 Validation completed. Average Loss: 2.3034\n",
      "New best gradients stored for epoch 10 with validation loss 2.3034 and accuracy 0.0%\n",
      "\n",
      "Training complete.\n",
      "Activation function: softmax Test Loss: 2.3030, Test Accuracy: 10.07%\n",
      "\n",
      "Testing activation function: elu\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 9.9485\n",
      "  Batch 20/35 - Loss: 3.3366\n",
      "  Batch 30/35 - Loss: 3.0963\n",
      "  Batch 35/35 - Loss: 3.8330\n",
      "Epoch 1 Training completed. Average Loss: 48.0290\n",
      "Epoch 1 Validation completed. Average Loss: 2.8122\n",
      "New best gradients stored for epoch 1 with validation loss 2.8122 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 3.3167\n",
      "  Batch 20/35 - Loss: 2.9060\n",
      "  Batch 30/35 - Loss: 2.3974\n",
      "  Batch 35/35 - Loss: 1.3190\n",
      "Epoch 2 Training completed. Average Loss: 2.3412\n",
      "Epoch 2 Validation completed. Average Loss: 2.4749\n",
      "New best gradients stored for epoch 2 with validation loss 2.4749 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 1.5676\n",
      "  Batch 20/35 - Loss: 1.3089\n",
      "  Batch 30/35 - Loss: 2.1168\n",
      "  Batch 35/35 - Loss: 0.9593\n",
      "Epoch 3 Training completed. Average Loss: 1.5982\n",
      "Epoch 3 Validation completed. Average Loss: 2.2376\n",
      "New best gradients stored for epoch 3 with validation loss 2.2376 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 0.9592\n",
      "  Batch 20/35 - Loss: 1.7753\n",
      "  Batch 30/35 - Loss: 1.5977\n",
      "  Batch 35/35 - Loss: 0.2692\n",
      "Epoch 4 Training completed. Average Loss: 1.0050\n",
      "Epoch 4 Validation completed. Average Loss: 2.4008\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 0.6336\n",
      "  Batch 20/35 - Loss: 0.7557\n",
      "  Batch 30/35 - Loss: 0.6042\n",
      "  Batch 35/35 - Loss: 0.7419\n",
      "Epoch 5 Training completed. Average Loss: 0.6765\n",
      "Epoch 5 Validation completed. Average Loss: 2.6704\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 0.9558\n",
      "  Batch 20/35 - Loss: 0.4740\n",
      "  Batch 30/35 - Loss: 0.1974\n",
      "  Batch 35/35 - Loss: 0.1484\n",
      "Epoch 6 Training completed. Average Loss: 0.4362\n",
      "Epoch 6 Validation completed. Average Loss: 2.9428\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.4777\n",
      "  Batch 20/35 - Loss: 0.1899\n",
      "  Batch 30/35 - Loss: 0.2311\n",
      "  Batch 35/35 - Loss: 0.0092\n",
      "Epoch 7 Training completed. Average Loss: 0.2511\n",
      "Epoch 7 Validation completed. Average Loss: 3.3203\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.1476\n",
      "  Batch 20/35 - Loss: 0.0669\n",
      "  Batch 30/35 - Loss: 0.0390\n",
      "  Batch 35/35 - Loss: 0.0275\n",
      "Epoch 8 Training completed. Average Loss: 0.1447\n",
      "Epoch 8 Validation completed. Average Loss: 3.3743\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.1106\n",
      "  Batch 20/35 - Loss: 0.0956\n",
      "  Batch 30/35 - Loss: 0.0726\n",
      "  Batch 35/35 - Loss: 0.0098\n",
      "Epoch 9 Training completed. Average Loss: 0.1404\n",
      "Epoch 9 Validation completed. Average Loss: 3.2272\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.0888\n",
      "  Batch 20/35 - Loss: 0.0356\n",
      "  Batch 30/35 - Loss: 0.0960\n",
      "  Batch 35/35 - Loss: 0.0221\n",
      "Epoch 10 Training completed. Average Loss: 0.1322\n",
      "Epoch 10 Validation completed. Average Loss: 3.6677\n",
      "\n",
      "Training complete.\n",
      "Activation function: elu Test Loss: 3.2175, Test Accuracy: 36.91%\n",
      "New best model found with accuracy: 36.91% and loss: 3.2175\n",
      "\n",
      "Testing activation function: selu\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 17.2726\n",
      "  Batch 20/35 - Loss: 5.3375\n",
      "  Batch 30/35 - Loss: 4.4186\n",
      "  Batch 35/35 - Loss: 9.2092\n",
      "Epoch 1 Training completed. Average Loss: 78.9815\n",
      "Epoch 1 Validation completed. Average Loss: 3.9982\n",
      "New best gradients stored for epoch 1 with validation loss 3.9982 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 3.1905\n",
      "  Batch 20/35 - Loss: 1.8540\n",
      "  Batch 30/35 - Loss: 1.5703\n",
      "  Batch 35/35 - Loss: 3.0428\n",
      "Epoch 2 Training completed. Average Loss: 2.7354\n",
      "Epoch 2 Validation completed. Average Loss: 2.5765\n",
      "New best gradients stored for epoch 2 with validation loss 2.5765 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.6210\n",
      "  Batch 20/35 - Loss: 1.6140\n",
      "  Batch 30/35 - Loss: 0.9966\n",
      "  Batch 35/35 - Loss: 2.2797\n",
      "Epoch 3 Training completed. Average Loss: 1.6875\n",
      "Epoch 3 Validation completed. Average Loss: 2.5883\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 0.7918\n",
      "  Batch 20/35 - Loss: 1.0151\n",
      "  Batch 30/35 - Loss: 1.0736\n",
      "  Batch 35/35 - Loss: 4.0711\n",
      "Epoch 4 Training completed. Average Loss: 1.2488\n",
      "Epoch 4 Validation completed. Average Loss: 2.7643\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 0.9816\n",
      "  Batch 20/35 - Loss: 0.8604\n",
      "  Batch 30/35 - Loss: 0.8188\n",
      "  Batch 35/35 - Loss: 0.1383\n",
      "Epoch 5 Training completed. Average Loss: 0.9274\n",
      "Epoch 5 Validation completed. Average Loss: 2.8907\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 0.2803\n",
      "  Batch 20/35 - Loss: 0.5154\n",
      "  Batch 30/35 - Loss: 0.1866\n",
      "  Batch 35/35 - Loss: 0.2249\n",
      "Epoch 6 Training completed. Average Loss: 0.6636\n",
      "Epoch 6 Validation completed. Average Loss: 3.6379\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.8946\n",
      "  Batch 20/35 - Loss: 0.2798\n",
      "  Batch 30/35 - Loss: 0.3409\n",
      "  Batch 35/35 - Loss: 0.0105\n",
      "Epoch 7 Training completed. Average Loss: 0.5034\n",
      "Epoch 7 Validation completed. Average Loss: 3.3978\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.5916\n",
      "  Batch 20/35 - Loss: 1.8341\n",
      "  Batch 30/35 - Loss: 0.1813\n",
      "  Batch 35/35 - Loss: 0.0196\n",
      "Epoch 8 Training completed. Average Loss: 0.2897\n",
      "Epoch 8 Validation completed. Average Loss: 3.5706\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.0367\n",
      "  Batch 20/35 - Loss: 0.1491\n",
      "  Batch 30/35 - Loss: 0.0497\n",
      "  Batch 35/35 - Loss: 0.0289\n",
      "Epoch 9 Training completed. Average Loss: 0.1436\n",
      "Epoch 9 Validation completed. Average Loss: 3.2773\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.0179\n",
      "  Batch 20/35 - Loss: 0.0376\n",
      "  Batch 30/35 - Loss: 0.0223\n",
      "  Batch 35/35 - Loss: 0.1244\n",
      "Epoch 10 Training completed. Average Loss: 0.0693\n",
      "Epoch 10 Validation completed. Average Loss: 3.2500\n",
      "\n",
      "Training complete.\n",
      "Activation function: selu Test Loss: 3.2801, Test Accuracy: 34.23%\n",
      "\n",
      "Testing activation function: gelu\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 3.1180\n",
      "  Batch 20/35 - Loss: 2.3495\n",
      "  Batch 30/35 - Loss: 2.1774\n",
      "  Batch 35/35 - Loss: 2.3405\n",
      "Epoch 1 Training completed. Average Loss: 26.9317\n",
      "Epoch 1 Validation completed. Average Loss: 2.1525\n",
      "New best gradients stored for epoch 1 with validation loss 2.1525 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.1473\n",
      "  Batch 20/35 - Loss: 2.1605\n",
      "  Batch 30/35 - Loss: 1.7937\n",
      "  Batch 35/35 - Loss: 1.7342\n",
      "Epoch 2 Training completed. Average Loss: 1.9920\n",
      "Epoch 2 Validation completed. Average Loss: 2.0654\n",
      "New best gradients stored for epoch 2 with validation loss 2.0654 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 1.9136\n",
      "  Batch 20/35 - Loss: 2.0191\n",
      "  Batch 30/35 - Loss: 1.7329\n",
      "  Batch 35/35 - Loss: 1.4843\n",
      "Epoch 3 Training completed. Average Loss: 1.6673\n",
      "Epoch 3 Validation completed. Average Loss: 2.1518\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 1.6171\n",
      "  Batch 20/35 - Loss: 1.5187\n",
      "  Batch 30/35 - Loss: 1.2170\n",
      "  Batch 35/35 - Loss: 0.9251\n",
      "Epoch 4 Training completed. Average Loss: 1.3849\n",
      "Epoch 4 Validation completed. Average Loss: 2.4670\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 0.9911\n",
      "  Batch 20/35 - Loss: 0.8329\n",
      "  Batch 30/35 - Loss: 1.1363\n",
      "  Batch 35/35 - Loss: 0.4099\n",
      "Epoch 5 Training completed. Average Loss: 0.7896\n",
      "Epoch 5 Validation completed. Average Loss: 2.6695\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 0.5467\n",
      "  Batch 20/35 - Loss: 0.6197\n",
      "  Batch 30/35 - Loss: 1.0836\n",
      "  Batch 35/35 - Loss: 1.9889\n",
      "Epoch 6 Training completed. Average Loss: 0.6098\n",
      "Epoch 6 Validation completed. Average Loss: 4.1575\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.6242\n",
      "  Batch 20/35 - Loss: 0.5681\n",
      "  Batch 30/35 - Loss: 0.7016\n",
      "  Batch 35/35 - Loss: 3.0805\n",
      "Epoch 7 Training completed. Average Loss: 0.5420\n",
      "Epoch 7 Validation completed. Average Loss: 5.1608\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.1855\n",
      "  Batch 20/35 - Loss: 0.4101\n",
      "  Batch 30/35 - Loss: 0.2723\n",
      "  Batch 35/35 - Loss: 0.1254\n",
      "Epoch 8 Training completed. Average Loss: 0.2735\n",
      "Epoch 8 Validation completed. Average Loss: 6.1457\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.3718\n",
      "  Batch 20/35 - Loss: 2.4691\n",
      "  Batch 30/35 - Loss: 0.3999\n",
      "  Batch 35/35 - Loss: 0.1586\n",
      "Epoch 9 Training completed. Average Loss: 0.4024\n",
      "Epoch 9 Validation completed. Average Loss: 3.5873\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.5120\n",
      "  Batch 20/35 - Loss: 0.0810\n",
      "  Batch 30/35 - Loss: 0.3442\n",
      "  Batch 35/35 - Loss: 0.0943\n",
      "Epoch 10 Training completed. Average Loss: 0.2782\n",
      "Epoch 10 Validation completed. Average Loss: 5.9069\n",
      "\n",
      "Training complete.\n",
      "Activation function: gelu Test Loss: 5.8004, Test Accuracy: 23.49%\n",
      "\n",
      "Testing activation function: swish\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 2.4753\n",
      "  Batch 20/35 - Loss: 2.4985\n",
      "  Batch 30/35 - Loss: 2.3092\n",
      "  Batch 35/35 - Loss: 2.2596\n",
      "Epoch 1 Training completed. Average Loss: 20.8819\n",
      "Epoch 1 Validation completed. Average Loss: 2.1481\n",
      "New best gradients stored for epoch 1 with validation loss 2.1481 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 1.9932\n",
      "  Batch 20/35 - Loss: 2.3624\n",
      "  Batch 30/35 - Loss: 1.8846\n",
      "  Batch 35/35 - Loss: 2.0413\n",
      "Epoch 2 Training completed. Average Loss: 1.9918\n",
      "Epoch 2 Validation completed. Average Loss: 2.0816\n",
      "New best gradients stored for epoch 2 with validation loss 2.0816 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 1.7236\n",
      "  Batch 20/35 - Loss: 1.9681\n",
      "  Batch 30/35 - Loss: 1.5131\n",
      "  Batch 35/35 - Loss: 1.7301\n",
      "Epoch 3 Training completed. Average Loss: 1.6584\n",
      "Epoch 3 Validation completed. Average Loss: 2.0646\n",
      "New best gradients stored for epoch 3 with validation loss 2.0646 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 1.3177\n",
      "  Batch 20/35 - Loss: 1.2808\n",
      "  Batch 30/35 - Loss: 0.9576\n",
      "  Batch 35/35 - Loss: 0.9653\n",
      "Epoch 4 Training completed. Average Loss: 1.1566\n",
      "Epoch 4 Validation completed. Average Loss: 2.3377\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 0.6882\n",
      "  Batch 20/35 - Loss: 0.6881\n",
      "  Batch 30/35 - Loss: 0.3554\n",
      "  Batch 35/35 - Loss: 0.3788\n",
      "Epoch 5 Training completed. Average Loss: 0.6286\n",
      "Epoch 5 Validation completed. Average Loss: 2.6971\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 0.3732\n",
      "  Batch 20/35 - Loss: 0.2579\n",
      "  Batch 30/35 - Loss: 0.1975\n",
      "  Batch 35/35 - Loss: 0.2242\n",
      "Epoch 6 Training completed. Average Loss: 0.3006\n",
      "Epoch 6 Validation completed. Average Loss: 3.1179\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 0.2506\n",
      "  Batch 20/35 - Loss: 0.0947\n",
      "  Batch 30/35 - Loss: 0.0810\n",
      "  Batch 35/35 - Loss: 0.0578\n",
      "Epoch 7 Training completed. Average Loss: 0.1712\n",
      "Epoch 7 Validation completed. Average Loss: 3.3429\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 0.0661\n",
      "  Batch 20/35 - Loss: 0.0587\n",
      "  Batch 30/35 - Loss: 0.2038\n",
      "  Batch 35/35 - Loss: 0.0044\n",
      "Epoch 8 Training completed. Average Loss: 0.0744\n",
      "Epoch 8 Validation completed. Average Loss: 3.8561\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 0.0528\n",
      "  Batch 20/35 - Loss: 0.5898\n",
      "  Batch 30/35 - Loss: 0.1442\n",
      "  Batch 35/35 - Loss: 0.1301\n",
      "Epoch 9 Training completed. Average Loss: 0.1874\n",
      "Epoch 9 Validation completed. Average Loss: 4.6868\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 0.0958\n",
      "  Batch 20/35 - Loss: 0.7493\n",
      "  Batch 30/35 - Loss: 0.0547\n",
      "  Batch 35/35 - Loss: 0.0286\n",
      "Epoch 10 Training completed. Average Loss: 0.1288\n",
      "Epoch 10 Validation completed. Average Loss: 4.1737\n",
      "\n",
      "Training complete.\n",
      "Activation function: swish Test Loss: 4.0817, Test Accuracy: 25.50%\n",
      "\n",
      "Testing activation function: hard_sigmoid\n",
      "\n",
      "Starting Epoch 1/10\n",
      "  Batch 10/35 - Loss: 2.3831\n",
      "  Batch 20/35 - Loss: 2.3361\n",
      "  Batch 30/35 - Loss: 2.3062\n",
      "  Batch 35/35 - Loss: 2.3185\n",
      "Epoch 1 Training completed. Average Loss: 2.3191\n",
      "Epoch 1 Validation completed. Average Loss: 2.3032\n",
      "New best gradients stored for epoch 1 with validation loss 2.3032 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 2/10\n",
      "  Batch 10/35 - Loss: 2.3095\n",
      "  Batch 20/35 - Loss: 2.3050\n",
      "  Batch 30/35 - Loss: 2.3079\n",
      "  Batch 35/35 - Loss: 2.3081\n",
      "Epoch 2 Training completed. Average Loss: 2.3083\n",
      "Epoch 2 Validation completed. Average Loss: 2.3021\n",
      "New best gradients stored for epoch 2 with validation loss 2.3021 and accuracy 0.0%\n",
      "\n",
      "Starting Epoch 3/10\n",
      "  Batch 10/35 - Loss: 2.3152\n",
      "  Batch 20/35 - Loss: 2.3065\n",
      "  Batch 30/35 - Loss: 2.3075\n",
      "  Batch 35/35 - Loss: 2.3093\n",
      "Epoch 3 Training completed. Average Loss: 2.3085\n",
      "Epoch 3 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 4/10\n",
      "  Batch 10/35 - Loss: 2.3151\n",
      "  Batch 20/35 - Loss: 2.3067\n",
      "  Batch 30/35 - Loss: 2.3075\n",
      "  Batch 35/35 - Loss: 2.3090\n",
      "Epoch 4 Training completed. Average Loss: 2.3084\n",
      "Epoch 4 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 5/10\n",
      "  Batch 10/35 - Loss: 2.3148\n",
      "  Batch 20/35 - Loss: 2.3065\n",
      "  Batch 30/35 - Loss: 2.3075\n",
      "  Batch 35/35 - Loss: 2.3089\n",
      "Epoch 5 Training completed. Average Loss: 2.3084\n",
      "Epoch 5 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 6/10\n",
      "  Batch 10/35 - Loss: 2.3147\n",
      "  Batch 20/35 - Loss: 2.3064\n",
      "  Batch 30/35 - Loss: 2.3074\n",
      "  Batch 35/35 - Loss: 2.3087\n",
      "Epoch 6 Training completed. Average Loss: 2.3083\n",
      "Epoch 6 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 7/10\n",
      "  Batch 10/35 - Loss: 2.3146\n",
      "  Batch 20/35 - Loss: 2.3064\n",
      "  Batch 30/35 - Loss: 2.3074\n",
      "  Batch 35/35 - Loss: 2.3086\n",
      "Epoch 7 Training completed. Average Loss: 2.3083\n",
      "Epoch 7 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 8/10\n",
      "  Batch 10/35 - Loss: 2.3144\n",
      "  Batch 20/35 - Loss: 2.3063\n",
      "  Batch 30/35 - Loss: 2.3074\n",
      "  Batch 35/35 - Loss: 2.3084\n",
      "Epoch 8 Training completed. Average Loss: 2.3082\n",
      "Epoch 8 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 9/10\n",
      "  Batch 10/35 - Loss: 2.3143\n",
      "  Batch 20/35 - Loss: 2.3062\n",
      "  Batch 30/35 - Loss: 2.3074\n",
      "  Batch 35/35 - Loss: 2.3083\n",
      "Epoch 9 Training completed. Average Loss: 2.3082\n",
      "Epoch 9 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Starting Epoch 10/10\n",
      "  Batch 10/35 - Loss: 2.3141\n",
      "  Batch 20/35 - Loss: 2.3061\n",
      "  Batch 30/35 - Loss: 2.3073\n",
      "  Batch 35/35 - Loss: 2.3081\n",
      "Epoch 10 Training completed. Average Loss: 2.3081\n",
      "Epoch 10 Validation completed. Average Loss: 2.3022\n",
      "\n",
      "Training complete.\n",
      "Activation function: hard_sigmoid Test Loss: 2.3028, Test Accuracy: 10.07%\n",
      "\n",
      "Best Model Test Accuracy: 36.91% with Loss: 3.2175\n"
     ]
    }
   ],
   "source": [
    "# Assume 'test_loader' is the DataLoader for your test set, and 'criterion' is the loss function (e.g., CrossEntropyLoss)\n",
    "best_loss, best_model = test_multiple_activation_functions(train_spectogram, val_spectogram, test_spectogram, criterion, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class DifferentOptCNN(nn.Module):\n",
    "    def __init__(self, optimizer_class, scheduler_fn, learning_rate, scheduler_lr, input_channels=1, num_classes=10):\n",
    "        super(DifferentOptCNN, self).__init__()\n",
    "\n",
    "        # Capas convolucionales con ajustes en kernel_size y stride para evitar reducción excesiva\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=5, stride=2, padding=2),  # Conv layer 1\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),              # Conv layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)              # Conv layer 3\n",
    "        ])\n",
    "        \n",
    "        # Max Pooling layer con tamaño de kernel ajustado\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calcular el tamaño final después de las convoluciones\n",
    "        self.final_feature_map_size = self._get_conv_output_size(input_channels)\n",
    "\n",
    "        # Capas densas (sin cambios)\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(self.final_feature_map_size, 64),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 64)\n",
    "        ])\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Definir función de activación ELU\n",
    "        self.activation = F.elu\n",
    "\n",
    "        # Configuración del optimizador y scheduler\n",
    "        self.optimizer = optimizer_class(self.parameters(), lr=learning_rate)\n",
    "        self.scheduler = scheduler_fn(self.optimizer, scheduler_lr)\n",
    "\n",
    "    def _get_conv_output_size(self, input_channels):\n",
    "        # Tamaño de entrada (ajustar si es necesario)\n",
    "        height, width = 201, 552  # Reemplaza con el tamaño correcto de entrada si cambió\n",
    "\n",
    "        # Cálculo del tamaño de salida\n",
    "        for layer in self.conv_layers:\n",
    "            height = (height + 2 * layer.padding[0] - layer.kernel_size[0]) // layer.stride[0] + 1\n",
    "            width = (width + 2 * layer.padding[1] - layer.kernel_size[1]) // layer.stride[1] + 1\n",
    "            height //= 2  # Max pooling reduce la altura a la mitad\n",
    "            width //= 2   # Max pooling reduce el ancho a la mitad\n",
    "\n",
    "        return height * width * 128  # Tamaño de la característica de salida final\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasar por las capas convolucionales con la función de activación ELU y pooling\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = self.activation(conv_layer(x))\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Aplanar la salida de las capas convolucionales antes de pasar por las densas\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pasar por las capas densas con la función de activación ELU\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = self.activation(fc_layer(x))\n",
    "        \n",
    "        # Capa de salida final\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing config: Optimizer=SGD, Scheduler=StepLR, Learning Rate=0.001\n",
      "\n",
      "Starting Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (32x1x55125). Calculated output size: (32x0x27562). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_loss, best_model, best_config\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso:\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mgrid_search_model_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 38\u001b[0m, in \u001b[0;36mgrid_search_model_configurations\u001b[1;34m(train, val, test, criterion, device, num_epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m best_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo en el conjunto de prueba\u001b[39;00m\n\u001b[0;32m     41\u001b[0m test_loss, accuracy \u001b[38;5;241m=\u001b[39m test_model_configuration(model, test, criterion, device)\n",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[38], line 60\u001b[0m, in \u001b[0;36mDifferentOptCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers:\n\u001b[0;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(conv_layer(x))\n\u001b[1;32m---> 60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Aplanar la salida de las capas convolucionales antes de pasar por las densas\u001b[39;00m\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (32x1x55125). Calculated output size: (32x0x27562). Output size is too small"
     ]
    }
   ],
   "source": [
    "def grid_search_model_configurations(train, val, test, criterion, device, num_epochs=10):\n",
    "    # Hiperparámetros a probar\n",
    "    optimizers = {\n",
    "        \"SGD\": optim.SGD,\n",
    "        \"Adam\": optim.Adam,\n",
    "        \"RMSprop\": optim.RMSprop\n",
    "    }\n",
    "    schedulers = {\n",
    "        \"StepLR\": lambda opt, lr: StepLR(opt, step_size=5, gamma=lr),\n",
    "        \"ExponentialLR\": lambda opt, lr: ExponentialLR(opt, gamma=lr),\n",
    "        \"ReduceLROnPlateau\": lambda opt, lr: ReduceLROnPlateau(opt, mode='min', patience=3, factor=lr)\n",
    "    }\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = float(\"inf\")\n",
    "    best_config = {}\n",
    "\n",
    "\n",
    "    for optimizer_name, optimizer_class in optimizers.items():\n",
    "        for scheduler_name, scheduler_fn in schedulers.items():\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\nTesting config: Optimizer={optimizer_name}, Scheduler={scheduler_name}, Learning Rate={lr}\")\n",
    "\n",
    "                # Inicializar el modelo con la función de activación fija\n",
    "                model = DifferentOptCNN(\n",
    "                    optimizer_class=optimizer_class,\n",
    "                    scheduler_fn=scheduler_fn,\n",
    "                    learning_rate=lr,\n",
    "                    scheduler_lr=lr,\n",
    "                    input_channels=20,  # Ajuste según el tamaño de los datos\n",
    "                    num_classes=10\n",
    "                )\n",
    "                model.to(device)\n",
    "\n",
    "                # Entrenar el modelo\n",
    "                best_gradients = train_model(model, train, val, criterion, model.optimizer, num_epochs, device)\n",
    "\n",
    "                # Evaluar el modelo en el conjunto de prueba\n",
    "                test_loss, accuracy = test_model_configuration(model, test, criterion, device)\n",
    "                \n",
    "                print(f\"Config: Optimizer={optimizer_name}, Scheduler={scheduler_name}, LR={lr} | Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "                # Guardar el mejor modelo según la precisión y la pérdida en el conjunto de prueba\n",
    "                if accuracy > best_accuracy or (accuracy == best_accuracy and test_loss < best_loss):\n",
    "                    best_accuracy = accuracy\n",
    "                    best_loss = test_loss\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_config = {\n",
    "                        \"optimizer\": optimizer_name,\n",
    "                        \"scheduler\": scheduler_name,\n",
    "                        \"learning_rate\": lr\n",
    "                    }\n",
    "                    print(f\"New best model found with accuracy: {best_accuracy:.2f}% and loss: {best_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nBest Model Test Accuracy: {best_accuracy:.2f}% with Loss: {best_loss:.4f}\")\n",
    "    print(f\"Best Configuration: {best_config}\")\n",
    "    return best_loss, best_model, best_config\n",
    "\n",
    "# Ejemplo de uso:\n",
    "grid_search_model_configurations(train_loader, val_loader, test_loader, criterion, device, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
